Directory structure:
└── docs/
    └── source/
        ├── advance.rst
        ├── cnconf.py
        ├── conf.py
        ├── contribution.rst
        ├── faq.rst
        ├── generate_rst.py
        ├── index.rst
        ├── infras.rst
        ├── modules.rst
        ├── quickstart.rst
        ├── sharpbits.rst
        ├── textbooktoc.rst
        ├── tutorial.rst
        ├── tutorial_cn.rst
        ├── whitepapertoc.rst
        ├── whitepapertoc_cn.rst
        ├── api/
        │   ├── about.rst
        │   ├── abstractcircuit.rst
        │   ├── analogcircuit.rst
        │   ├── applications.rst
        │   ├── backends.rst
        │   ├── basecircuit.rst
        │   ├── channels.rst
        │   ├── circuit.rst
        │   ├── cloud.rst
        │   ├── compiler.rst
        │   ├── cons.rst
        │   ├── densitymatrix.rst
        │   ├── experimental.rst
        │   ├── fgs.rst
        │   ├── gates.rst
        │   ├── interfaces.rst
        │   ├── keras.rst
        │   ├── mps_base.rst
        │   ├── mpscircuit.rst
        │   ├── noisemodel.rst
        │   ├── quantum.rst
        │   ├── quditcircuit.rst
        │   ├── quditgates.rst
        │   ├── results.rst
        │   ├── shadows.rst
        │   ├── simplify.rst
        │   ├── stabilizercircuit.rst
        │   ├── templates.rst
        │   ├── timeevol.rst
        │   ├── torchnn.rst
        │   ├── translation.rst
        │   ├── utils.rst
        │   ├── vis.rst
        │   ├── applications/
        │   │   ├── ai.rst
        │   │   ├── dqas.rst
        │   │   ├── finance.rst
        │   │   ├── graphdata.rst
        │   │   ├── layers.rst
        │   │   ├── optimization.rst
        │   │   ├── physics.rst
        │   │   ├── utils.rst
        │   │   ├── vags.rst
        │   │   ├── van.rst
        │   │   ├── vqes.rst
        │   │   ├── ai/
        │   │   │   └── ensemble.rst
        │   │   ├── finance/
        │   │   │   └── portfolio.rst
        │   │   └── physics/
        │   │       ├── baseline.rst
        │   │       └── fss.rst
        │   ├── backends/
        │   │   ├── backend_factory.rst
        │   │   ├── cupy_backend.rst
        │   │   ├── jax_backend.rst
        │   │   ├── numpy_backend.rst
        │   │   ├── pytorch_backend.rst
        │   │   └── tensorflow_backend.rst
        │   ├── cloud/
        │   │   ├── abstraction.rst
        │   │   ├── apis.rst
        │   │   ├── config.rst
        │   │   ├── local.rst
        │   │   ├── quafu_provider.rst
        │   │   ├── tencent.rst
        │   │   ├── utils.rst
        │   │   └── wrapper.rst
        │   ├── compiler/
        │   │   ├── composed_compiler.rst
        │   │   ├── qiskit_compiler.rst
        │   │   └── simple_compiler.rst
        │   ├── interfaces/
        │   │   ├── jax.rst
        │   │   ├── numpy.rst
        │   │   ├── scipy.rst
        │   │   ├── tensorflow.rst
        │   │   ├── tensortrans.rst
        │   │   └── torch.rst
        │   ├── results/
        │   │   ├── counts.rst
        │   │   ├── qem.rst
        │   │   ├── readout_mitigation.rst
        │   │   └── qem/
        │   │       ├── benchmark_circuits.rst
        │   │       └── qem_methods.rst
        │   └── templates/
        │       ├── ansatz.rst
        │       ├── blocks.rst
        │       ├── chems.rst
        │       ├── conversions.rst
        │       ├── dataset.rst
        │       ├── graphs.rst
        │       ├── hamiltonians.rst
        │       ├── lattice.rst
        │       └── measurements.rst
        ├── contribs/
        │   ├── development_Mac.md
        │   ├── development_Mac_cn.md
        │   ├── development_MacARM.md
        │   ├── development_MacM1.rst
        │   ├── development_MacM2.md
        │   ├── development_windows.rst
        │   └── development_wsl2.rst
        ├── locale/
        │   └── zh/
        │       └── LC_MESSAGES/
        │           ├── advance.po
        │           ├── contribs.po
        │           ├── contribution.po
        │           ├── faq.po
        │           ├── index.po
        │           ├── index_cn.po
        │           ├── infras.po
        │           ├── quickstart.po
        │           ├── sharpbits.po
        │           ├── tutorial.po
        │           ├── tutorial_cn.po
        │           └── whitepapertoc.po
        ├── textbook/
        │   ├── chap1.ipynb
        │   ├── chap2.ipynb
        │   └── chap3.ipynb
        ├── tutorials/
        │   ├── barren_plateaus.ipynb
        │   ├── barren_plateaus_cn.ipynb
        │   ├── circuit_basics.ipynb
        │   ├── circuit_basics_cn.ipynb
        │   ├── circuit_qudit_basics.ipynb
        │   ├── circuit_qudit_basics_cn.ipynb
        │   ├── contractors.ipynb
        │   ├── contractors_cn.ipynb
        │   ├── distributed_simulation.ipynb
        │   ├── dqas.ipynb
        │   ├── dqas_cn.ipynb
        │   ├── fermion_gaussian_states.ipynb
        │   ├── gradient_benchmark.ipynb
        │   ├── gradient_benchmark_cn.ipynb
        │   ├── qaoa_cn.ipynb
        │   ├── qcloud_sdk.ipynb
        │   ├── qml_scenarios.ipynb
        │   ├── qml_scenarios_cn.ipynb
        │   ├── sklearn_svc.ipynb
        │   ├── sklearn_svc_cn.ipynb
        │   ├── stabilizer_circuit.ipynb
        │   ├── template.ipynb
        │   ├── template_cn.ipynb
        │   ├── tfim_vqe.ipynb
        │   ├── tfim_vqe_cn.ipynb
        │   ├── tfim_vqe_diffreph.ipynb
        │   ├── tfim_vqe_diffreph_cn.ipynb
        │   ├── torch_qml.ipynb
        │   ├── torch_qml_cn.ipynb
        │   ├── vqe_h2o.ipynb
        │   └── vqe_h2o_cn.ipynb
        └── whitepaper/
            ├── 3-circuits-gates.ipynb
            ├── 3-circuits-gates_cn.ipynb
            ├── 4-gradient-optimization.ipynb
            ├── 4-gradient-optimization_cn.ipynb
            ├── 5-density-matrix.ipynb
            ├── 5-density-matrix_cn.ipynb
            ├── 6-1-conditional-measurements-post-selection.ipynb
            ├── 6-1-conditional-measurements-post-selection_cn.ipynb
            ├── 6-3-vmap.ipynb
            ├── 6-3-vmap_cn.ipynb
            ├── 6-4-quoperator.ipynb
            ├── 6-4-quoperator_cn.ipynb
            ├── 6-5-custom-contraction.ipynb
            ├── 6-5-custom-contraction_cn.ipynb
            ├── 6-6-advanced-automatic-differentiation.ipynb
            └── 6-6-advanced-automatic-differentiation_cn.ipynb

================================================
FILE: docs/source/advance.rst
================================================
================
Advanced Usage
================

MPS Simulator
----------------

TensorCircuit-NG provides Matrix Product State (MPS) simulation as an efficient alternative to exact simulation for quantum circuits. MPS simulation can handle larger quantum systems by trading off accuracy for computational efficiency.

MPS simulator is very straightforward to use, we provide the same set of API for ``MPSCircuit`` as ``Circuit``, 
the only new line is to set the bond dimension for the new simulator.

.. code-block:: python

    c = tc.MPSCircuit(n)
    c.set_split_rules({"max_singular_values": 50})

The larger bond dimension we set, the better approximation ratio (of course the more computational cost we pay).


Stacked gates syntax
------------------------

Stacked-gate is a simple syntactic sugar rendering circuit construction easily when multiple gate of the same type are applied on different qubits, namely, the index for gate call can accept list of ints instead of one integer.

.. code-block:: python

    >>> import tensorcircuit as tc
    >>> c = tc.Circuit(4)
    >>> c.h(range(3))
    >>> c.draw()
         ┌───┐
    q_0: ┤ H ├
         ├───┤
    q_1: ┤ H ├
         ├───┤
    q_2: ┤ H ├
         └───┘
    q_3: ─────


    >>> c = tc.Circuit(4)
    >>> c.cnot([0, 1], [2, 3])
    >>> c.draw()

    q_0: ──■───────
           │
    q_1: ──┼────■──
         ┌─┴─┐  │
    q_2: ┤ X ├──┼──
         └───┘┌─┴─┐
    q_3: ─────┤ X ├
              └───┘

    >>> c = tc.Circuit(4)
    >>> c.rx(range(4), theta=tc.backend.convert_to_tensor([0.1, 0.2, 0.3, 0.4]))
    >>> c.draw()
         ┌─────────┐
    q_0: ┤ Rx(0.1) ├
         ├─────────┤
    q_1: ┤ Rx(0.2) ├
         ├─────────┤
    q_2: ┤ Rx(0.3) ├
         ├─────────┤
    q_3: ┤ Rx(0.4) ├
         └─────────┘



Split Two-qubit Gates
-------------------------

The two-qubit gates applied on the circuit can be decomposed via SVD, which may further improve the optimality of the contraction pathfinding.

``split`` configuration can be set at circuit-level or gate-level.

.. code-block:: python

    split_conf = {
        "max_singular_values": 2,  # how many singular values are kept
        "fixed_choice": 1, # 1 for normal one, 2 for swapped one
    }

    c = tc.Circuit(nwires, split=split_conf)

    # or

    c.exp1(
            i,
            (i + 1) % nwires,
            theta=paramc[2 * j, i],
            unitary=tc.gates._zz_matrix,
            split=split_conf
        )

Note ``max_singular_values`` must be specified to make the whole procedure static and thus jittable.

Analog circuit simulation
-----------------------------

TensorCircuit-NG support digital-analog hybrid simulation (say cases in Rydberg atom arrays), where the analog part is simulated by the neural differential equation solver given the API to specify a time dependent Hamiltonian.
The simulation is still differentiable and jittable. Only jax backend is supported for analog simulation as the neural ode engine is built on top of jax. 

This utility is super helpful for optimizing quantum control or investigating digital-analog hybrid variational quantum schemes.

We support two modes of analog simulation, where :py:meth:`tensorcircuit.experimental.evol_global` evolve the state via a Hamiltonian define on the whole system, and :py:meth:`tensorcircuit.experimental.evol_local` evolve the state via a Hamiltonian define on a local subsystem.

.. Note::

    ``evol_global`` uses sparse Hamiltonian while ``evol_local`` uses dense Hamiltonian.


.. code-block:: python

    # in this demo, we build a jittable and differentiable simulation function `hybrid_evol` 
    # with both digital gates and local/global analog Hamiltonian evolutions

    import optax
    import tensorcircuit as tc
    from tensorcircuit.experimental import evol_global, evol_local

    K = tc.set_backend("jax")


    def h_fun(t, b):
        return b * tc.gates.x().tensor


    hy = tc.quantum.PauliStringSum2COO([[2, 0]])


    def h_fun2(t, b):
        return b[2] * K.cos(b[0] * t + b[1]) * hy


    @K.jit
    @K.value_and_grad
    def hybrid_evol(params):
        c = tc.Circuit(2)
        c.x([0, 1])
        c = evol_local(c, [1], h_fun, 1.0, params[0])
        c.cx(1, 0)
        c.h(0)
        c = evol_global(c, h_fun2, 1.0, params[1:])
        return K.real(c.expectation_ps(z=[0, 1]))


    b = K.implicit_randn([4])
    v, gs = hybrid_evol(b)


Time Evolution
------------------

TensorCircuit-NG provides several methods for simulating quantum time evolution, including exact diagonalization, Krylov subspace methods, and ODE-based approaches. 
These methods are essential for studying quantum dynamics, particularly in many-body systems, and all support automatic differentiation (AD) and JIT compilation for enhanced performance.

**Exact Diagonalization:**

For small systems where full diagonalization is feasible, the :py:meth:`tensorcircuit.timeevol.ed_evol` method provides exact time evolution by directly computing matrix exponentials
(alias :py:meth:`tensorcircuit.timeevol.hamiltonian_evol`):

.. code-block:: python

    import tensorcircuit as tc
    
    n = 4
    g = tc.templates.graphs.Line1D(n, pbc=False)
    h = tc.quantum.heisenberg_hamiltonian(g, hzz=1.0, hxx=1.0, hyy=1.0, sparse=False)
    
    # Initial Neel state: |↑↓↑↓⟩
    c = tc.Circuit(n)
    c.x([1, 3])  # Apply X gates to qubits 1 and 3
    psi0 = c.state()
    
    # Imaginary time evolution times
    times = tc.backend.convert_to_tensor([0.0, 0.5, 1.0, 2.0])
    
    # Evolve and get states
    states = tc.timeevol.ed_evol(h, psi0, times)
    print(states)
    

    def evolve_and_measure(params):
        # Parametrized Hamiltonian
        h_param = tc.quantum.heisenberg_hamiltonian(
            g, hzz=params[0], hxx=params[1], hyy=params[2], sparse=False
        )
        states = tc.timeevol.ed_evol(h_param, psi0, times)
        # Measure observable on final state
        circuit = tc.Circuit(n, inputs=states[-1])
        return tc.backend.real(circuit.expectation_ps(z=[0]))

    evolve_and_measure(tc.backend.ones([3]))

This method is particularly efficient for time-independent Hamiltonians as it uses eigendecomposition to compute the evolution. 
It provides exact results but is limited to small systems (typically <16 qubits) due to the exponential growth of the Hilbert space.

.. note::

    For real time evolution, the time should be chosen as ``times = 1.j * tc.backend.convert_to_tensor([0.0, 0.5, 1.0, 2.0])``


**Krylov Subspace Methods:**

For larger systems where exact diagonalization becomes intractable, the Krylov subspace method provides an efficient approximation. 
The :py:meth:`tensorcircuit.timeevol.krylov_evol` function implements this approach:

.. code-block:: python

    import tensorcircuit as tc
    
    # Create a Heisenberg Hamiltonian for a 1D chain
    n = 10
    g = tc.templates.graphs.Line1D(n, pbc=False)
    h = tc.quantum.heisenberg_hamiltonian(g, hzz=1.0, hxx=1.0, hyy=1.0, sparse=True)
    
    # Initial domain wall state: |↑↑↑↑↑↓↓↓↓↓⟩
    c = tc.Circuit(n)
    c.x(range(n//2, n))
    psi0 = c.state()
    
    # Real time evolution points
    times = tc.backend.convert_to_tensor([0.0, 0.5, 1.0, 2.0])
    
    # Perform Krylov evolution with a 30-dimensional subspace
    states = tc.timeevol.krylov_evol(h, psi0, times, subspace_dimension=30)
    
    # Krylov method also supports AD and JIT

    def krylov_evolution(params):
        # Parametrized initial state
        c = tc.Circuit(n)
        for i in range(n):
            c.rx(i, theta=params[i])
        psi0_param = c.state()
        states = tc.timeevol.krylov_evol(h, psi0_param, [1.0], subspace_dimension=20)
        # Measure total magnetization
        circuit = tc.Circuit(n, inputs=states[0])
        mz = sum(circuit.expectation_ps(z=[i]) for i in range(n))
        return tc.backend.real(mz)

The Krylov method constructs a small subspace that captures the essential dynamics, making it possible to simulate larger systems efficiently. 
It supports both standard and scan-based jit-friendly implementations:

.. code-block:: python

    # Standard implementation (default)
    states = tc.timeevol.krylov_evol(h, psi0, times, subspace_dimension=20, scan_impl=False)
    
    # Scan-based implementation for better JIT performance
    states = tc.timeevol.krylov_evol(h, psi0, times, subspace_dimension=20, scan_impl=True)

**ODE-Based Evolution:**

For time-dependent Hamiltonians or when fine control over the evolution process is needed, TensorCircuit provides ODE-based evolution methods. 
These methods solve the time-dependent Schrödinger equation directly by integrating the equation :math:`i\frac{d}{dt}|\psi(t)\rangle = H(t)|\psi(t)\rangle`.

TensorCircuit provides two ODE-based evolution methods depending on whether the Hamiltonian acts on the entire system or just a local subsystem:

1. **Global Evolution** (:py:meth:`tensorcircuit.timeevol.ode_evol_global`): For time-dependent Hamiltonians acting on the entire system. The Hamiltonian should be provided in sparse matrix format for efficiency.

.. code-block:: python

    import tensorcircuit as tc
    from jax import jit, value_and_grad
    
    # Set JAX backend for ODE support
    K = tc.set_backend("jax")
    
     # H(t) = -∑ᵢ Jᵢ(t) ZᵢZᵢ₊₁ - ∑ᵢ hᵢ(t) Xᵢ

    # Time-dependent coefficients
    def time_dep_J(t):
        return 1.0 + 0.5 * tc.backend.sin(2.0 * t)

    def time_dep_h(t):
        return 0.5 * tc.backend.cos(1.5 * t)

    zz_ham = tc.quantum.PauliStringSum2COO(
        [[3, 3, 0, 0], [0, 3, 3, 0], [0, 0, 3, 3]], [1, 1, 1]
    )
    x_ham = tc.quantum.PauliStringSum2COO(
        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], [1, 1, 1, 1]
    )

    # Hamiltonian construction function
    def hamiltonian_func(t):
        # Create time-dependent ZZ terms
        zz_coeff = time_dep_J(t)

        # Create time-dependent X terms
        x_coeff = time_dep_h(t)

        return zz_coeff * zz_ham + x_coeff * x_ham

    # Initial state: |↑↓↑↓⟩
    c = tc.Circuit(4)
    c.x([1, 3])
    psi0 = c.state()

    # Time points for evolution
    times = tc.backend.arange(0, 5, 0.5)

    # Perform global ODE evolution
    states = tc.timeevol.ode_evol_global(hamiltonian_func, psi0, times)
    assert tc.backend.shape_tuple(states) == (10, 16)

    zz_ham = tc.quantum.PauliStringSum2COO([[3, 3, 0, 0], [0, 3, 3, 0]], [1, 1])
    x_ham = tc.quantum.PauliStringSum2COO([[1, 0, 0, 0], [0, 1, 0, 0]], [1, 1])

    # Example with parameterized Hamiltonian and optimization
    def parametrized_hamiltonian(t, params):
        # params = [J0, J1, h0, h1] - parameters to optimize
        J_t = params[0] + params[1] * tc.backend.sin(2.0 * t)
        h_t = params[2] + params[3] * tc.backend.cos(1.5 * t)

        return J_t * zz_ham + h_t * x_ham

    # Observable function: measure ZZ correlation
    def zz_correlation(state):
        n = int(np.log2(state.shape[0]))
        circuit = tc.Circuit(n, inputs=state)
        return circuit.expectation_ps(z=[0, 1])

    @tc.backend.jit
    @tc.backend.value_and_grad
    def objective_function(params):
        states = tc.timeevol.ode_evol_global(
            parametrized_hamiltonian,
            psi0,
            tc.backend.convert_to_tensor([0, 1.0]),
            None,
            params,
        )
        # Measure ZZ correlation at final time
        final_state = states[-1]
        return tc.backend.real(zz_correlation(final_state))

    print(objective_function(tc.backend.ones([4])))
    


2. **Local Evolution** (:py:meth:`tensorcircuit.timeevol.ode_evol_local`): For time-dependent Hamiltonians acting on a subsystem of qubits. The Hamiltonian should be provided in dense matrix format.

.. code-block:: python

    import tensorcircuit as tc
    import jax.numpy as jnp
    from jax import jit
    
    # Set JAX backend for ODE support
    tc.set_backend("jax")
    K = tc.backend
    
    # Time-dependent local Hamiltonian on qubits 1 and 2
    # H(t) = Ω(t) * (cos(φ(t)) * X + sin(φ(t)) * Y)
    def local_hamiltonian(t, Omega, phi):
        # Rabi oscillation Hamiltonian
        angle = phi * t
        coeff = Omega * jnp.cos(2.0 * t)  # Amplitude modulation
        
        # Single-qubit Rabi Hamiltonian (2x2 matrix)
        hx = coeff * jnp.cos(angle) * tc.gates.x().tensor
        hy = coeff * jnp.sin(angle) * tc.gates.y().tensor
        return hx + hy
    
    # Initial state: GHZ state |0000⟩ + |1111⟩
    c = tc.Circuit(4)
    c.h(0)
    for i in range(3):
        c.cnot(i, i+1)
    psi0 = c.state()
    
    times = tc.backend.arange(0.0, 3.0, 0.1)
    
    # Evolve with local Hamiltonian acting on qubit 1
    states = tc.timeevol.ode_evol_local(
        local_hamiltonian,
        psi0,
        times,
        [1],  # Apply to qubit 1
        None,
        1.0, 
        2.0 # Omega=1.0, phi=2.0
    )
    

Both ODE-based methods support automatic differentiation and JIT compilation when using the JAX backend, making them suitable for optimization tasks in quantum control and variational quantum algorithms. 
The methods integrate the time-dependent Schrödinger equation using JAX's ODE solvers, providing flexible and efficient simulation of quantum dynamics with time-dependent Hamiltonians.

.. note::

    1. ODE-based methods currently only support the JAX backend due to the dependency on JAX's ODE solvers.
    2. Global evolution requires sparse Hamiltonian matrices for efficiency with large systems.
    3. Local evolution requires dense Hamiltonian matrices and is suitable for subsystems with few qubits.
    4. Both methods support callback functions to compute observables during evolution without storing all state vectors.

**Comparison of Time Evolution Methods:**

+--------------------------+----------------+------------------+------------------+------------------+
| Method                   | System Size    | Accuracy         | AD Support       | JIT Support      |
+==========================+================+==================+==================+==================+
| ED Evolution             | < 16 qubits    | Exact            | ✅               | ✅               |
+--------------------------+----------------+------------------+------------------+------------------+
| Krylov Evolution         | 16-30+ qubits  | Approximate      | ✅               | ✅ (JAX only)    |
+--------------------------+----------------+------------------+------------------+------------------+
| ODE Local Evolution      | Any size       | Solver-dependent | ✅ (JAX only)    | ✅ (JAX only)    |
+--------------------------+----------------+------------------+------------------+------------------+
| ODE Global Evolution     | ~ 20 qubits    | Solver-dependent | ✅ (JAX only)    | ✅ (JAX only)    |
+--------------------------+----------------+------------------+------------------+------------------+

**Method Selection Guidelines:**

1. **Exact diagonalization Evolution**: Best for small systems where exact results are required. Most efficient for time-independent Hamiltonians. Support imaginary time evolution.

2. **Krylov Evolution**: Ideal for large systems with time-independent Hamiltonians. Provides a good balance between accuracy and computational efficiency. The subspace dimension controls the trade-off between accuracy and speed.

3. **ODE Local Evolution**: Suitable for time-dependent Hamiltonians acting on a few qubits. Most flexible for complex control protocols or digital-analog hybrid programs.

4. **ODE Global Evolution**: Best for time-dependent Hamiltonians acting on the entire system. 

**Advanced Usage:**

Callback functions can be used to compute observables during evolution without storing all state vectors:

.. code-block:: python

    def compute_total_magnetization(state):
        # Compute total magnetization ⟨∑Zᵢ⟩
        n = int(tc.backend.log2(tc.backend.shape_tuple(state)[0]))
        circuit = tc.Circuit(n, inputs=state)
        total_mz = sum(circuit.expectation_ps(z=[i]) for i in range(n))
        return tc.backend.real(total_mz)
    
    # Evolve with callback
    magnetizations = tc.timeevol.krylov_evol(
        h, psi0, times, subspace_dimension=20, callback=compute_total_magnetization
    )

All time evolution methods in TensorCircuit support automatic differentiation and JIT compilation, making them suitable for variational optimization and other machine learning applications in quantum physics.


Jitted Function Save/Load
-----------------------------

To reuse the jitted function, we can save it on the disk via support from the TensorFlow `SavedModel <https://www.tensorflow.org/guide/saved_model>`_. That is to say, only jitted quantum function on the TensorFlow backend can be saved on the disk. 

We wrap the tf-backend ``SavedModel`` as very easy-to-use function :py:meth:`tensorcircuit.keras.save_func` and :py:meth:`tensorcircuit.keras.load_func`.

For the JAX-backend quantum function, one can first transform them into the tf-backend function via JAX experimental support: `jax2tf <https://github.com/google/jax/tree/main/jax/experimental/jax2tf>`_.

**Updates**: jax now also support jitted function save/load via ``export`` module, see `jax documentation <https://jax.readthedocs.io/en/latest/export/export.html>`_.

We wrap the jax function export capability in ``experimental`` module and can be used as follows

.. code-block:: python

    from tensorcircuit import experimental

    K = tc.set_backend("jax")

    @K.jit
    def f(weights):
        c = tc.Circuit(3)
        c.rx(range(3), theta=weights)
        return K.real(c.expectation_ps(z=[0]))

    print(f(K.ones([3])))

    experimental.jax_jitted_function_save("temp.bin", f, K.ones([3]))

    f_load = tc.experimental.jax_jitted_function_load("temp.bin")
    f_load(K.ones([3]))



Parameterized Measurements
-----------------------------

For plain measurements API on a ``tc.Circuit``, eg. ``c = tc.Circuit(3)``, if we want to evaluate the expectation :math:`<Z_1Z_2>`, we need to call the API as ``c.expectation((tc.gates.z(), [1]), (tc.gates.z(), [2]))``. 

In some cases, we may want to tell the software what to measure but in a tensor fashion. For example, if we want to get the above expectation, we can use the following API: :py:meth:`tensorcircuit.templates.measurements.parameterized_measurements`.

.. code-block:: python

    c = tc.Circuit(3)
    z1z2 = tc.templates.measurements.parameterized_measurements(c, tc.array_to_tensor([0, 3, 3, 0]), onehot=True) # 1

This API corresponds to measure :math:`I_0Z_1Z_2I_3` where 0, 1, 2, 3 are for local I, X, Y, and Z operators respectively.

Sparse Matrix
----------------

We support COO format sparse matrix as most backends only support this format, and some common backend methods for sparse matrices are listed below:

.. code-block:: python

    def sparse_test():
        m = tc.backend.coo_sparse_matrix(indices=np.array([[0, 1],[1, 0]]), values=np.array([1.0, 1.0]), shape=[2, 2])
        n = tc.backend.convert_to_tensor(np.array([[1.0], [0.0]]))
        print("is sparse: ", tc.backend.is_sparse(m), tc.backend.is_sparse(n))
        print("sparse matmul: ", tc.backend.sparse_dense_matmul(m, n))

    for K in ["tensorflow", "jax", "numpy"]:
        with tc.runtime_backend(K):
            print("using backend: ", K)
            sparse_test()

The sparse matrix is specifically useful to evaluate Hamiltonian expectation on the circuit, where sparse matrix representation has a good tradeoff between space and time.
Please refer to :py:meth:`tensorcircuit.templates.measurements.sparse_expectation` for more detail.

For different representations to evaluate Hamiltonian expectation in tensorcircuit, please refer to :doc:`tutorials/tfim_vqe_diffreph`.


Hamiltonian Matrix Building
----------------------------

TensorCircuit-NG provides multiple ways to build Hamiltonian matrices, especially for sparse Hamiltonians constructed from Pauli strings. This is crucial for quantum many-body physics simulations and variational quantum algorithms.

**Pauli String Based Construction:**

The most flexible way to build Hamiltonians is through Pauli strings:

.. code-block:: python

    import tensorcircuit as tc
    
    # Define Pauli strings and their weights
    # Each Pauli string is represented by a list of integers:
    # 0: Identity, 1: X, 2: Y, 3: Z
    pauli_strings = [
        [1, 1, 0],  # X₁X₂I₃
        [3, 3, 0],  # Z₁Z₂I₃
        [0, 0, 1],  # I₁I₂X₃
    ]
    weights = [0.5, 1.0, -0.2]
    
    # Build sparse Hamiltonian
    h_sparse = tc.quantum.PauliStringSum2COO(pauli_strings, weights)
    
    # Or dense Hamiltonian if preferred
    h_dense = tc.quantum.PauliStringSum2Dense(pauli_strings, weights)


**High-Level Hamiltonian Construction:**

For common Hamiltonians like Heisenberg model:

.. code-block:: python

    # Create a 1D chain with 10 sites
    g = tc.templates.graphs.Line1D(10, pbc=True)  # periodic boundary condition
    
    # XXZ model
    h = tc.quantum.heisenberg_hamiltonian(
        g,
        hxx=1.0,  # XX coupling
        hyy=1.0,  # YY coupling
        hzz=1.2,  # ZZ coupling
        hx=0.5,   # X field
        sparse=True
    )


**Advanced Usage:**

1. Converting between xyz and Pauli string representations:

.. code-block:: python

    # Convert Pauli string to xyz format
    xyz_dict = tc.quantum.ps2xyz([1, 2, 2, 0])  # X₁Y₂Y₃I₄
    print(xyz_dict)  # {'x': [0], 'y': [1, 2], 'z': []}
    
    # Convert back to Pauli string
    ps = tc.quantum.xyz2ps(xyz_dict, n=4)
    print(ps)  # [1, 2, 2, 0]


2. Working with MPO format:

TensorCircuit-NG supports conversion from different MPO (Matrix Product Operator) formats, particularly from TensorNetwork and Quimb libraries. This is useful when you want to leverage existing MPO implementations or convert between different frameworks.

**TensorNetwork MPO:**

For TensorNetwork MPOs, you can convert predefined models like the Transverse Field Ising (TFI) model:

.. code-block:: python

    import tensorcircuit as tc
    import tensornetwork as tn
    
    # Create TFI Hamiltonian MPO from TensorNetwork
    nwires = 6
    Jx = np.array([1.0] * (nwires - 1))  # XX coupling strength
    Bz = np.array([-1.0] * nwires)       # Transverse field strength
    
    # Create TensorNetwork MPO
    tn_mpo = tn.matrixproductstates.mpo.FiniteTFI(
        Jx, Bz, 
        dtype=np.complex64
    )
    
    # Convert to TensorCircuit format
    tc_mpo = tc.quantum.tn2qop(tn_mpo)
    
    # Get dense matrix representation
    h_matrix = tc_mpo.eval_matrix()

Note: TensorNetwork MPO currently only supports open boundary conditions.

**Quimb MPO:**

Quimb provides more flexible MPO construction options:

.. code-block:: python

    import tensorcircuit as tc
    import quimb.tensor as qtn
    
    # Create Ising Hamiltonian MPO using Quimb
    nwires = 6
    J = 4.0    # ZZ coupling
    h = 2.0    # X field
    qb_mpo = qtn.MPO_ham_ising(
        nwires, 
        J, h,
        cyclic=True  # Periodic boundary conditions
    )
    
    # Convert to TensorCircuit format
    tc_mpo = tc.quantum.quimb2qop(qb_mpo)
    
    # Custom Hamiltonian construction
    builder = qtn.SpinHam1D()
    builder += 1.0, "Y"  # Add Y term with strength 1.0
    builder += 0.5, "X"  # Add X term with strength 0.5
    H = builder.build_mpo(3)  # Build for 3 sites
    
    # Convert to TensorCircuit MPO
    h_tc = tc.quantum.quimb2qop(H)



Stabilizer Circuit Simulator
-----------------------------

TensorCircuit-NG provides a Stabilizer Circuit simulator for efficient simulation of Clifford circuits. 
This simulator is particularly useful for quantum error correction, measurement induced phase transition, etc.

The stabilizer simulation is backend by Python package `Stim <https://github.com/quantumlib/Stim>`_, please ensure you have ``pip install stim`` first.


.. code-block:: python

    import tensorcircuit as tc
    
    # Create a stabilizer circuit
    c = tc.StabilizerCircuit(2)
    
    # Apply Clifford gates
    c.h(0)
    c.cnot(0, 1)
    
    # Measure qubits
    results = c.measure(0, 1)  # Returns measurement outcomes
    
    # Sample multiple shots
    samples = c.sample(batch=1000)  # Returns array of shape (1000, 2)

**Supported Operations**

The simulator supports common Clifford gates and operations:

- Single-qubit gates: H, X, Y, Z, S, SDG (S dagger)
- Two-qubit gates: CNOT, CZ, SWAP
- Measurements: projective measurements (``c.measurement`` doesn't affect the state while ``c.cond_measure`` collpases the state)
- Post-selection (``c.post_select``)
- Random Clifford gates (``c.random_gate``)
- Gates defined by tableau (``c.tableau_gate``)
- Entanglement calculation (``c.entanglement_entropy``)
- Pauli string operator expectation (``c.expectation_ps``)
- Openqasm and qir transformation as usual circuits
- Initialization state provided by Pauli string stabilizer (``tc.StabCircuit(inputs=...)``) or inverse tableau (``tc.StabCircuit(tableau_inputs=)``)
- Probabilistic noise (``c.depolarizing``)


Example: Quantum Teleportation

.. code-block:: python

    c = tc.StabilizerCircuit(3)
    
    # Prepare Bell pair between qubits 1 and 2
    c.h(1)
    c.cnot(1, 2)
    
    # State to teleport on qubit 0 (must be Clifford)
    c.x(0)
    
    # Teleportation circuit
    c.cnot(0, 1)
    c.h(0)
    
    # Measure and apply corrections
    r0 = c.cond_measure(0)
    r1 = c.cond_measure(1)
    if r0 == 1:
        c.z(2)
    if r1 == 1:
        c.x(2)




Fermion Gaussian State Simulator
--------------------------------

TensorCircuit-NG provides a powerful Fermion Gaussian State (FGS) simulator for efficient simulation of non-interacting fermionic systems (with or without U(1) symmtery). The simulator is particularly useful for studying quantum many-body physics and entanglement properties.


.. code-block:: python

    import tensorcircuit as tc
    import numpy as np

    # Initialize a 4-site system with sites 0 and 2 occupied
    sim = tc.FGSSimulator(L=4, filled=[0, 2])
    
    # Evolve with hopping terms
    sim.evol_hp(i=0, j=1, chi=1.0)  # hopping between sites 0 and 1
    
    # Calculate entanglement entropy for subsystem of sites 0, 1
    entropy = sim.entropy([2, 3])


The simulator supports various operations including:

1. State initialization from quadratic Hamiltonians ground states
2. Time evolution (real and imaginary)
3. Entanglement measures (von Neumann, Renyi entropies and entanglement asymmetry)
4. Correlation matrix calculations
5. Measurements


Here's an example studying entanglement asymmetry in tilted ferromagnet states:

.. code-block:: python

    def xy_hamiltonian(theta, L):
        # XY model with tilted field
        gamma = 2 / (np.cos(theta) ** 2 + 1) - 1
        mu = 4 * np.sqrt(1 - gamma**2) * np.ones([L])
        
        # Construct Hamiltonian terms
        h = (generate_hopping_h(2.0, L) + 
             generate_pairing_h(gamma * 2, L) + 
             generate_chemical_h(mu))
        return h

    def get_saq_sa(theta, l, L, k, batch=1024):
        # Calculate entanglement asymmetry in the middle subsystem with size l
        traceout = [i for i in range(0, L//2 - l//2)] + \
                  [i for i in range(L//2 + l//2, L)]
        
        # Get Hamiltonian ground state which is within FGS
        hi = xy_hamiltonian(theta, L)
        sim = tc.FGSSimulator(L, hc=hi)
        
        # Get both symmetry-resolved and standard entanglement
        return (np.real(sim.renyi_entanglement_asymmetry(k, traceout, batch=batch)),
                sim.renyi_entropy(k, traceout))


Randoms, Jit, Backend Agnostic, and Their Interplay
--------------------------------------------------------

This section explains how random number generation interacts with JIT compilation and backend agnosticism in TensorCircuit. Understanding this interplay is crucial for reproducible and correct simulation results, especially when using JAX.

**Key Management for Reproducibility:**
In JAX, random number generation is deterministic and relies on explicit "keys" that manage the random state. This is different from TensorFlow or NumPy, where random states are often managed implicitly. For reproducible results and correct JIT compilation, JAX requires these keys to be passed and split explicitly.

**Why Explicit Key Management?**
When a JIT-compiled function is called multiple times with the same inputs, JAX aims to produce the same output. If random numbers were generated implicitly within a JIT-compiled function, subsequent calls would produce the same "random" numbers, which is often not the desired behavior for simulations requiring true randomness across runs. 
Explicit key management ensures that each call to a random function, even within JIT, uses a new, distinct random state derived from a split key, thus maintaining the desired randomness and reproducibility.

.. code-block:: python

    import tensorcircuit as tc
    K = tc.set_backend("tensorflow")
    K.set_random_state(42)

    @K.jit
    def r():
        return K.implicit_randn()

    print(r(), r()) # different, correct

.. code-block:: python

    import tensorcircuit as tc
    K = tc.set_backend("jax")
    K.set_random_state(42)

    @K.jit
    def r():
        return K.implicit_randn()

    print(r(), r()) # the same, wrong


.. code-block:: python

    import tensorcircuit as tc
    import jax
    K = tc.set_backend("jax")
    key = K.set_random_state(42)

    @K.jit
    def r(key):
        K.set_random_state(key)
        return K.implicit_randn()

    key1, key2 = K.random_split(key)

    print(r(key1), r(key2)) # different, correct

Therefore, a unified jittable random infrastructure with backend agnostic can be formulated as 

.. code-block:: python

    import tensorcircuit as tc
    import jax
    K = tc.set_backend("tensorflow")

    def ba_key(key):
        if tc.backend.name == "tensorflow":
            return None
        if tc.backend.name == "jax":
            return jax.random.PRNGKey(key)
        raise ValueError("unsupported backend %s"%tc.backend.name)

        
    @K.jit
    def r(key=None):
        if key is not None:
            K.set_random_state(key)
        return K.implicit_randn()

    key = ba_key(42)

    key1, key2 = K.random_split(key)

    print(r(key1), r(key2))

And a more neat approach to achieve this is as follows:

.. code-block:: python

    key = K.get_random_state(42)

    @K.jit
    def r(key):
        K.set_random_state(key)
        return K.implicit_randn()

    key1, key2 = K.random_split(key)

    print(r(key1), r(key2))

It is worth noting that since ``Circuit.unitary_kraus`` and ``Circuit.general_kraus`` call ``implicit_rand*`` API, the correct usage of these APIs is the same as above.

One may wonder why random numbers are dealt in such a complicated way, please refer to the `Jax design note <https://jax.readthedocs.io/en/latest/jep/263-prng.html>`_ for some hints.

If vmap is also involved apart from jit, I currently find no way to maintain the backend agnosticity as TensorFlow seems to have no support of vmap over random keys (ping me on GitHub if you think you have a way to do this). I strongly recommend the users using Jax backend in the vmap+random setup.



================================================
FILE: docs/source/cnconf.py
================================================
# -*- coding: utf-8 -*-
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys

sys.path.insert(0, os.path.abspath("../../"))
sys.path.insert(0, os.path.abspath("../ext/"))

# -- Project information -----------------------------------------------------

project = "tensorcircuit-ng"
copyright = "2020, The TensorCircuit Authors"
author = "refraction-ray"

# The short X.Y version
version = ""
# The full version, including alpha/beta/rc tags
release = ""


# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.mathjax",
    "sphinx.ext.viewcode",
    "sphinx.ext.autosectionlabel",
    "sphinx.ext.ifconfig",
    "sphinx_copybutton",
    "nbsphinx",
    "toctree_filter",
    "sphinx.ext.napoleon",
    "myst_parser",
    "sphinx_design",
]

autosectionlabel_prefix_document = True

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = ".rst"

# The master toctree document.
master_doc = "index"

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = "zh"

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path .
exclude_patterns = []

autodoc_default_options = {
    "members": True,
    "special-members": "__init__",
    "undoc-members": True,
    "private-members": False,
    "exclude-members": "__weakref__, _abc_cache, _abc_negative_cache, _abc_negative_cache_version,_abc_registry",
}

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = "sphinx"


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = "furo"

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
# html_theme_options = {}
# currently nature theme doesn't support these github related options (refraction-ray)
html_theme_options = {
    # "github_user": "refraction-ray",
    # "github_repo": "tensorcircuit",
    # "github_button": "true",
    # "github_type": "star",
    # "github_banner": "true",
    # "show_powered_by": "false",
}


# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}

# rtd github link config
html_context = {
    "display_github": True,  # Integrate GitHub
    "github_user": "tensorcircuit",  # Username
    "github_repo": "tensorcircuit-ng",  # Repo name
    "github_version": "master",  # Version
    "conf_py_path": "/source/",  # Path in the checkout to the docs root
}


# -- Options for HTMLHelp output ---------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = "tensorcircuitdoc"


# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',
    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',
    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',
    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (
        master_doc,
        "tensorcircuit.tex",
        "tensorcircuit Documentation",
        "refraction-ray",
        "manual",
    ),
]


# -- Options for manual page output ------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [(master_doc, "tensorcircuit", "tensorcircuit Documentation", [author], 1)]


# -- Options for Texinfo output ----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        master_doc,
        "tensorcircuit",
        "tensorcircuit Documentation",
        author,
        "tensorcircuit",
        "One line description of project.",
        "Miscellaneous",
    ),
]

locale_dirs = ["locale/"]  # path is example but recommended.
# -- Extension configuration -------------------------------------------------

# sphinx-copybutton
copybutton_prompt_text = r">>> |\.\.\. |\$ |In \[\d*\]: | {2,5}\.\.\.: | {5,8}: "
copybutton_prompt_is_regexp = True



================================================
FILE: docs/source/conf.py
================================================
# -*- coding: utf-8 -*-
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys

sys.path.insert(0, os.path.abspath("../../"))
sys.path.insert(0, os.path.abspath("../ext/"))

# -- Project information -----------------------------------------------------

project = "tensorcircuit-ng"
copyright = "2020, TensorCircuit Development Team. Created by Shi-Xin Zhang."
author = "refraction-ray"

# The short X.Y version
version = ""
# The full version, including alpha/beta/rc tags
release = ""


# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.mathjax",
    "sphinx.ext.viewcode",
    "sphinx.ext.autosectionlabel",
    "sphinx.ext.ifconfig",
    "sphinx_copybutton",
    "nbsphinx",
    "toctree_filter",
    "sphinx.ext.napoleon",
    "myst_parser",
    "sphinx_design",
]

nbsphinx_allow_errors = True

autosectionlabel_prefix_document = True

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = ".rst"

# The master toctree document.
master_doc = "index"

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = "en"


# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path .
exclude_patterns = []

autodoc_default_options = {
    "members": True,
    "special-members": "__init__",
    "undoc-members": True,
    "private-members": False,
    "exclude-members": "__weakref__, _abc_cache, _abc_negative_cache, _abc_negative_cache_version,_abc_registry",
}

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = "sphinx"


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = "furo"

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
# html_theme_options = {}
# currently nature theme doesn't support these github related options (refraction-ray)
html_theme_options = {
    # "github_user": "refraction-ray",
    # "github_repo": "tensorcircuit",
    # "github_button": "true",
    # "github_type": "star",
    # "github_banner": "true",
    # "show_powered_by": "false",
}


# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}

# rtd github link config
html_context = {
    "display_github": True,  # Integrate GitHub
    "github_user": "tensorcircuit",  # Username
    "github_repo": "tensorcircuit-ng",  # Repo name
    "github_version": "master",  # Version
    "conf_py_path": "/source/",  # Path in the checkout to the docs root
}


# -- Options for HTMLHelp output ---------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = "tensorcircuitdoc"

html_title = "TensorCircuit Next Generation"

# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',
    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',
    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',
    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (
        master_doc,
        "tensorcircuit.tex",
        "tensorcircuit Documentation",
        "refraction-ray",
        "manual",
    ),
]


# -- Options for manual page output ------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [(master_doc, "tensorcircuit", "tensorcircuit Documentation", [author], 1)]


# -- Options for Texinfo output ----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        master_doc,
        "tensorcircuit",
        "tensorcircuit Documentation",
        author,
        "tensorcircuit",
        "One line description of project.",
        "Miscellaneous",
    ),
]

locale_dirs = ["locale/"]  # path is example but recommended.
# -- Extension configuration -------------------------------------------------

# sphinx-copybutton
copybutton_prompt_text = r">>> |\.\.\. |\$ |In \[\d*\]: | {2,5}\.\.\.: | {5,8}: "
copybutton_prompt_is_regexp = True



================================================
FILE: docs/source/contribution.rst
================================================
Guide for Contributors
============================

We welcome everyone’s contributions! The development of TensorCircuit-NG is open-sourced and centered on `GitHub <https://github.com/tensorcircuit/tensorcircuit-ng>`_.

There are various ways to contribute:

* Answering questions on the discussions page or issue page.

* Raising issues such as bug reports or feature requests on the issue page.

* Improving the documentation (docstrings/tutorials) by pull requests.

* Contributing to the codebase by pull requests.



Pull Request Guidelines
-------------------------------

We welcome pull requests from everyone. For large PRs involving feature enhancement or API changes, we ask that you first open a GitHub issue to discuss your proposal.

The following git workflow is recommended for contribution by PR:

* Configure your git username and email so that they match your GitHub account if you haven't.

.. code-block:: bash

    git config user.name <GitHub name>
    git config user.email <GitHub email>

* Fork the TensorCircuit-NG repository by clicking the Fork button on GitHub. This will create an independent version of the codebase in your own GitHub account.

* Clone your forked repository and set up an ``upstream`` reference to the official TensorCircuit-NG repository.

.. code-block:: bash

    git clone <your-forked-repo-git-link>
    cd tensorcircuit
    git remote add upstream <official-repo-git-link>

* Configure the python environment locally for development. The following commands are recommended:

.. code-block:: bash

    pip install -r requirements/requirements.txt
    pip install -r requirements/requirements-dev.txt
    pip install -r requirements/requirements-types.txt
    pip install -r requirements/requirements-extra.txt

Extra packages may be required for specific development tasks.

* Pip installing your fork from the source. This allows you to modify the code locally and immediately test it out.

.. code-block:: bash

    pip install -e .

* Create a feature branch where you can make modifications and developments. DON'T open PR from your master/main branch.

.. code-block:: bash

    git checkout -b <name-of-change>

* Make sure your changes can pass all checks by running: ``bash check_all.sh``. (See the :ref:`Checks` section below for details)

* Once you are satisfied with your changes, create a commit as follows:

.. code-block:: bash

    git add file1.py file2.py ...
    git commit -m "Your commit message (should be brief and informative)"
    
* You should sync your code with the official repo:

.. code-block:: bash

    git fetch upstream
    git rebase upstream/master      # resolve conflicts if any

* Note that PRs typically comprise a single git commit, you should squash all your commits in the feature branch. Using ``git rebase -i`` for commits squash, see `instructions <https://www.internalpointers.com/post/squash-commits-into-one-git>`_

* Push your commit from your feature branch. This will create a remote branch in your forked repository on GitHub, from which you will raise a PR.

.. code-block:: bash

  git push --set-upstream origin <name-of-change>

* Create a PR from the official TensorCircuit-NG repository and send it for review. Some comments and remarks attached with the PR are recommended. If the PR is not finally finished, please add [WIP] at the beginning of the title of your PR.

* The PR will be reviewed by the developers and may get approved or change requested. In the latter case, you can further revise the PR according to suggestions and feedback from the code reviewers.

* The PR you opened can be automatically updated once you further push commits to your forked repository. Please remember to ping the code reviewers in the PR conversation soon.

* Please always include new docs and tests for your PR if possible and record your changes on CHANGELOG.


Checks
--------------------

The simplest way to ensure the codebase is ok with checks and tests is to run one-in-all scripts ``./check_all.sh`` (you may need to ``chmod +x check_all.sh`` to grant permissions on this file).

The scripts include the following components:

* black

* mypy: configure file is now in ``pyproject.toml``, results strongly correlated with the version of numpy, we fix ``numpy==1.21.5`` as mypy standard in CI.

* pylint: configure file is ``.pylintrc``

* pytest: see :ref:`Pytest` sections for details. 

* sphinx doc builds: see :ref:`Docs` section for details.

Make sure the scripts check are successful by 💐.

Similar tests and checks are also available via GitHub action as CI infrastructures.

Please also include corresponding changes for CHANGELOG.md and docs for the PR.


Pytest
---------

For pytest, one can speed up the test by ``pip install pytest-xdist``, and then run parallelly as ``pytest -v -n [number of processes]``. 
We also have included some micro-benchmark tests, which work with ``pip install pytest-benchmark``.

**Fixtures:**

There are some pytest fixtures defined in the conftest file, which are for customization on backends and dtype in function level.
``highp`` is a fixture for complex128 simulation. While ``npb``, ``tfb``, ``jaxb`` and ``torchb`` are fixtures for global numpy, tensorflow, jax and pytorch backends, respectively.
To test different backends in one function, we need to use the parameterized fixture, which is enabled by ``pip install pytest-lazy-fixture``. Namely, we have the following approach to test different backends in one function.

.. code-block:: python

    from pytest_lazyfixture import lazy_fixture as lf

    @pytest.mark.parametrize("backend", [lf("npb"), lf("tfb"), lf("jaxb"), lf("torchb")])
    def test_parameterized_backend(backend):
        print(tc.backend.name)



Docs
--------

We use `sphinx <https://www.sphinx-doc.org/en/master/>`__ to manage the documentation.

The source files for docs are .rst file in docs/source.

For English docs, ``sphinx-build source build/html`` and ``make latexpdf LATEXMKOPTS="-silent"`` in docs dir are enough.
The html and pdf version of the docs are in docs/build/html and docs/build/latex, respectively.

**Formula Environment Attention**

It should be noted that the formula environment ``$$CONTENT$$`` in markdown is equivalent to the ``equation`` environment in latex.
Therefore, in the jupyter notebook documents, do not nest the formula environment in ``$$CONTENT$$`` that is incompatible with
``equation`` in latex, such as ``eqnarray``, which will cause errors in the pdf file built by ``nbsphinx``.
However, compatible formula environments can be used. For example, this legal code in markdown

.. code-block:: markdown

    $$
    \begin{split}
        X&=Y\\
        &=Z
    \end{split}
    $$

will be convert to

.. code-block:: latex

    \begin{equation}
        \begin{split}
            X&=Y\\
            &=Z
        \end{split}
    \end{equation}

in latex automatically by ``nbsphinx``, which is a legal latex code. However, this legal code in markdown

.. code-block:: markdown

    $$
    \begin{eqnarray}
        X&=&Y\\
        &=&Z
    \end{eqnarray}
    $$

will be convert to

.. code-block:: latex

    \begin{equation}
        \begin{eqnarray}
            X&=&Y\\
            &=&Z
        \end{eqnarray}
    \end{equation}

in latex, which is an illegal latex code.

**Auto Generation of API Docs:**

We utilize a python script to generate/refresh all API docs rst files under /docs/source/api based on the codebase /tensorcircuit.

.. code-block:: bash

    cd docs/source
    python generate_rst.py

**i18n:**

For Chinese docs, we refer to the standard i18n workflow provided by sphinx, see `here <https://www.sphinx-doc.org/en/master/usage/advanced/intl.html>`__.

To update the po file from updated English rst files, using

.. code-block:: bash

    cd docs
    make gettext
    sphinx-intl update -p build/gettext -l zh


Edit these .po files to add translations (`poedit <https://poedit.net/>`__ recommended). These files are in docs/source/locale/zh/LC_MESSAGES.

To generate the Chinese version of the documentation: ``sphinx-build source -D language="zh" build/html_cn`` which is in the separate directory ``.../build/html_cn/index.html``, whereas English version is in the directory ``.../build/html/index.html``.


Releases
------------

Firstly, ensure that the version numbers in __init__.py and CHANGELOG are correctly updated.

**GitHub Release**

.. code-block:: bash

    git tag v0.x.y 
    git push origin v0.x.y
    # assume origin is the upstream name

And from GitHub page choose draft a release from tag.

**PyPI Release**

.. code-block:: bash

    python -m build
    export VERSION=0.x.y
    twine upload dist/tensorcircuit_ng-${VERSION}-py3-none-any.whl dist/tensorcircuit_ng-${VERSION}.tar.gz

For upload authetication via token, please refer `this tutorial <https://kynan.github.io/blog/2020/05/23/how-to-upload-your-package-to-the-python-package-index-pypi-test-server>`__ .
Latest version of twine direct accepts token.


**DockerHub Release**

Make sure the DockerHub account is logged in via ``docker login``.

.. code-block:: bash

    sudo docker build . -f docker/Dockerfile -t tensorcircuit
    sudo docker tag tensorcircuit:latest tensorcircuit/tensorcircuit:0.x.y
    sudo docker push tensorcircuit/tensorcircuit:0.x.y
    sudo docker tag tensorcircuit:latest tensorcircuit/tensorcircuit:latest
    sudo docker push tensorcircuit/tensorcircuit:latest

**Binder Release**

One may need to update the tensorcirucit version for binder environment by pushing new commit in refraction-ray/tc-env repo with new version update in its ``requriements.txt``.
See `mybind setup <https://discourse.jupyter.org/t/tip-speed-up-binder-launches-by-pulling-github-content-in-a-binder-link-with-nbgitpuller/922>`_ for speed up via nbgitpuller. 


================================================
FILE: docs/source/faq.rst
================================================
Frequently Asked Questions
============================

What is the relation between TensorCircuit and TensorCircuit-NG?
-------------------------------------------------------------------

Both packages are created by `Shi-Xin Zhang <https://www.iop.cas.cn/rcjy/tpyjy/?id=6789>`_ (`@refraction-ray <https://github.com/refraction-ray>`_). For the history of the evolution of TensorCircuit-NG, please refer to `history <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/HISTORY.md>`_.

From users' perspective, TensorCircuit-NG maintains full compatibility with the TensorCircuit API, enhancing it with additional features and critical bug fixes. Only TensorCircuit-NG is kept up-to-date with the fast evolving scientific computing and machine learning ecosystem in Python.

TensorCircuit-NG is intended as a drop-in replacement for TensorCircuit, namely, by simply ``pip uninstall tensorcircuit`` and ``pip install tensorcircuit-ng``, your existing applications should continue to function seamlessly without requiring any modification to the codebase (``import tensorcircuit`` still works).



How can I run TensorCircuit-NG on GPU?
-----------------------------------------

This is done directly through the ML backend. GPU support is determined by whether ML libraries are can run on GPU, we don't handle this within tensorcircuit-ng.
It is the users' responsibility to configure a GPU-compatible environment for these ML packages. Please refer to the installation documentation for these ML packages and directly use the official dockerfiles provided by TensorCircuit-NG.

- TensorFlow: ``pip install "tensorflow[and-cuda]"``

- Jax: ``pip install -U "jax[cuda12]"``

With GPU compatible environment, we can switch the use of GPU or CPU by a backend agnostic environment variable ``CUDA_VISIBLE_DEVICES``.


When should I use GPU?
----------------------------------------------------

In general, for a circuit with qubit count larger than 16 or for circuit simulation with large batch dimension more than 16, GPU simulation will be faster than CPU simulation.
That is to say, for very small circuits and the very small batch dimensions of vectorization, GPU may show worse performance than CPU.
But one have to carry out detailed benchmarks on the hardware choice, since the performance is determined by the hardware and task details.

For tensor network tasks of more regular shape, such as MPS-MPO contraction, GPU can be much more favored and efficient than CPU.


How can I use multiple GPUs?
----------------------------------------------------

For different observables evaluation on different cards, see `example <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/vqe_parallel_pmap.py>`_.

For distributed simulation of one circuit on multiple cards, see `example for expectation <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/slicing_auto_pmap_vqa.py>`_ and `example for MPO <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/slicing_auto_pmap_mpo.py>`_.
We also introduce a new interface for the multi-GPU tensornetwork contraction, see `example for VQE <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/distributed_interface_vqe.py>`_ and `example for amplitude <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/distributed_interface_amplitude.py>`_.


When should I jit the function?
----------------------------------------------------

For a function with "tensor in and tensor out", wrapping it with jit will greatly accelerate the evaluation. Since the first time of evaluation takes longer time (staging time), jit is only good for functions which have to be evaluated frequently.


.. Warning::

    Be caution that jit can be easily misused if the users are not familiar with jit mechanism, which may lead to:
    
        1. very slow performance due to recompiling/staging for each run, 
        2. error when run function with jit, 
        3. or wrong results without any warning.

    The most possible reasons for each problem are:
    
        1. function input are not all in the tensor form,
        2. the output shape of all ops in the function may require the knowledge of the input value more than the input shape, or use mixed ops from numpy and ML framework
        3. subtle interplay between random number generation and jit (see :ref:`advance:Randoms, Jit, Backend Agnostic, and Their Interplay` for the correct solution), respectively.


Which ML framework backend should I use?
--------------------------------------------

Since the Numpy backend has no support for AD, if you want to evaluate the circuit gradient, you must set the backend as one of the ML frameworks beyond Numpy.

Since PyTorch has very limited support for vectorization and jit while our package strongly depends on these features, it is not recommended to use. Though one can always wrap a quantum function on another backend using a PyTorch interface, say :py:meth:`tensorcircuit.interfaces.torch_interface`.

In terms of the choice between TensorFlow and Jax backend, the better one may depend on the use cases and one may want to benchmark both to pick the better one. There is no one-for-all recommendation and this is why we maintain the backend agnostic form of our software.

Some general rules of thumb:

* On both CPU and GPU, the running time of a jitted function is faster for jax backend.

* But on GPU, jit staging time is usually much longer for jax backend.

* For hybrid machine learning tasks, TensorFlow has a better ML ecosystem and reusable classical ML models.

* Jax has some built-in advanced features that are lacking in TensorFlow, such as checkpoint in AD and pmap for distributed computing.

* Jax is much insensitive to dtype where type promotion is handled automatically which means easier debugging.

* TensorFlow can cache the jitted function on the disk via SavedModel, which further amortizes the staging time.


What is the counterpart of ``QuantumLayer`` for PyTorch and Jax backend?
----------------------------------------------------------------------------

Since PyTorch doesn't have mature vmap and jit support and Jax doesn't have native classical ML layers, we highly recommend TensorFlow as the backend for quantum-classical hybrid machine learning tasks, where ``QuantumLayer`` plays an important role.
For PyTorch, we can in principle wrap the corresponding quantum function into a PyTorch module, we currently have the built-in support for this wrapper as ``tc.TorchLayer``.
In terms of the Jax backend, we highly suggested keeping the functional programming paradigm for such machine learning tasks.
Besides, it is worth noting that, jit and vmap are automatically taken care of in ``QuantumLayer``.

When do I need to customize the contractor and how?
------------------------------------------------------

As a rule of thumb, for the circuit with qubit counts larger than 16 and circuit depth larger than 8, customized contraction may outperform the default built-in greedy contraction strategy.

To set up or not set up the customized contractor is about a trade-off between the time on contraction pathfinding and the time on the real contraction via matmul.

The customized contractor costs much more time than the default contractor in terms of contraction path searching, and via the path it finds, the real contraction can take less time and space.

If the circuit simulation time is the bottleneck of the whole workflow, one can always try customized contractors to see whether there is some performance improvement.

We recommend to using `cotengra library <https://cotengra.readthedocs.io/en/latest/index.html>`_ to set up the contractor, since there are lots of interesting hyperparameters to tune, we can achieve a better trade-off between the time on contraction path search and the time on the real tensor network contraction.

It is also worth noting that for jitted function which we usually use, the contraction path search is only called at the first run of the function, which further amortizes the time and favors the use of a highly customized contractor.

In terms of how-to on contractor setup, please refer to :ref:`quickstart:Setup the Contractor`.

Is there some API less cumbersome than ``expectation`` for Pauli string?
----------------------------------------------------------------------------

Say we want to measure something like :math:`\langle X_0Z_1Y_2Z_4 \rangle` for a six-qubit system, the general ``expectation`` API may seem to be cumbersome.
So one can try one of the following options:

* ``c.expectation_ps(x=[0], y=[2], z=[1, 4])`` 

* ``tc.templates.measurements.parameterized_measurements(c, np.array([1, 3, 2, 0, 3, 0]), onehot=True)``

Can I apply quantum operation based on previous classical measurement results?
----------------------------------------------------------------------------------------------------

Try the following: (the pipeline is even fully jittable!)

.. code-block:: python

    c = tc.Circuit(2)
    c.H(0)
    r = c.cond_measurement(0)
    c.conditional_gate(r, [tc.gates.i(), tc.gates.x()], 1)

``cond_measurement`` will return 0 or 1 based on the measurement result on z-basis, and ``conditional_gate`` applies gate_list[r] on the circuit.

How to understand the difference between different measurement methods for ``Circuit``?
----------------------------------------------------------------------------------------------------

* :py:meth:`tensorcircuit.circuit.Circuit.measure` : used at the end of the circuit execution, return bitstring based on quantum amplitude probability (can also with the probability), the circuit and the output state are unaffected (no collapse). The jittable version is ``measure_jit``.

* :py:meth:`tensorcircuit.circuit.Circuit.cond_measure`: also with alias ``cond_measurement``, usually used in the middle of the circuit execution. Apply a POVM on z basis on the given qubit, the state is collapsed and nomarlized based on the measurement projection. The method returns an integer Tensor indicating the measurement result 0 or 1 based on the quantum amplitude probability. 

* :py:meth:`tensorcircuit.circuit.Circuit.post_select`: also with alia ``mid_measurement``, usually used in the middle of the circuit execution. The measurement result is fixed as given from ``keep`` arg of this method. The state is collapsed but unnormalized based on the given measurement projection.

Please refer to the following demos:

.. code-block:: python

    c = tc.Circuit(2)
    c.H(0)
    c.H(1)
    print(c.measure(0, 1))
    # ('01', -1.0)
    print(c.measure(0, with_prob=True))
    # ('0', (0.4999999657714588+0j))
    print(c.state()) # unaffected
    # [0.49999998+0.j 0.49999998+0.j 0.49999998+0.j 0.49999998+0.j]

    c = tc.Circuit(2)
    c.H(0)
    c.H(1)
    print(c.cond_measure(0))  # measure the first qubit return +z
    # 0
    print(c.state())  # collapsed and normalized
    # [0.70710678+0.j 0.70710678+0.j 0.        +0.j 0.        +0.j]

    c = tc.Circuit(2)
    c.H(0)
    c.H(1)
    print(c.post_select(0, keep=1))  # measure the first qubit and it is guranteed to return -z
    # 1
    print(c.state())  # collapsed but unnormalized
    # [0.        +0.j 0.        +0.j 0.49999998+0.j 0.49999998+0.j]


How to understand difference between ``tc.array_to_tensor`` and ``tc.backend.convert_to_tensor``?
------------------------------------------------------------------------------------------------------

``tc.array_to_tensor`` convert array to tensor as well as automatically cast the type to the default dtype of TensorCircuit-NG,
i.e. ``tc.dtypestr`` and it also support to specify dtype as ``tc.array_to_tensor( , dtype="complex128")``.
Instead, ``tc.backend.convert_to_tensor`` keeps the dtype of the input array, and to cast it as complex dtype, we have to
explicitly call ``tc.backend.cast`` after conversion. Besides, ``tc.array_to_tensor`` also accepts multiple inputs as
``a_tensor, b_tensor = tc.array_to_tensor(a_array, b_array)``.


How to arrange the circuit gate placement in the visualization from ``c.tex()``?
----------------------------------------------------------------------------------------------------

Try ``lcompress=True`` or ``rcompress=True`` option in :py:meth:`tensorcircuit.circuit.Circuit.tex` API to make the circuit align from the left or from the right.

Or try ``c.unitary(0, unitary=tc.backend.eye(2), name="invisible")`` to add placeholder on the circuit which is invisible for circuit visualization.


How many different formats for the circuit sample results?
--------------------------------------------------------------------------

When performing measurements or sampling in TensorCircuit-NG, there are six different formats available for the results:

1. ``"sample_int"``
    Returns measurement results as integer array.

    .. code-block:: python

        >>> c = tc.Circuit(2)
        >>> c.h(0)
        >>> c.sample(batch=3, format="sample_int")
        array([0, 2, 0])  # Each number represents a measurement outcome

2. ``"sample_bin"``
    Returns measurement results as a list of binary arrays.

    .. code-block:: python

        >>> c.sample(batch=3, format="sample_bin")
        Array([[0, 0],
                [1, 0],
                [1, 0]], dtype=int32)  # Each sub array represents a binary string

3. ``"count_vector"``
    Returns counts as a vector where index represents the state.

    .. code-block:: python

        >>> c.sample(batch=3, format="count_vector")
        Array([1, 0, 2, 0], dtype=int32)  # [#|00⟩, #|01⟩, #|10⟩, #|11⟩]

4. ``"count_tuple"``
    Returns counts as a tuple of indices and their frequencies.

    .. code-block:: python

        >>> c.sample(batch=4, format="count_tuple", jittable=False)
        (Array([0, 2], dtype=int32), Array([2, 1], dtype=int32))  # (int_states, frequencies)

5. ``"count_dict_bin"``
    Returns counts as a dictionary with binary strings as keys.

    .. code-block:: python

        >>> c.sample(batch=4, format="count_dict_bin")
        {"00": 2, "01": 0, "10": 2, "11": 0}

6. ``"count_dict_int"``
    Returns counts as a dictionary with integers as keys.

    .. code-block:: python

        >>> c.sample(batch=4, format="count_dict_int")
        {0: 2, 1: 0, 2: 2, 3: 0}  # {state_integer: frequency}


For more input parameters, see API doc :py:meth:`tensorcircuit.circuit.Circuit.sample`.


How to get the entanglement entropy from the circuit output?
--------------------------------------------------------------------

Try the following:

.. code-block:: python

    c = tc.Circuit(4)
    # omit circuit construction

    rho = tc.quantum.reduced_density_matrix(s, cut=[0, 1, 2])
    # get the redueced density matrix, where cut list is the index to be traced out

    rho.shape
    # (2, 2)

    ee = tc.quantum.entropy(rho)
    # get the entanglement entropy

    renyi_ee = tc.quantum.renyi_entropy(rho, k=2)
    # get the k-th order renyi entropy


================================================
FILE: docs/source/generate_rst.py
================================================
import os
import glob
import shutil
from os.path import join as pj


class RSTGenerator:
    title_line = "=" * 80
    toctree = ".. toctree::\n    {}"
    automodule = ".. automodule:: {}\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :inherited-members:"

    def __init__(
        self, package_name, docs_folder, package_folder, ignore_modules=["__init__"]
    ):
        self.name = package_name
        self.dfolder = docs_folder
        self.pfolder = package_folder
        self.ingnored_modules = set(ignore_modules)
        self.tree = {}

    def cleanup(self):
        if os.path.exists("modules.rst"):
            os.remove("modules.rst")
        try:
            shutil.rmtree(self.dfolder)
        except FileNotFoundError:
            pass
        os.makedirs(self.dfolder)

    def write(self, path, content):
        if isinstance(content, list):
            content = "\n".join(content)

        with open(path, "w") as f:
            f.write(content.replace("\\", r"/"))

        print(f"Finish writing {path}")

    def _file_generate(self, package_parents):
        file_list = []
        for module_name in glob.glob(pj(self.pfolder, *package_parents, "*.py")):
            module_name = os.path.basename(module_name)[:-3]
            if module_name in self.ingnored_modules:
                continue

            rst_file = pj(self.dfolder, *package_parents, f"{module_name}.rst")
            name = f"{self.name}"
            for n in package_parents:
                name += f".{n}"
            name += f".{module_name}"
            content = [
                name,
                self.title_line,
                self.automodule.format(name),
            ]
            self.write(rst_file, content)
            if not package_parents:
                upper = self.dfolder
            else:
                upper = package_parents[-1]
            file_list.append(upper + f"/{module_name}.rst")
        for subdir in glob.glob(pj(self.pfolder, *package_parents, "*/")):
            if "_" in subdir:
                continue
            subdir = os.path.basename(os.path.normpath(subdir))
            os.makedirs(pj(self.dfolder, *package_parents, subdir), exist_ok=True)
            rst_file = pj(self.dfolder, *package_parents, f"{subdir}.rst")
            subdir_filelist = self._file_generate(package_parents + [subdir])

            name = f"{self.name}"
            for n in package_parents:
                name += f".{n}"
            name += f".{subdir}"
            content = [
                name,
                self.title_line,
                self.toctree.format("\n    ".join(sorted(subdir_filelist))),
            ]
            self.write(rst_file, content)

            if not package_parents:
                upper = self.dfolder
            else:
                upper = package_parents[-1]
            file_list.append(upper + f"/{subdir}.rst")
        return file_list

    def modules_file(self, file_list):
        """Write the modules.rst"""
        content = [
            self.name,
            self.title_line,
            self.toctree.format("\n    ".join(sorted(file_list))),
        ]
        self.write("modules.rst", content)

    def start(self):
        self.cleanup()
        file_list = self._file_generate([])
        self.modules_file(file_list)


if __name__ == "__main__":
    # All path must be relative path to the folder of moduels.rst
    RSTGenerator(
        "tensorcircuit",
        "./api",
        "../../tensorcircuit",
        [
            "__init__",
            "abstract_backend",
            "tf_ops",
            "jax_ops",
            "pytorch_ops",
            "asciiart",
        ],
    ).start()



================================================
FILE: docs/source/index.rst
================================================
TensorCircuit Next Generation
===========================================================

.. image:: https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/docs/source/statics/logong.png?raw=true
    :target: https://github.com/tensorcircuit/tensorcircuit-ng


**Welcome and congratulations! You have found TensorCircuit: the Next Generation.** 👏 


Introduction
---------------

`TensorCircuit-NG <https://github.com/tensorcircuit/tensorcircuit-ng>`_ is an open-source high-performance quantum software framework in Python.

* It is built for humans. 👽

* It is designed for speed, flexibility and elegance. 🚀

* It is empowered by advanced tensor network simulator engines. 🔋

* It is ready for quantum hardware access with CPU/GPU/QPU (local/cloud) hybrid solutions. 🖥

* It is implemented with industry-standard machine learning frameworks: TensorFlow, JAX, and PyTorch. 🤖

* It is flexible and powerful to build and simulate tensor networks, neural networks and quantum circuits together. 🧠

* It is compatible with machine learning engineering paradigms: automatic differentiation, just-in-time compilation, vectorized parallelism and GPU acceleration. 🛠

With the help of TensorCircuit-NG, now get ready to efficiently and elegantly solve interesting and challenging quantum computing and quantum many-body problems: from academic research prototype to industry application deployment.

.. important::
   Please cite the `whitepaper <https://quantum-journal.org/papers/q-2023-02-02-912/>`_ when using TensorCircuit or TensorCircuit-NG in your research. The bibtex information is provided by ``tc.cite()``.

.. note::
   The TensorCircuit package is outdated. 
   We recommend upgrading to TensorCircuit-NG for the latest features and improvements. 
   You can upgrade by running the following command:
   ``pip uninstall tensorcircuit && pip install tensorcircuit-ng``


Useful Links
--------------------


TensorCircuit is created and now maintained as 
`TensorCircuit-NG <https://github.com/tensorcircuit/tensorcircuit-ng>`_ by `Shi-Xin Zhang <https://github.com/refraction-ray>`_.

The current core authors of TensorCircuit-NG are `Shi-Xin Zhang <https://github.com/refraction-ray>`_ and `Yu-Qin Chen <https://github.com/yutuer21>`_.
We also thank `contributions <https://github.com/tensorcircuit/tensorcircuit-ng/graphs/contributors>`_ from the open source community.

If you have any further questions or collaboration ideas, please use the issue tracker or forum below, or send email to shixinzhang#iphy.ac.cn


.. card-carousel:: 2

   .. card:: Source code
      :link: https://github.com/tensorcircuit/tensorcircuit-ng
      :shadow: md

      GitHub

   
   .. card:: PyPI
      :link:  https://pypi.org/project/tensorcircuit-ng
      :shadow: md

      ``pip install tensorcircuit-ng``


   .. card:: Documentation
      :link: https://tensorcircuit-ng.readthedocs.io
      :shadow: md

      Readthedocs


   .. card:: Whitepaper
      :link: https://quantum-journal.org/papers/q-2023-02-02-912/
      :shadow: md

      *Quantum* journal


   .. card:: Issue Tracker
      :link: https://github.com/tensorcircuit/tensorcircuit-ng/issues
      :shadow: md

      GitHub Issues


   .. card:: Forum
      :link: https://github.com/tensorcircuit/tensorcircuit-ng/discussions
      :shadow: md

      GitHub Discussions



   .. card:: DockerHub
      :link: https://hub.docker.com/repository/docker/tensorcircuit/tensorcircuit
      :shadow: md

      ``docker pull``
      

   .. card:: Application
      :link: https://github.com/tensorcircuit/tensorcircuit-ng#research-and-applications
      :shadow: md

      Research using TC





Unified Quantum Programming
------------------------------

TensorCircuit-NG is unifying infrastructures and interfaces for quantum computing.

.. grid:: 1 2 4 4
   :margin: 0
   :padding: 0
   :gutter: 2

   .. grid-item-card:: Unified Backends
      :columns: 12 6 3 3
      :shadow: md

      Jax/TensorFlow/PyTorch/Numpy/Cupy

   .. grid-item-card:: Unified Devices
      :columns: 12 6 3 3
      :shadow: md

      CPU/GPU/TPU

   .. grid-item-card:: Unified Providers
      :columns: 12 6 3 3
      :shadow: md

      QPUs from different vendors

   .. grid-item-card:: Unified Resources
      :columns: 12 6 3 3
      :shadow: md

      local/cloud/HPC


.. grid:: 1 2 4 4
   :margin: 0
   :padding: 0
   :gutter: 2

   .. grid-item-card:: Unified Interfaces
      :columns: 12 6 3 3
      :shadow: md

      numerical sim/hardware exp

   .. grid-item-card:: Unified Engines
      :columns: 12 6 3 3
      :shadow: md

      ideal/noisy/approx/analog/stabilizer

   .. grid-item-card:: Unified Representations
      :columns: 12 6 3 3
      :shadow: md

      from/to_IR/qiskit/openqasm/json

   .. grid-item-card:: Unified Objects
      :columns: 12 6 3 3
      :shadow: md

      neural-net/tensor-net/quantum-circuit




Reference Documentation
----------------------------

The following documentation sections briefly introduce TensorCircuit-NG to the users and developpers.

.. toctree::
   :maxdepth: 2

   quickstart.rst
   advance.rst
   faq.rst
   sharpbits.rst
   infras.rst
   contribution.rst

Tutorials
---------------------

The following documentation sections include integrated examples in the form of Jupyter Notebook.

.. toctree-filt::
   :maxdepth: 2

   :zh:tutorial.rst
   :zh:whitepapertoc.rst
   :en:tutorial_cn.rst
   :en:whitepapertoc_cn.rst
   :en:textbooktoc.rst



API References
=======================

.. toctree::
   :maxdepth: 2
    
   modules.rst
    

Indices and Tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`



================================================
FILE: docs/source/infras.rst
================================================
=================================
TensorCircuit-NG: What is inside?
=================================

This part of the documentation is mainly for advanced users and developers who want to learn more about what happened behind the scene and delve into the codebase.


Overview of Modules
-----------------------

**Core Modules:**

- :py:mod:`tensorcircuit.abstractcircuit` and :py:mod:`tensorcircuit.basecircuit`: Hierarchical abstraction of circuit class.

- :py:mod:`tensorcircuit.circuit`: The core object :py:obj:`tensorcircuit.circuit.Circuit`. It supports circuit construction, simulation, representation, and visualization without noise or with noise using the Monte Carlo trajectory approach.

- :py:mod:`tensorcircuit.cons`: Runtime ML backend, dtype and contractor setups. We provide three sets of set methods for global setup, function level setup using function decorators, and context setup using ``with`` context managers. We also include customized contractor infrastructures in this module.

- :py:mod:`tensorcircuit.gates`: Definition of quantum gates, either fixed ones or parameterized ones, as well as :py:obj:`tensorcircuit.gates.GateF` class for gates.

**Backend Agnostic Abstraction:**

- :py:mod:`tensorcircuit.backends` provides a set of backend API and the corresponding implementation on Numpy, Jax, TensorFlow, and PyTorch backends. These backends are inherited from the TensorNetwork package and are highly customized.

**Noisy Simulation Related Modules:**

- :py:mod:`tensorcircuit.channels`: Definition of quantum noise channels.

- :py:mod:`tensorcircuit.densitymatrix`: Referenced and highly efficient implementation of ``tc.DMCircuit`` class, with similar set API of ``tc.Circuit`` while simulating the noise in the full form of the density matrix.

- :py:mod:`tensorcircuit.noisemodel`: The global noise configuration and circuit noisy method APIs

**ML Interfaces Related Modules:**

- :py:mod:`tensorcircuit.interfaces`: Provide interfaces when quantum simulation backend is different from neural libraries. Currently include PyTorch, TensorFlow, NumPy and SciPy optimizer interfaces.

- :py:mod:`tensorcircuit.keras`: Provide TensorFlow Keras layers, as well as wrappers of jitted function, save/load from tf side.

- :py:mod:`tensorcircuit.torchnn`: Provide PyTorch nn Modules.

**MPS and MPO Utiliy Modules:**

- :py:mod:`tensorcircuit.quantum`: Provide definition and classes for Matrix Product States as well as Matrix Product Operators, we also include various quantum physics and quantum information quantities in this module.

**MPS Based Simulator Modules:**

- :py:mod:`tensorcircuit.mps_base`: Customized and jit/AD compatible MPS class from TensorNetwork package.

- :py:mod:`tensorcircuit.mpscircuit`: :py:obj:`tensorcircuit.mpscircuit.MPSCircuit` class with similar (but subtly different) APIs as ``tc.Circuit``, where the simulation engine is based on MPS TEBD.

**Supplemental Modules:**

- :py:mod:`tensorcircuit.simplify`: Provide tools and utility functions to simplify the tensornetworks before the real contractions.

- :py:mod:`tensorcircuit.experimental`: Experimental functions, long and stable support is not guaranteed.

- :py:mod:`tensorcircuit.utils`: Some general function tools that are not quantum at all.

- :py:mod:`tensorcircuit.vis`: Visualization code for circuit drawing.

- :py:mod:`tensorcircuit.translation`: Translate circuit object to circuit object in other quantum packages.

**Processing and error mitigation on sample results:**

- :py:mod:`tensorcircuit.results`: Provide tools to process count dict and to apply error mitigation.

**Cloud quantum hardware access module:**

- :py:mod:`tensorcircuit.cloud`: Provide quantum cloud SDK that can access and program the real quantum hardware.

- :py:mod:`tensorcircuit.compiler`: Provide compiler chains to compile and transform quantum circuits.

**Shortcuts and Templates for Circuit Manipulation:**

- :py:mod:`tensorcircuit.templates`: provide handy shortcuts functions for expectation or circuit building patterns.

**Applications:**

- :py:mod:`tensorcircuit.applications`: most code here is not maintained and deprecated, use at your own risk.

.. note::

    Recommend reading order -- only read the part of code you care about for your purpose. 
    If you want to get an overview of the codebase, please read ``tc.circuit`` followed by ``tc.cons`` and ``tc.gates``.


Relation between TensorCircuit-NG and TensorNetwork-NG
--------------------------------------------------------

TensorCircuit has a strong connection with the `TensorNetwork package <https://github.com/google/TensorNetwork>`_ released by Google. Since the TensorNetwork package has poor documentation and tutorials, most of the time, we need to delve into the codebase of TensorNetwork to figure out what happened. In other words, to read the TensorCircuit codebase, one may have to frequently refer to the TensorNetwork codebase. As TensorNetwork package is not maintained anymore, we rely on the TensorNetwork-NG package maintained by the same authors of TensorCircuit-NG.

Inside TensorCircuit-NG, we heavily utilize TensorNetwork-related APIs from the TensorNetwork package and highly customized several modules from TensorNetwork by inheritance and rewriting:

- We implement our own /backends from TensorNetwork's /backends by adding much more APIs and fixing lots of bugs in TensorNetwork's implementations on certain backends via monkey patching. (The upstream is inactive and not that responsive anyhow.)

- We borrow TensorNetwork's code in /quantum to our ``tc.quantum`` module, since TensorNetwork has no ``__init__.py`` file to export these MPO and MPS related objects. Of course, we have made substantial improvements since then.

- We borrow the TensorNetwork's code in /matrixproductstates as ``tc.mps_base`` for bug fixing and jit/AD compatibility, so that we have better support for our MPS based quantum circuit simulator.


Relations of Circuit-like classes
---------------------------------------

.. code-block::

                                           |- Circuit
                        |- BaseCircuit --- |
    AbstractCircuit  ---|                  |- DMCircuitReference --- DMCircuit
                        |- MPSCircuit



QuOperator/QuVector and MPO/MPS
---------------------------------------------------

:py:class:`tensorcircuit.quantum.QuOperator`, :py:class:`tensorcircuit.quantum.QuVector` and :py:class:`tensorcircuit.quantum.QuAdjointVector` are classes adopted from TensorNetwork package.
They behave like a matrix/vector (column or row) when interacting with other ingredients while the inner structure is maintained by the tensornetwork for efficiency and compactness.

We use code examples and associated tensor diagrams to illustrate these object abstractions.

.. note::

    ``QuOperator`` can express MPOs and ``QuVector`` can express MPSs, but they can express more than these fixed structured tensor networks.

.. code-block:: python

    import tensornetwork as tn

    n1 = tn.Node(np.ones([2, 2, 2]))
    n2 = tn.Node(np.ones([2, 2, 2]))
    n3 = tn.Node(np.ones([2, 2]))
    n1[2]^n2[2]
    n2[1]^n3[0]

    matrix = tc.quantum.QuOperator(out_edges=[n1[0], n2[0]], in_edges=[n1[1], n3[1]])

    n4 = tn.Node(np.ones([2]))
    n5 = tn.Node(np.ones([2]))

    vector = tc.quantum.QuVector([n4[0], n5[0]])

    nvector = matrix @ vector 

    assert type(nvector) == tc.quantum.QuVector
    nvector.eval_matrix() 
    # array([[16.], [16.], [16.], [16.]])

.. figure:: statics/quop.png
    :scale: 50%

Note how in this example, ``matrix`` is not a typical MPO but still can be expressed as ``QuOperator``. Indeed, any tensor network with two sets of dangling edges of the same dimension can be treated as ``QuOperator``. ``QuVector`` is even more flexible since we can treat all dangling edges as the vector dimension.

Also, note how ``^`` is overloaded as ``tn.connect`` to connect edges between different nodes in TensorNetwork. And indexing the node gives the edges of the node, eg. ``n1[0]`` means the first edge of node ``n1``.

The convention to define the ``QuOperator`` is firstly giving ``out_edges`` (left index or row index of the matrix) and then giving ``in_edges`` (right index or column index of the matrix). The edges list contains edge objects from the TensorNetwork library.

Such QuOperator/QuVector abstraction support various calculations only possible on matrix/vectors, such as matmul (``@``), adjoint (``.adjoint()``), scalar multiplication (``*``), tensor product (``|``), and partial trace (``.partial_trace(subsystems_to_trace_out)``).
To extract the matrix information of these objects, we can use ``.eval()`` or ``.eval_matrix()``, the former keeps the shape information of the tensor network while the latter gives the matrix representation with shape rank 2.


Quantum Cloud SDK: Layerwise API design
-----------------------------------------------------

From lower level to higher level, a view of API layers invoking QPU calls

- Vendor specific implementation of functional API in, e.g., :py:mod:`tensorcircuit.cloud.tencent`

- Provider agnostic functional lower level API for task/device management in :py:mod:`tensorcircuit.cloud.apis`

- Object oriented abstraction for Provider/Device/Task in :py:mod:`tensorcircuit.cloud.abstraction`

- Unified batch submission interface as standarized in :py:meth:`tensorcircuit.cloud.wrapper.batch_submit_template`

- Numerical and experimental unified all-in-one interface as :py:meth:`tensorcircuit.cloud.wrapper.batch_expectation_ps`

- Application level code with QPU calls built directly on ``batch_expectation_ps`` or more fancy algorithms can be built on ``batch_submit_func`` so that these algorithms can be reused as long as one function ``batch_submit_func`` is defined for a given vendor (cheaper than defining a new provider from lower level).


.. Note::

    For compiler, error mitigation and results post-processing parts, they can be carefully designed to decouple with the QPU calls,
    so they are separately implemented in :py:mod:`tensorcircuit.compiler` and :py:mod:`tensorcircuit.results`, 
    and they can be independently useful even without tc's cloud access.



================================================
FILE: docs/source/modules.rst
================================================
tensorcircuit
================================================================================
.. toctree::
    ./api/about.rst
    ./api/abstractcircuit.rst
    ./api/analogcircuit.rst
    ./api/applications.rst
    ./api/backends.rst
    ./api/basecircuit.rst
    ./api/channels.rst
    ./api/circuit.rst
    ./api/cloud.rst
    ./api/compiler.rst
    ./api/cons.rst
    ./api/densitymatrix.rst
    ./api/experimental.rst
    ./api/fgs.rst
    ./api/gates.rst
    ./api/interfaces.rst
    ./api/keras.rst
    ./api/mps_base.rst
    ./api/mpscircuit.rst
    ./api/noisemodel.rst
    ./api/quantum.rst
    ./api/quditcircuit.rst
    ./api/quditgates.rst
    ./api/results.rst
    ./api/shadows.rst
    ./api/simplify.rst
    ./api/stabilizercircuit.rst
    ./api/templates.rst
    ./api/timeevol.rst
    ./api/torchnn.rst
    ./api/translation.rst
    ./api/utils.rst
    ./api/vis.rst


================================================
FILE: docs/source/quickstart.rst
================================================
================
Quick Start
================

Installation
--------------

- For x86 Linux, 

``pip install tensorcircuit-ng`` 

is in general enough. 

Since there are many optional packages for various features, 
the users may need to install more packages by pip when required. 

For Nvidia GPU,
please refer to the GPU aware installation guide of corresponding machine learning frameworks: 
`TensorFlow <https://www.tensorflow.org/install/pip>`_ (``pip install 'tensorflow[and-cuda]'``), 
`Jax <https://docs.jax.dev/en/latest/installation.html#pip-installation-nvidia-gpu-cuda-installed-via-pip-easier>`_ (``pip install 'jax[cuda-12]'``), 
or `PyTorch <https://pytorch.org/get-started/locally/>`_.

Docker is also accessible: 

``sudo docker run -it --network host --gpus all tensorcircuit/tensorcircuit``.

For more details on docker setup, please refer to `docker readme <https://github.com/tensorcircuit/tensorcircuit-ng/tree/master/docker>`_.


Overall, the installation of TensorCircuit-NG is simple, since it is purely in Python and hence portable. 
As long as the users can take care of the installation of ML frameworks on the corresponding operating system, TensorCircuit-NG will work as expected.

To debug the installation issue or report bugs, please check the environment information by ``tc.about()``.

.. Note::
    We also provide a nightly build of tensorcircuit via PyPI which can be accessed by
    ``pip uninstall tensorcircuit-ng``, then
    ``pip install tensorcircuit-nightly``


Circuit Object
------------------

The basic object for TensorCircuit-NG is ``tc.Circuit``. 

Initialize the circuit with the number of qubits ``c=tc.Circuit(n)``.

**Input States:**

The default input function for the circuit is :math:`\vert 0^n \rangle`. One can change this to other wavefunctions by directly feeding the inputs state vectors w: ``c=tc.Circuit(n, inputs=w)``.

One can also feed matrix product states as input states for the circuit, but we leave MPS/MPO usage for future sections.

**Quantum Gates:**

We can apply gates on circuit objects. For example, using ``c.H(1)`` or ``c.rx(2, theta=0.2)``, we can apply Hadamard gate on qubit 1 (0-based) or apply Rx gate on qubit 2 as :math:`e^{-i\theta/2 X}`.

The same rule also applies to multi-qubit gates, such as ``c.cnot(0, 1)``.

There are also highly customizable gates, two instances are:

- ``c.exp1(0, 1, unitary=m, theta=0.2)`` which is for the exponential gate :math:`e^{i\theta m}` of any matrix m as long as :math:`m^2=1`.

- ``c.any(0, 1, unitary=m)`` which is for applying the unitary gate m on the circuit.

These two examples are flexible and support gates on any number of qubits.

**Measurements and Expectations:**

The most straightforward way to get the output from the circuit object is by getting the output wavefunction in vector form as ``c.state()``.

For bitstring sampling, we have ``c.perfect_sampling()`` which returns the bitstring and the corresponding probability amplitude.

To measure part of the qubits, we can use ``c.measure(0, 1)``, if we want to know the corresponding probability of the measurement output, try ``c.measure(0, 1, with_prob=True)``. The measure API is by default non-jittable, but we also have a jittable version as ``c.measure_jit(0, 1)``.

The measurement and sampling utilize advanced algorithms based on tensornetwork and thus require no knowledge or space for the full wavefunction.

See the example below:

.. code-block:: python

    K = tc.set_backend("jax")
    @K.jit
    def sam(key):
        K.set_random_state(key)
        n = 50
        c = tc.Circuit(n)
        for i in range(n):
            c.H(i)
        return c.perfect_sampling()

    sam(jax.random.PRNGKey(42))
    sam(jax.random.PRNGKey(43))


To compute expectation values for local observables, we have ``c.expectation([tc.gates.z(), [0]], [tc.gates.z(), [1]])`` for :math:`\langle Z_0Z_1 \rangle` or ``c.expectation([tc.gates.x(), [0]])`` for :math:`\langle X_0 \rangle`.

This expectation API is rather flexible, as one can measure an m on several qubits as ``c.expectation([m, [0, 1, 2]])``.

We can also extract the unitary matrix underlying the whole circuit as follows:

.. code-block:: python

    >>> n = 2
    >>> c = tc.Circuit(n, inputs=tc.backend.eye(2**n))
    >>> c.X(1)
    >>> tc.backend.reshapem(c.state())
    array([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],
        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],
        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],
        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], dtype=complex64)

**Circuit Transformations:**

We currently support transform ``tc.Circuit`` from and to Qiskit ``QuantumCircuit`` object.

Export to Qiskit (possible for further hardware experiment, compiling, and visualization): ``c.to_qiskit()``.

Import from Qiskit: ``c = tc.Circuit.from_qiskit(QuantumCircuit, n)``.
Parameterized Qiskit circuit is supported by passing the parameters to the ``binding_parameters`` argument
of the ``from_qiskit`` function, similar to the ``assign_parameters`` function in Qiskit.

**Circuit Visualization:** 

``c.vis_tex()`` can generate tex code for circuit visualization based on LaTeX `quantikz <https://arxiv.org/abs/1809.03842>`__ package.

There are also some automatic pipeline helper functions to directly generate figures from tex code, but they require extra installations in the environment.

``render_pdf(tex)`` function requires full installation of LaTeX locally. And in the Jupyter environment, we may prefer ``render_pdf(tex, notebook=True)`` to return jpg figures, which further require wand magicwand library installed, see `here <https://docs.wand-py.org/en/latest/>`__.

Or since we can transform ``tc.Circuit`` into QuantumCircuit easily, we have a simple pipeline to first transform ``tc.Circuit`` into Qiskit and then call the visualization built in Qiskit. Namely, we have ``c.draw()`` API.

**Circuit Intermediate Representation:**

TensorCircuit provides its own circuit IR as a python list of dicts. This IR can be further utilized to run compiling, generate serialization qasm, or render circuit figures.

The IR is given as a list, each element is a dict containing information on one gate that is applied to the circuit. Note gate attr in the dict is a python function that returns the gate's node.

.. code-block:: python

    >>> c = tc.Circuit(2)
    >>> c.cnot(0, 1)
    >>> c.crx(1, 0, theta=0.2)
    >>> c.to_qir()
    [{'gate': cnot, 'index': (0, 1), 'name': 'cnot', 'split': None}, {'gate': crx, 'index': (1, 0), 'name': 'crx', 'split': None, 'parameters': {'theta': 0.2}}]

We can also create new copied circuit via ``c.copy()`` which internally utilize the ``qir``.


Programming Paradigm
-------------------------

The most common case and the most typical programming paradigm for TensorCircuit-NG are to evaluate the circuit output and the corresponding quantum gradients, which is common in variational quantum algorithms.

.. code-block:: python

    import tensorcircuit as tc

    K = tc.set_backend("tensorflow")

    n = 1


    def loss(params, n):
        c = tc.Circuit(n)
        for i in range(n):
            c.rx(i, theta=params[0, i])
        for i in range(n):
            c.rz(i, theta=params[1, i])
        loss = 0.0
        for i in range(n):
            loss += c.expectation([tc.gates.z(), [i]])
        return K.real(loss)


    vgf = K.jit(K.value_and_grad(loss), static_argnums=1)
    params = K.implicit_randn([2, n])
    print(vgf(params, n))  # get the quantum loss and the gradient

Also for a non-quantum example (linear regression) demonstrating the backend agnostic feature, variables with pytree support, AD/jit/vmap usage, and variational optimization loops. Please refer to the example script: `linear regression example <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/universal_lr.py>`_.
This example might be more friendly to the machine learning community since it is purely classical while also showcasing the main features and paradigms of tensorcircuit-ng.

If the user has no intention to maintain the application code in a backend agnostic fashion, the API for ML frameworks can be more handily used and interleaved with the TensorCircuit API.

.. code-block:: python

    import tensorcircuit as tc
    import tensorflow as tf

    K = tc.set_backend("tensorflow")

    n = 1


    def loss(params, n):
        c = tc.Circuit(n)
        for i in range(n):
            c.rx(i, theta=params[0, i])
        for i in range(n):
            c.rz(i, theta=params[1, i])
        loss = 0.0
        for i in range(n):
            loss += c.expectation([tc.gates.z(), [i]])
        return tf.math.real(loss)

    def vgf(params, n):
        with tf.GradientTape() as tape:
            tape.watch(params)
            l = loss(params, n)
        return l, tape.gradient(l, params)

    vgf = tf.function(vgf)
    params = tf.random.normal([2, n])
    print(vgf(params, n))  # get the quantum loss and the gradient


Automatic Differentiation, JIT, and Vectorized Parallelism
-------------------------------------------------------------

For concepts of AD, JIT and VMAP, please refer to `Jax documentation <https://docs.jax.dev/en/latest/tutorials.html>`__ .

The related API design in TensorCircuit-NG closely follows the functional programming design pattern in Jax with some slight differences. So we strongly recommend users learn some basics about Jax no matter which ML backend they intend to use.

**AD Support:**

Automatic Differentiation (AD) is crucial for quantum circuit optimization. TensorCircuit-NG supports various differentiation operations:

* Gradients: First-order derivatives
* Vector-Jacobian products (vjps): Efficient backward-mode differentiation
* Jacobian-vector products (jvps): Efficient forward-mode differentiation
* Natural gradients: Geometry-aware optimization
* Jacobians: Full derivative matrices
* Hessians: Second-order derivatives

Example of gradient computation:

.. code-block:: python

    import tensorcircuit as tc
    K = tc.set_backend("tensorflow")

    def circuit(params):
        c = tc.Circuit(2)
        c.rx(0, theta=params[0])
        c.ry(1, theta=params[1])
        c.cnot(0, 1)
        return K.real(c.expectation([tc.gates.z(), [0]]))

    # Get value and gradient
    params = K.ones([2])
    value, grad = K.value_and_grad(circuit)(params)
    print("Value:", value)
    print("Gradient:", grad)

    # Compute Hessian
    hess = K.hessian(circuit)(params)
    print("Hessian:", hess)

**JIT Support:**

Just-In-Time (JIT) compilation significantly accelerates quantum circuit simulation by optimizing the computation graph. Key points:

* Use JIT for functions that will be called multiple times
* JIT compilation has some overhead, so it's most beneficial for repeated executions
* Ensure input shapes and types are consistent to avoid recompilation
* The input and output of the functions are all tensors, except static inputs.

Example of JIT acceleration:

.. code-block:: python

    import time
    
    # Define a quantum circuit function
    def noisy_circuit(key):
        c = tc.Circuit(5)
        for i in range(5):
            c.h(i)
            c.depolarizing(i, px=0.01, py=0.01, pz=0.01, status=key[i])
        return c.expectation_ps(z=[0])

    # Compare performance with and without JIT
    start = time.time()
    for _ in range(100):
        noisy_circuit(K.ones([5]))
    print("Without JIT:", time.time() - start)

    jitted_circuit = K.jit(noisy_circuit)
    start = time.time()
    for _ in range(100):
        jitted_circuit(K.ones([5]))
    print("With JIT:", time.time() - start)

**VMAP Support:**

Vectorized mapping (vmap) enables parallel evaluation across multiple inputs or parameters:

* Batch processing of quantum circuit input wavefunctions
* Batch processing quantum circuit structure
* Parallel parameter optimization
* Efficient Monte Carlo sampling for noise simulation
* Vectorized measurement operations

Example of vmap for parallel circuit evaluation:

.. code-block:: python

    # Define a parameterized circuit
    def param_circuit(params):
        c = tc.Circuit(2)
        c.rx(0, theta=params[0])
        c.ry(1, theta=params[1])
        return K.real(c.expectation([tc.gates.z(), [0]]))

    # Create batch of parameters
    batch_params = K.ones([10, 2])

    # Vectorize the circuit evaluation
    vmap_circuit = K.vmap(param_circuit)
    results = vmap_circuit(batch_params)
    

For more advanced usage patterns and detailed examples of vmap, refer to our `vmap tutorial <https://tensorcircuit-ng.readthedocs.io/en/latest/whitepaper/6-3-vmap.html>`_.


Backend Agnosticism
-------------------------

TensorCircuit-NG supports TensorFlow, Jax, and PyTorch backends. We recommend using TensorFlow or Jax backend since PyTorch lacks advanced jit and vmap features.

The backend can be set as ``K=tc.set_backend("jax")`` and ``K`` is the backend with a full set of APIs as a conventional ML framework, which can also be accessed by ``tc.backend``.

.. code-block:: python

    >>> import tensorcircuit as tc
    >>> K = tc.set_backend("tensorflow")
    >>> K.ones([2,2])
    <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=
    array([[1.+0.j, 1.+0.j],
        [1.+0.j, 1.+0.j]], dtype=complex64)>
    >>> tc.backend.eye(3)
    <tf.Tensor: shape=(3, 3), dtype=complex64, numpy=
    array([[1.+0.j, 0.+0.j, 0.+0.j],
        [0.+0.j, 1.+0.j, 0.+0.j],
        [0.+0.j, 0.+0.j, 1.+0.j]], dtype=complex64)>
    >>> tc.set_backend("jax")
    <tensorcircuit.backends.jax_backend.JaxBackend object at 0x7fb00e0fd6d0>
    >>> tc.backend.name
    'jax'
    >>> tc.backend.implicit_randu()
    WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
    DeviceArray([0.7400521], dtype=float32)

The supported APIs in the backend come from two sources, one part is implemented in `TensorNetwork package <https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/abstract_backend.py>`__
and the other part is implemented in `TensorCircuit package <modules.html#module-tensorcircuit.backends>`__. To see all the backend agnostic APIs, try:

.. code-block:: python

    >>> [s for s in dir(tc.backend) if not s.startswith("_")]
    ['abs',
    'acos',
    'acosh',
    'addition',
    'adjoint',
    'arange',
    'argmax',
    'argmin',
    'asin',
    'asinh',
    'atan',
    'atan2',
    'atanh',
    'broadcast_left_multiplication',
    'broadcast_right_multiplication',
    'cast',
    'cholesky',
    'concat',
    'cond',
    'conj',
    'convert_to_tensor',
    'coo_sparse_matrix',
    'coo_sparse_matrix_from_numpy',
    'copy',
    'cos',
    'cosh',
    'cumsum',
    'deserialize_tensor',
    'device',
    'device_move',
    'diagflat',
    'diagonal',
    'divide',
    'dtype',
    'eigh',
    'eigs',
    'eigsh',
    'eigsh_lanczos',
    'eigvalsh',
    'einsum',
    'eps',
    'exp',
    'expm',
    'eye',
    'from_dlpack',
    'g',
    'gather1d',
    'get_random_state',
    'gmres',
    'grad',
    'hessian',
    'i',
    'imag',
    'implicit_randc',
    'implicit_randn',
    'implicit_randu',
    'index_update',
    'inv',
    'is_sparse',
    'is_tensor',
    'item',
    'jacbwd',
    'jacfwd',
    'jacrev',
    'jit',
    'jvp',
    'kron',
    'left_shift',
    'log',
    'matmul',
    'max',
    'mean',
    'min',
    'minor',
    'mod',
    'multiply',
    'name',
    'norm',
    'numpy',
    'one_hot',
    'onehot',
    'ones',
    'optimizer',
    'outer_product',
    'pivot',
    'power',
    'probability_sample',
    'qr',
    'randn',
    'random_split',
    'random_uniform',
    'real',
    'relu',
    'reshape',
    'reshape2',
    'reshapem',
    'reverse',
    'right_shift',
    'rq',
    'scatter',
    'searchsorted',
    'serialize_tensor',
    'set_random_state',
    'shape_concat',
    'shape_prod',
    'shape_tensor',
    'shape_tuple',
    'sigmoid',
    'sign',
    'sin',
    'sinh',
    'size',
    'sizen',
    'slice',
    'softmax',
    'solve',
    'sparse_dense_matmul',
    'sparse_shape',
    'sqrt',
    'sqrtmh',
    'stack',
    'stateful_randc',
    'stateful_randn',
    'stateful_randu',
    'std',
    'stop_gradient',
    'subtraction',
    'sum',
    'svd',
    'switch',
    'tan',
    'tanh',
    'tensordot',
    'tile',
    'to_dense',
    'to_dlpack',
    'trace',
    'transpose',
    'tree_flatten',
    'tree_map',
    'tree_unflatten',
    'unique_with_counts',
    'value_and_grad',
    'vectorized_value_and_grad',
    'vjp',
    'vmap',
    'vvag',
    'zeros']


Switch the Dtype
--------------------

TensorCircuit-NG supports simulation using 32/64 bit precession. The default dtype is 32-bit as "complex64".
Change this by ``tc.set_dtype("complex128")``.

``tc.dtypestr`` always returns the current dtype string: either "complex64" or "complex128". Accordingly, ``tc.rdtypestr`` always returns the current real dtype string: either "float32" or "float64".


Setup the Contractor
------------------------

TensorCircuit-NG is a tensornetwork contraction-based quantum circuit simulator. A contractor is for searching for the optimal contraction path of the circuit tensornetwork.

There are various advanced contractors provided by third-party packages, such as `opt-einsum <https://github.com/dgasmith/opt_einsum>`__ and `cotengra <https://github.com/jcmgray/cotengra>`__.

`opt-einsum` is shipped with TensorNetwork package. To use cotengra, one needs to pip install it; kahypar is also recommended to install with cotengra.

Some setup cases:

.. code-block:: python

    import tensorcircuit as tc
    
    # 1. cotengra contractors, have better and consistent performance for large circuit simulation
    import cotengra as ctg

    optr = ctg.ReusableHyperOptimizer(
        methods=["greedy", "kahypar"],
        parallel=True,
        minimize="flops",
        max_time=120,
        max_repeats=4096,
        progbar=True,
    )
    tc.set_contractor("custom", optimizer=optr, preprocessing=True)
    # by preprocessing set as True, tensorcircuit will automatically merge all single-qubit gates into entangling gates

    # 2.  RandomGreedy contractor
    tc.set_contractor("custom_stateful", optimizer=oem.RandomGreedy, max_time=60, max_repeats=128, minimize="size")

    # 3. state simulator like contractor provided by tensorcircuit, maybe better when there is ring topology for two-qubit gate layout
    tc.set_contractor("plain-experimental")

For advanced configurations on cotengra contractors, please refer to cotengra `doc <https://cotengra.readthedocs.io/en/latest/advanced.html>`__ .

**Setup in Function or Context Level**

Beside global level setup, we can also setup the backend, the dtype, and the contractor at the function level or context manager level:

.. code-block:: python

    with tc.runtime_backend("tensorflow"):
        with tc.runtime_dtype("complex128"):
            m = tc.backend.eye(2)
    n = tc.backend.eye(2)
    print(m, n) # m is tf tensor while n is numpy array

    @tc.set_function_backend("tensorflow")
    @tc.set_function_dtype("complex128")
    def f():
        return tc.backend.eye(2)
    print(f()) # complex128 tf tensor


Noisy Circuit Simulation
----------------------------

**Monte Carlo State Simulator:**

For the Monte Carlo trajectory noise simulator, the unitary Kraus channel can be handled easily. TensorCircuit-NG also supports fully jittable and differentiable general Kraus channel Monte Carlo simulation, though.

.. code-block:: python

    def noisecircuit(random):
        c = tc.Circuit(1)
        c.x(0)
        c.thermalrelaxation(
            0,
            t1=300,
            t2=400,
            time=1000,
            method="ByChoi",
            excitedstatepopulation=0,
            status=random,
        )
        return c.expectation_ps(z=[0])


    K = tc.set_backend("tensorflow")
    noisec_vmap = K.jit(K.vmap(noisecircuit, vectorized_argnums=0))
    nmc = 10000
    random = K.implicit_randu(nmc)
    valuemc = K.mean(K.numpy(noisec_vmap(random)))
    # (0.931+0j)


**Density Matrix Simulator:**

Density matrix simulator ``tc.DMCircuit`` simulates the noise in a full form, but takes twice qubits to do noiseless simulation. The API is the same as ``tc.Circuit``.

.. code-block:: python

    def noisecircuitdm():
        dmc = tc.DMCircuit(1)
        dmc.x(0)
        dmc.thermalrelaxation(
            0, t1=300, t2=400, time=1000, method="ByChoi", excitedstatepopulation=0
        )
        return dmc.expectation_ps(z=[0])


    K = tc.set_backend("tensorflow")
    noisec_jit = K.jit(noisecircuitdm)
    valuedm = noisec_jit()
    # (0.931+0j)


**Experiment with quantum errors:**

Multiple quantum errors can be added on circuit.

.. code-block:: python

    c = tc.Circuit(1)
    c.x(0)
    c.thermalrelaxation(
        0, t1=300, t2=400, time=1000, method="ByChoi", excitedstatepopulation=0
    )
    c.generaldepolarizing(0, p=0.01, num_qubits=1)
    c.phasedamping(0, gamma=0.2)
    c.amplitudedamping(0, gamma=0.25, p=0.2)
    c.reset(0)
    c.expectation_ps(z=[0])


**Experiment with readout error:**

Readout error can be added in experiments for sampling and expectation value calculation.

.. code-block:: python

    c = tc.Circuit(3)
    c.X(0)
    readout_error = []
    readout_error.append([0.9, 0.75])  # readout error of qubit 0   p0|0=0.9, p1|1=0.75
    readout_error.append([0.4, 0.7])  # readout error of qubit 1
    readout_error.append([0.7, 0.9])  # readout error of qubit 2
    value = c.sample_expectation_ps(z=[0, 1, 2], readout_error=readout_error)
    # tf.Tensor(0.039999977, shape=(), dtype=float32)
    instances = c.sample(
        batch=3,
        allow_state=True,
        readout_error=readout_error,
        random_generator=tc.backend.get_random_state(42),
        format_="sample_bin"
    )
    # tf.Tensor(
    # [[1 0 0]
    # [1 0 0]
    # [1 0 1]], shape=(3, 3), dtype=int32)


MPS and MPO
----------------

TensorCircuit-NG has its class for MPS and MPO originally defined in TensorNetwork as ``tc.QuVector``, ``tc.QuOperator``.

``tc.QuVector`` can be extracted from ``tc.Circuit`` as the tensor network form for the output state (uncontracted) by ``c.quvector()``.

The QuVector forms a wavefunction w, which can also be fed into Circuit as the inputs state as ``c=tc.Circuit(n, mps_inputs=w)``.

- MPS as input state for circuit

The MPS/QuVector representation of the input state has a more efficient and compact form.

.. code-block:: python

    n = 3
    nodes = [tc.gates.Gate(np.array([0.0, 1.0])) for _ in range(n)]
    mps = tc.quantum.QuVector([nd[0] for nd in nodes])
    c = tc.Circuit(n, mps_inputs=mps)
    c.x(0)
    c.expectation_ps(z=[0])
    # 1.0

- MPS as (uncomputed) output state for circuit

For example, a quick way to calculate the wavefunction overlap without explicitly computing the state amplitude is given as below:

.. code-block:: python

    >>> c = tc.Circuit(3)
    >>> [c.H(i) for i in range(3)]
    [None, None, None]
    >>> c.cnot(0, 1)
    >>> c2 = tc.Circuit(3)
    >>> [c2.H(i) for i in range(3)]
    [None, None, None]
    >>> c2.cnot(1, 0)
    >>> q = c.quvector()
    >>> q2 = c2.quvector().adjoint()
    >>> (q2@q).eval_matrix()
    array([[0.9999998+0.j]], dtype=complex64)

- MPO as the gate on the circuit

Instead of a common quantum gate in matrix/node format, we can directly apply a gate in MPO/QuOperator format.

.. code-block:: python

    >>> x0, x1 = tc.gates.x(), tc.gates.x()
    >>> mpo = tc.quantum.QuOperator([x0[0], x1[0]], [x0[1], x1[1]])
    >>> c = tc.Circuit(2)
    >>> c.mpo(0, 1, mpo=mpo)
    >>> c.state()
    array([0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j], dtype=complex64)

The representative gate defined in MPO format is the ``multicontrol`` gate.

- MPO as the operator for expectation evaluation on a circuit

We can also measure operator expectation on the circuit output state where the operator is in MPO/QuOperator format.

.. code-block:: python

    >>> z0, z1 = tc.gates.z(), tc.gates.z()
    >>> mpo = tc.quantum.QuOperator([z0[0], z1[0]], [z0[1], z1[1]])
    >>> c = tc.Circuit(2)
    >>> c.X(0)
    >>> tc.templates.measurements.mpo_expectation(c, mpo)
    -1.0

Interfaces
-------------

**PyTorch Interface to Hybrid with PyTorch Modules:**

As we have mentioned in the backend section, the PyTorch backend may lack advanced features. This doesn't mean we cannot hybrid the advanced circuit module with PyTorch neural module. We can run the quantum function on TensorFlow or Jax backend while wrapping it with a Torch interface.

.. code-block:: python

    import tensorcircuit as tc
    from tensorcircuit.interfaces import torch_interface
    import torch

    tc.set_backend("tensorflow")


    def f(params):
        c = tc.Circuit(1)
        c.rx(0, theta=params[0])
        c.ry(0, theta=params[1])
        return c.expectation([tc.gates.z(), [0]])


    f_torch = torch_interface(f, jit=True)

    a = torch.ones([2], requires_grad=True)
    b = f_torch(a)
    c = b ** 2
    c.backward()

    print(a.grad)

For a GPU/CPU, torch/tensorflow, quantum/classical hybrid machine learning pipeline enabled by tensorcircuit, see `example script <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/hybrid_gpu_pipeline.py>`__.

There is also a more flexible torch interface that support static non-tensor inputs as keyword arguments, which can be utilized as below:

.. code-block:: python

    def f(a, i):
        s = 0.
        for _ in range(i):
            s += a
        return s

    f_torch = tc.interfaces.torch_interface_kws(f)
    f_torch(torch.ones([2]), i=3)


We also provider wrapper of quantum function for torch module as :py:meth:`tensorcircuit.TorchLayer` alias to :py:meth:`tensorcircuit.torchnn.QuantumNet`.

For ``TorchLayer``, ``use_interface=True`` is by default, which natively allow the quantum function defined on other tensorcircuit backends, such as jax or tf for speed consideration.

``TorchLayer`` can process multiple input arguments as multiple function inputs, following torch practice.

.. code-block:: python

    n = 3
    p = 0.1
    K = tc.backend
    torchb = tc.get_backend("pytorch")

    def f(state, noise, weights):
        c = tc.Circuit(n, inputs=state)
        for i in range(n):
            c.rz(i, theta=weights[i])
        for i in range(n):
            c.depolarizing(i, px=p, py=p, pz=p, status=noise[i])
        return K.real(c.expectation_ps(x=[0]))

    layer = tc.TorchLayer(f, [n], use_vmap=True, vectorized_argnums=[0, 1])
    state = torchb.ones([2, 2**n]) / 2 ** (n / 2)
    noise = 0.2 * torchb.ones([2, n], dtype="float32")
    l = layer(state,noise)
    lsum = torchb.sum(l)
    print(l)
    lsum.backward()
    for p in layer.parameters():
        print(p.grad)


**TensorFlow interfaces:**

Similar rules apply similar as torch interface. The interface can even be used within jit environment outside.
See :py:meth:`tensorcircuit.interfaces.tensorflow.tensorflow_interface`.

We also provider ``enable_dlpack=True`` option in torch and tf interfaces, which allow the tensor transformation happen without memory transfer via dlpack,
higher version of tf or torch package required.

We also provider wrapper of quantum function for keras layer as :py:meth:`tensorcircuit.KerasLayer` alias to :py:meth:`tensorcircuit.keras.KerasLayer`.

``KerasLayer`` can process multiple input arguments with the input as a dict, following the common keras practice, see example below.

.. code-block:: python

    def f(inputs, weights):
        state = inputs["state"]
        noise = inputs["noise"]
        c = tc.Circuit(n, inputs=state)
        for i in range(n):
            c.rz(i, theta=weights[i])
        for i in range(n):
            c.depolarizing(i, px=p, py=p, pz=p, status=noise[i])
        return K.real(c.expectation_ps(x=[0]))

    layer = tc.KerasLayer(f, [n])
    v = {"state": K.ones([1, 2**n]) / 2 ** (n / 2), "noise": 0.2 * K.ones([1, n])}
    with tf.GradientTape() as tape:
        l = layer(v)
    grad = tape.gradient(l, layer.trainable_variables)

**JAX interfaces:**

TensorCircuit-NG also newly introduces JAX interface to seamlessly integrate with JAX's ecosystem. 
This allows you to use JAX's powerful features like automatic differentiation, JIT compilation, and vectorization with quantum circuits or functions running on any backend.

Basic usage with JAX interface:

.. code-block:: python

    import tensorcircuit as tc
    import jax
    import jax.numpy as jnp

    # Set non-jax backend
    tc.set_backend("tensorflow")

    def circuit(params):
        c = tc.Circuit(2)
        c.rx(0, theta=params[0])
        c.ry(1, theta=params[1])
        c.cnot(0, 1)
        return tc.backend.real(c.expectation_ps(z=[1]))

    # Wrap the circuit with JAX interface
    jax_circuit = tc.interfaces.jax_interface(circuit, jit=True)

    # Now you can use JAX features
    params = jnp.ones(2)
    value, grad = jax.value_and_grad(jax_circuit)(params)
    print("Value:", value)
    print("Gradient:", grad)

Some advanced features:

1. DLPack support for efficient tensor conversion:

.. code-block:: python

    # Enable DLPack for zero-copy tensor conversion
    jax_circuit = tc.interfaces.jax_interface(circuit, 
                                            jit=True, 
                                            enable_dlpack=True)

2. Explicit output shape specification for better performance:

.. code-block:: python

    # Specify output shape and dtype
    jax_circuit = tc.interfaces.jax_interface(circuit,
                                            jit=True,
                                            output_shape=(1,),
                                            output_dtype=jnp.float32)

3. Multiple outputs support:

.. code-block:: python

    def multi_output_circuit(params):
    c = tc.Circuit(2)
    c.rx(0, theta=params[0])
    c.ry(1, theta=params[1])
    z0 = c.expectation([tc.gates.z(), [0]])
    z1 = c.expectation([tc.gates.z(), [1]])
    return tc.backend.real(z0), tc.backend.real(z1)

    jax_circuit = tc.interfaces.jax_interface(multi_output_circuit,
                                            jit=True,
                                            output_shape=[[], []],
                                            output_dtype=[jnp.float32, jnp.float32])
    # Now you can use JAX features
    params = jnp.ones(2)
    value, grad = jax.value_and_grad(tc.utils.append(jax_circuit, sum))(params)



**Scipy Interface to Utilize Scipy Optimizers:**

Automatically transform quantum functions as scipy-compatible values and grad functions as provided for scipy interface with ``jac=True``.

.. code-block:: python

    n = 3

    def f(param):
        c = tc.Circuit(n)
        for i in range(n):
            c.rx(i, theta=param[0, i])
            c.rz(i, theta=param[1, i])
        loss = c.expectation(
            [
                tc.gates.y(),
                [
                    0,
                ],
            ]
        )
        return tc.backend.real(loss)

    f_scipy = tc.interfaces.scipy_optimize_interface(f, shape=[2, n])
    r = optimize.minimize(f_scipy, np.zeros([2 * n]), method="L-BFGS-B", jac=True)


Templates as Shortcuts
------------------------

**Measurements:**

* Ising type Hamiltonian defined on a general graph

See :py:meth:`tensorcircuit.templates.measurements.spin_glass_measurements`

* Heisenberg Hamiltonian on a general graph with possible external fields

See :py:meth:`tensorcircuit.templates.measurements.heisenberg_measurements`

**Circuit Blocks:**

.. code-block:: python

    c = tc.Circuit(4)
    c = tc.templates.blocks.example_block(c, tc.backend.ones([16]))

.. figure:: statics/example_block.png

.. code-block:: python

    c = tc.Circuit(4)
    c = tc.templates.blocks.Bell_pair_block(c)

.. figure:: statics/bell_pair_block.png
    :scale: 50%




================================================
FILE: docs/source/sharpbits.rst
================================================
=================================
TensorCircuit: The Sharp Bits 🔪
=================================

Be fast is never for free, though much cheaper in TensorCircuit-NG, but you have to be cautious especially in terms of AD, JIT compatibility.
We will go through the main sharp edges 🔪 in this note.

Jit Compatibility
---------------------

Non tensor input or varying shape tensor input
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The input must be in tensor form and the input tensor shape must be fixed otherwise the recompilation is incurred which is time-consuming.
Therefore, if there are input args that are non-tensor or varying shape tensors and frequently change, jit is not recommend.

.. code-block:: python

    K = tc.set_backend("tensorflow")

    @K.jit
    def f(a):
        print("compiling")
        return 2*a

    f(K.ones([2]))
    # compiling
    # <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([2.+0.j, 2.+0.j], dtype=complex64)>

    f(K.zeros([2]))
    # <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([0.+0.j, 0.+0.j], dtype=complex64)>

    f(K.ones([3]))
    # compiling
    # <tf.Tensor: shape=(3,), dtype=complex64, numpy=array([2.+0.j, 2.+0.j, 2.+0.j], dtype=complex64)>

Mix use of numpy and ML backend APIs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To make the function jittable and ad-aware, every ops in the function should be called via ML backend (``tc.backend`` API or direct API for the chosen backend ``tf`` or ``jax``).
This is because the ML backend has to create the computational graph to carry out AD and JIT transformation. For numpy ops, they will be only called in jit staging time (the first run).

.. code-block:: python

    K = tc.set_backend("tensorflow")

    @K.jit
    def f(a):
        return np.dot(a, a)

    f(K.ones([2]))
    # NotImplementedError: Cannot convert a symbolic Tensor (a:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported

Numpy call inside jitted function can be helpful if you are sure of the behavior is what you expect.

.. code-block:: python

    K = tc.set_backend("tensorflow")

    @K.jit
    def f(a):
        print("compiling")
        n = a.shape[0]
        m = int(np.log(n)/np.log(2))
        return K.reshape(a, [2 for _ in range(m)])

    f(K.ones([4]))
    # compiling
    # <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=
    # array([[1.+0.j, 1.+0.j],
    #        [1.+0.j, 1.+0.j]], dtype=complex64)>

    f(K.zeros([4]))
    # <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=
    # array([[0.+0.j, 0.+0.j],
    #        [0.+0.j, 0.+0.j]], dtype=complex64)>

    f(K.zeros([2]))
    # compiling
    # <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([0.+0.j, 0.+0.j], dtype=complex64)>

list append under if
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Append something to a Python list within if whose condition is based on tensor values will lead to wrong results.
Actually values of both branch will be attached to the list. See example below.

.. code-block:: python

    K = tc.set_backend("tensorflow")

    @K.jit
    def f(a):
        l = []
        one = K.ones([])
        zero = K.zeros([])
        if a > 0:
            l.append(one)
        else:
            l.append(zero)
        return l

    f(-K.ones([], dtype="float32"))

    # [<tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>,
    # <tf.Tensor: shape=(), dtype=complex64, numpy=0j>]

The above code raise ``ConcretizationTypeError`` exception directly for Jax backend since Jax jit doesn't support tensor value if condition.

Similarly, conditional gate application must be takend carefully.

.. code-block:: python

    K = tc.set_backend("tensorflow")

    @K.jit
    def f():
        c = tc.Circuit(1)
        c.h(0)
        a = c.cond_measure(0)
        if a > 0.5:
            c.x(0)
        else:
            c.z(0)
        return c.state()

    f()
    # InaccessibleTensorError: tf.Graph captured an external symbolic tensor.

    # The correct implementation is

    @K.jit
    def f():
        c = tc.Circuit(1)
        c.h(0)
        a = c.cond_measure(0)
        c.conditional_gate(a, [tc.gates.z(), tc.gates.x()], 0)
        return c.state()

    f()
    # <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([0.99999994+0.j, 0.        +0.j], dtype=complex64)>


Tensor variables consistency
-------------------------------------------------------


All tensor variables' backend (tf vs jax vs ..), dtype (float vs complex), shape and device (cpu vs gpu) must be compatible/consistent.

Inspect the backend, dtype, shape and device using the following codes.

.. code-block:: python

    for backend in ["numpy", "tensorflow", "jax", "pytorch"]:
        with tc.runtime_backend(backend):
            a = tc.backend.ones([2, 3])
            print("tensor backend:", tc.interfaces.which_backend(a))
            print("tensor dtype:", tc.backend.dtype(a))
            print("tensor shape:", tc.backend.shape_tuple(a))
            print("tensor device:", tc.backend.device(a))

If the backend is inconsistent, one can convert the tensor backend via :py:meth:`tensorcircuit.interfaces.tensortrans.general_args_to_backend`.

.. code-block:: python

    for backend in ["numpy", "tensorflow", "jax", "pytorch"]:
        with tc.runtime_backend(backend):
            a = tc.backend.ones([2, 3])
            print("tensor backend:", tc.interfaces.which_backend(a))
            b = tc.interfaces.general_args_to_backend(a, target_backend="jax", enable_dlpack=False)
            print("tensor backend:", tc.interfaces.which_backend(b))

If the dtype is inconsistent, one can convert the tensor dtype using ``tc.backend.cast``.

.. code-block:: python

    for backend in ["numpy", "tensorflow", "jax", "pytorch"]:
        with tc.runtime_backend(backend):
            a = tc.backend.ones([2, 3])
            print("tensor dtype:", tc.backend.dtype(a))
            b = tc.backend.cast(a, dtype="float64")
            print("tensor dtype:", tc.backend.dtype(b))

Also note the jax issue on float64/complex128, see `jax gotcha <https://github.com/google/jax#current-gotchas>`_.

If the shape is not consistent, one can convert the shape by ``tc.backend.reshape``.

If the device is not consistent, one can move the tensor between devices by ``tc.backend.device_move``.


AD Consistency
---------------------

Gradients in terms of complex dtypes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TF and JAX backend manage the differentiation rules differently for complex-valued function (actually up to a complex conjuagte). See issue discussion `tensorflow issue <https://github.com/tensorflow/tensorflow/issues/3348>`_.

In TensorCircuit-NG, currently we make the difference in AD transparent, namely, when switching the backend, the AD behavior and result for complex valued function can be different and determined by the nature behavior of the corresponding backend framework.
All AD relevant ops such as ``grad`` or ``jacrev`` may be affected. Therefore, the user must be careful when dealing with AD on complex valued function in a backend agnostic way in TensorCircuit.

See example script on computing Jacobian with different modes on different backends: `jacobian_cal.py <https://github.com/tensorcircuit/tensorcircuit-ng/blob/master/examples/jacobian_cal.py>`_.
Also see the code below for a reference:

.. code-block:: python

    bks = ["tensorflow", "jax"]
    n = 2
    for bk in bks:
        print(bk, "backend")
        with tc.runtime_backend(bk) as K:
            def wfn(params):
                c = tc.Circuit(n)
                for i in range(n):
                    c.H(i)
                for i in range(n):
                    c.rz(i, theta=params[i])
                    c.rx(i, theta=params[i])
                return K.real(c.expectation_ps(z=[0])+c.expectation_ps(z=[1]))
            print(K.grad(wfn)(K.ones([n], dtype="complex64"))) # default
            print(K.grad(wfn)(K.ones([n], dtype="float32")))

    # tensorflow backend
    # tf.Tensor([0.90929717+0.9228758j 0.90929717+0.9228758j], shape=(2,), dtype=complex64)
    # tf.Tensor([0.90929717 0.90929717], shape=(2,), dtype=float32)
    # jax backend
    # [0.90929747-0.9228759j 0.90929747-0.9228759j]
    # [0.90929747 0.90929747]


VMAP outside grad-like function on tensorflow backend
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Vmap (vectorized map) outside a grad-like function may cause incorrected results on TensorFlow backends due to a long existing `bug <https://github.com/tensorflow/tensorflow/issues/52148>`_ in TensorFlow codebase. So better always stick to the first-vmap-then-differentiated paradigm.

Grad over vmap function
~~~~~~~~~~~~~~~~~~~~~~~~~

A related issue is the different behavior for ``K.grad(K.vmap(f))`` on different backends. For tensorflow backend, the function to be differentiated has a scalar output which is the sum of all outputs.

However, for Jax backend, the function simply raise error as only scalar output function can be differentiated, no implicit sum of the vectorized ``f`` is assumed. For non-scalar output, one should use `jacrev` or `jacfwd` to get the gradient information.

Specifically, ``K.grad(K.vmap(f))`` on TensorFlow backend is equilvalent to ``K.grad(K.append(K.vamp(f), K.sum))`` on Jax backend.


================================================
FILE: docs/source/textbooktoc.rst
================================================
=================================
量子计算教程
=================================

.. toctree::

    textbook/chap1.ipynb
    textbook/chap2.ipynb
    textbook/chap3.ipynb
    textbook/chap4.ipynb
    textbook/chap5.ipynb


================================================
FILE: docs/source/tutorial.rst
================================================
=================================
Jupyter Tutorials
=================================

.. toctree::

    tutorials/circuit_basics.ipynb
    tutorials/circuit_qudit_basics.ipynb
    tutorials/qaoa.ipynb
    tutorials/qaoa_bo.ipynb
    tutorials/qaoa_nae3sat.ipynb
    tutorials/qaoa_quantum_dropout.ipynb
    tutorials/tfim_vqe.ipynb
    tutorials/mnist_qml.ipynb
    tutorials/torch_qml.ipynb
    tutorials/qml_scenarios.ipynb
    tutorials/vqe_h2o.ipynb
    tutorials/tfim_vqe_diffreph.ipynb
    tutorials/mera.ipynb
    tutorials/gradient_benchmark.ipynb
    tutorials/contractors.ipynb
    tutorials/stabilizer_circuit.ipynb
    tutorials/fermion_gaussian_states.ipynb
    tutorials/operator_spreading.ipynb
    tutorials/optimization_and_expressibility.ipynb
    tutorials/vqex_mbl.ipynb
    tutorials/dqas.ipynb
    tutorials/barren_plateaus.ipynb
    tutorials/qubo_problem.ipynb
    tutorials/lattice.ipynb
    tutorials/portfolio_optimization.ipynb
    tutorials/imag_time_evo.ipynb
    tutorials/classical_shadows.ipynb
    tutorials/sklearn_svc.ipynb
    tutorials/distributed_simulation.ipynb
    tutorials/qcloud_sdk_demo.ipynb


================================================
FILE: docs/source/tutorial_cn.rst
================================================
=================================
案例教程
=================================

.. toctree::

    tutorials/circuit_basics_cn.ipynb
    tutorials/circuit_qudit_basics_cn.ipynb
    tutorials/qaoa_cn.ipynb
    tutorials/tfim_vqe_cn.ipynb
    tutorials/mnist_qml_cn.ipynb
    tutorials/torch_qml_cn.ipynb
    tutorials/qml_scenarios_cn.ipynb
    tutorials/vqe_h2o_cn.ipynb
    tutorials/tfim_vqe_diffreph_cn.ipynb
    tutorials/mera_cn.ipynb
    tutorials/gradient_benchmark_cn.ipynb
    tutorials/contractors_cn.ipynb
    tutorials/operator_spreading_cn.ipynb
    tutorials/optimization_and_expressibility_cn.ipynb
    tutorials/vqex_mbl_cn.ipynb
    tutorials/dqas_cn.ipynb
    tutorials/barren_plateaus_cn.ipynb
    tutorials/sklearn_svc_cn.ipynb


================================================
FILE: docs/source/whitepapertoc.rst
================================================
=================================
Whitepaper Tutorials
=================================

.. toctree::

    whitepaper/3-circuits-gates.ipynb
    whitepaper/4-gradient-optimization.ipynb
    whitepaper/5-density-matrix.ipynb
    whitepaper/6-1-conditional-measurements-post-selection.ipynb
    whitepaper/6-2-pauli-string-expectation.ipynb
    whitepaper/6-3-vmap.ipynb
    whitepaper/6-4-quoperator.ipynb
    whitepaper/6-5-custom-contraction.ipynb
    whitepaper/6-6-advanced-automatic-differentiation.ipynb


================================================
FILE: docs/source/whitepapertoc_cn.rst
================================================
=================================
白皮书教程
=================================

.. toctree::

    whitepaper/3-circuits-gates_cn.ipynb
    whitepaper/4-gradient-optimization_cn.ipynb
    whitepaper/5-density-matrix_cn.ipynb
    whitepaper/6-1-conditional-measurements-post-selection_cn.ipynb
    whitepaper/6-2-pauli-string-expectation_cn.ipynb
    whitepaper/6-3-vmap_cn.ipynb
    whitepaper/6-4-quoperator_cn.ipynb
    whitepaper/6-5-custom-contraction_cn.ipynb
    whitepaper/6-6-advanced-automatic-differentiation_cn.ipynb


================================================
FILE: docs/source/api/about.rst
================================================
tensorcircuit.about
================================================================================
.. automodule:: tensorcircuit.about
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/abstractcircuit.rst
================================================
tensorcircuit.abstractcircuit
================================================================================
.. automodule:: tensorcircuit.abstractcircuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/analogcircuit.rst
================================================
tensorcircuit.analogcircuit
================================================================================
.. automodule:: tensorcircuit.analogcircuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications.rst
================================================
tensorcircuit.applications
================================================================================
.. toctree::
    applications/ai.rst
    applications/dqas.rst
    applications/finance.rst
    applications/graphdata.rst
    applications/layers.rst
    applications/optimization.rst
    applications/physics.rst
    applications/utils.rst
    applications/vags.rst
    applications/van.rst
    applications/vqes.rst


================================================
FILE: docs/source/api/backends.rst
================================================
tensorcircuit.backends
================================================================================
.. toctree::
    backends/backend_factory.rst
    backends/cupy_backend.rst
    backends/jax_backend.rst
    backends/numpy_backend.rst
    backends/pytorch_backend.rst
    backends/tensorflow_backend.rst


================================================
FILE: docs/source/api/basecircuit.rst
================================================
tensorcircuit.basecircuit
================================================================================
.. automodule:: tensorcircuit.basecircuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/channels.rst
================================================
tensorcircuit.channels
================================================================================
.. automodule:: tensorcircuit.channels
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/circuit.rst
================================================
tensorcircuit.circuit
================================================================================
.. automodule:: tensorcircuit.circuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud.rst
================================================
tensorcircuit.cloud
================================================================================
.. toctree::
    cloud/abstraction.rst
    cloud/apis.rst
    cloud/config.rst
    cloud/local.rst
    cloud/quafu_provider.rst
    cloud/tencent.rst
    cloud/utils.rst
    cloud/wrapper.rst


================================================
FILE: docs/source/api/compiler.rst
================================================
tensorcircuit.compiler
================================================================================
.. toctree::
    compiler/composed_compiler.rst
    compiler/qiskit_compiler.rst
    compiler/simple_compiler.rst


================================================
FILE: docs/source/api/cons.rst
================================================
tensorcircuit.cons
================================================================================
.. automodule:: tensorcircuit.cons
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/densitymatrix.rst
================================================
tensorcircuit.densitymatrix
================================================================================
.. automodule:: tensorcircuit.densitymatrix
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/experimental.rst
================================================
tensorcircuit.experimental
================================================================================
.. automodule:: tensorcircuit.experimental
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/fgs.rst
================================================
tensorcircuit.fgs
================================================================================
.. automodule:: tensorcircuit.fgs
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/gates.rst
================================================
tensorcircuit.gates
================================================================================
.. automodule:: tensorcircuit.gates
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces.rst
================================================
tensorcircuit.interfaces
================================================================================
.. toctree::
    interfaces/jax.rst
    interfaces/numpy.rst
    interfaces/scipy.rst
    interfaces/tensorflow.rst
    interfaces/tensortrans.rst
    interfaces/torch.rst


================================================
FILE: docs/source/api/keras.rst
================================================
tensorcircuit.keras
================================================================================
.. automodule:: tensorcircuit.keras
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/mps_base.rst
================================================
tensorcircuit.mps_base
================================================================================
.. automodule:: tensorcircuit.mps_base
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/mpscircuit.rst
================================================
tensorcircuit.mpscircuit
================================================================================
.. automodule:: tensorcircuit.mpscircuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/noisemodel.rst
================================================
tensorcircuit.noisemodel
================================================================================
.. automodule:: tensorcircuit.noisemodel
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/quantum.rst
================================================
tensorcircuit.quantum
================================================================================
.. automodule:: tensorcircuit.quantum
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/quditcircuit.rst
================================================
tensorcircuit.quditcircuit
================================================================================
.. automodule:: tensorcircuit.quditcircuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/quditgates.rst
================================================
tensorcircuit.quditgates
================================================================================
.. automodule:: tensorcircuit.quditgates
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/results.rst
================================================
tensorcircuit.results
================================================================================
.. toctree::
    results/counts.rst
    results/qem.rst
    results/readout_mitigation.rst


================================================
FILE: docs/source/api/shadows.rst
================================================
tensorcircuit.shadows
================================================================================
.. automodule:: tensorcircuit.shadows
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/simplify.rst
================================================
tensorcircuit.simplify
================================================================================
.. automodule:: tensorcircuit.simplify
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/stabilizercircuit.rst
================================================
tensorcircuit.stabilizercircuit
================================================================================
.. automodule:: tensorcircuit.stabilizercircuit
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates.rst
================================================
tensorcircuit.templates
================================================================================
.. toctree::
    templates/ansatz.rst
    templates/blocks.rst
    templates/chems.rst
    templates/conversions.rst
    templates/dataset.rst
    templates/graphs.rst
    templates/hamiltonians.rst
    templates/lattice.rst
    templates/measurements.rst


================================================
FILE: docs/source/api/timeevol.rst
================================================
tensorcircuit.timeevol
================================================================================
.. automodule:: tensorcircuit.timeevol
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/torchnn.rst
================================================
tensorcircuit.torchnn
================================================================================
.. automodule:: tensorcircuit.torchnn
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/translation.rst
================================================
tensorcircuit.translation
================================================================================
.. automodule:: tensorcircuit.translation
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/utils.rst
================================================
tensorcircuit.utils
================================================================================
.. automodule:: tensorcircuit.utils
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/vis.rst
================================================
tensorcircuit.vis
================================================================================
.. automodule:: tensorcircuit.vis
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/ai.rst
================================================
tensorcircuit.applications.ai
================================================================================
.. toctree::
    ai/ensemble.rst


================================================
FILE: docs/source/api/applications/dqas.rst
================================================
tensorcircuit.applications.dqas
================================================================================
.. automodule:: tensorcircuit.applications.dqas
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/finance.rst
================================================
tensorcircuit.applications.finance
================================================================================
.. toctree::
    finance/portfolio.rst


================================================
FILE: docs/source/api/applications/graphdata.rst
================================================
tensorcircuit.applications.graphdata
================================================================================
.. automodule:: tensorcircuit.applications.graphdata
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/layers.rst
================================================
tensorcircuit.applications.layers
================================================================================
.. automodule:: tensorcircuit.applications.layers
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/optimization.rst
================================================
tensorcircuit.applications.optimization
================================================================================
.. automodule:: tensorcircuit.applications.optimization
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/physics.rst
================================================
tensorcircuit.applications.physics
================================================================================
.. toctree::
    physics/baseline.rst
    physics/fss.rst


================================================
FILE: docs/source/api/applications/utils.rst
================================================
tensorcircuit.applications.utils
================================================================================
.. automodule:: tensorcircuit.applications.utils
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/vags.rst
================================================
tensorcircuit.applications.vags
================================================================================
.. automodule:: tensorcircuit.applications.vags
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/van.rst
================================================
tensorcircuit.applications.van
================================================================================
.. automodule:: tensorcircuit.applications.van
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/vqes.rst
================================================
tensorcircuit.applications.vqes
================================================================================
.. automodule:: tensorcircuit.applications.vqes
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/ai/ensemble.rst
================================================
tensorcircuit.applications.ai.ensemble
================================================================================
.. automodule:: tensorcircuit.applications.ai.ensemble
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/finance/portfolio.rst
================================================
tensorcircuit.applications.finance.portfolio
================================================================================
.. automodule:: tensorcircuit.applications.finance.portfolio
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/physics/baseline.rst
================================================
tensorcircuit.applications.physics.baseline
================================================================================
.. automodule:: tensorcircuit.applications.physics.baseline
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/applications/physics/fss.rst
================================================
tensorcircuit.applications.physics.fss
================================================================================
.. automodule:: tensorcircuit.applications.physics.fss
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/backends/backend_factory.rst
================================================
tensorcircuit.backends.backend_factory
================================================================================
.. automodule:: tensorcircuit.backends.backend_factory
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/backends/cupy_backend.rst
================================================
tensorcircuit.backends.cupy_backend
================================================================================
.. automodule:: tensorcircuit.backends.cupy_backend
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/backends/jax_backend.rst
================================================
tensorcircuit.backends.jax_backend
================================================================================
.. automodule:: tensorcircuit.backends.jax_backend
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/backends/numpy_backend.rst
================================================
tensorcircuit.backends.numpy_backend
================================================================================
.. automodule:: tensorcircuit.backends.numpy_backend
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/backends/pytorch_backend.rst
================================================
tensorcircuit.backends.pytorch_backend
================================================================================
.. automodule:: tensorcircuit.backends.pytorch_backend
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/backends/tensorflow_backend.rst
================================================
tensorcircuit.backends.tensorflow_backend
================================================================================
.. automodule:: tensorcircuit.backends.tensorflow_backend
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/abstraction.rst
================================================
tensorcircuit.cloud.abstraction
================================================================================
.. automodule:: tensorcircuit.cloud.abstraction
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/apis.rst
================================================
tensorcircuit.cloud.apis
================================================================================
.. automodule:: tensorcircuit.cloud.apis
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/config.rst
================================================
tensorcircuit.cloud.config
================================================================================
.. automodule:: tensorcircuit.cloud.config
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/local.rst
================================================
tensorcircuit.cloud.local
================================================================================
.. automodule:: tensorcircuit.cloud.local
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/quafu_provider.rst
================================================
tensorcircuit.cloud.quafu_provider
================================================================================
.. automodule:: tensorcircuit.cloud.quafu_provider
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/tencent.rst
================================================
tensorcircuit.cloud.tencent
================================================================================
.. automodule:: tensorcircuit.cloud.tencent
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/utils.rst
================================================
tensorcircuit.cloud.utils
================================================================================
.. automodule:: tensorcircuit.cloud.utils
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/cloud/wrapper.rst
================================================
tensorcircuit.cloud.wrapper
================================================================================
.. automodule:: tensorcircuit.cloud.wrapper
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/compiler/composed_compiler.rst
================================================
tensorcircuit.compiler.composed_compiler
================================================================================
.. automodule:: tensorcircuit.compiler.composed_compiler
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/compiler/qiskit_compiler.rst
================================================
tensorcircuit.compiler.qiskit_compiler
================================================================================
.. automodule:: tensorcircuit.compiler.qiskit_compiler
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/compiler/simple_compiler.rst
================================================
tensorcircuit.compiler.simple_compiler
================================================================================
.. automodule:: tensorcircuit.compiler.simple_compiler
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces/jax.rst
================================================
tensorcircuit.interfaces.jax
================================================================================
.. automodule:: tensorcircuit.interfaces.jax
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces/numpy.rst
================================================
tensorcircuit.interfaces.numpy
================================================================================
.. automodule:: tensorcircuit.interfaces.numpy
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces/scipy.rst
================================================
tensorcircuit.interfaces.scipy
================================================================================
.. automodule:: tensorcircuit.interfaces.scipy
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces/tensorflow.rst
================================================
tensorcircuit.interfaces.tensorflow
================================================================================
.. automodule:: tensorcircuit.interfaces.tensorflow
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces/tensortrans.rst
================================================
tensorcircuit.interfaces.tensortrans
================================================================================
.. automodule:: tensorcircuit.interfaces.tensortrans
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/interfaces/torch.rst
================================================
tensorcircuit.interfaces.torch
================================================================================
.. automodule:: tensorcircuit.interfaces.torch
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/results/counts.rst
================================================
tensorcircuit.results.counts
================================================================================
.. automodule:: tensorcircuit.results.counts
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/results/qem.rst
================================================
tensorcircuit.results.qem
================================================================================
.. toctree::
    qem/benchmark_circuits.rst
    qem/qem_methods.rst


================================================
FILE: docs/source/api/results/readout_mitigation.rst
================================================
tensorcircuit.results.readout_mitigation
================================================================================
.. automodule:: tensorcircuit.results.readout_mitigation
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/results/qem/benchmark_circuits.rst
================================================
tensorcircuit.results.qem.benchmark_circuits
================================================================================
.. automodule:: tensorcircuit.results.qem.benchmark_circuits
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/results/qem/qem_methods.rst
================================================
tensorcircuit.results.qem.qem_methods
================================================================================
.. automodule:: tensorcircuit.results.qem.qem_methods
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/ansatz.rst
================================================
tensorcircuit.templates.ansatz
================================================================================
.. automodule:: tensorcircuit.templates.ansatz
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/blocks.rst
================================================
tensorcircuit.templates.blocks
================================================================================
.. automodule:: tensorcircuit.templates.blocks
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/chems.rst
================================================
tensorcircuit.templates.chems
================================================================================
.. automodule:: tensorcircuit.templates.chems
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/conversions.rst
================================================
tensorcircuit.templates.conversions
================================================================================
.. automodule:: tensorcircuit.templates.conversions
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/dataset.rst
================================================
tensorcircuit.templates.dataset
================================================================================
.. automodule:: tensorcircuit.templates.dataset
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/graphs.rst
================================================
tensorcircuit.templates.graphs
================================================================================
.. automodule:: tensorcircuit.templates.graphs
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/hamiltonians.rst
================================================
tensorcircuit.templates.hamiltonians
================================================================================
.. automodule:: tensorcircuit.templates.hamiltonians
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/lattice.rst
================================================
tensorcircuit.templates.lattice
================================================================================
.. automodule:: tensorcircuit.templates.lattice
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/api/templates/measurements.rst
================================================
tensorcircuit.templates.measurements
================================================================================
.. automodule:: tensorcircuit.templates.measurements
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:


================================================
FILE: docs/source/contribs/development_Mac.md
================================================
# Tensorcircuit Installation Guide on MacOS

Contributed by [_Mark (Zixuan) Song_](https://marksong.tech)

Apple has updated Tensorflow (for MacOS) so that installation on M-series (until M2) and Intel-series Mac can follow the exact same procedure.

## Starting From Scratch

For completely new Macos or Macos without Xcode installed.

If you have Xcode installed, skip to Install TC backends.

### Install Xcode Command Line Tools

<font color=gray><em>Need graphical access to the machine.</em></font>

Run `xcode-select --install` to install if on optimal internet.

Or Download it from [Apple](https://developer.apple.com/download/more/) Command Line Tools installation image then install it if the internet connection is weak.

## Install TC Backends

There are four backends to choose from, Numpy, Tensorflow, Jax, and Torch.

### Install Jax, Pytorch (Optional)

```bash
pip install [Package Name]
```
### Install Tensorflow (Optional - Recommended)

#### Install Miniconda (Optional - Recommended)

If you wish to install Tensorflow optimized for MacOS (`tensorflow-macos`) or Tensorflow GPU optimized (`tensorflow-metal`) please install miniconda.

If you wish to install Vanilla Tensorflow developed by Google (`tensorflow`) please skip this step.

```bash
curl -o ~/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash ~/miniconda.sh -b -p $HOME/miniconda
source ~/miniconda/bin/activate
conda install -c apple tensorflow-deps
```

#### Installation

```bash
pip install tensorflow
```

If you wish to use tensorflow-metal PluggableDevice, then continue install (not recommended):

```bash
pip install tensorflow-metal
```

#### Verify Tensorflow Installation

```python
import tensorflow as tf

cifar = tf.keras.datasets.cifar100
(x_train, y_train), (x_test, y_test) = cifar.load_data()
model = tf.keras.applications.ResNet50(
    include_top=True,
    weights=None,
    input_shape=(32, 32, 3),
    classes=100,)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer="adam", loss=loss_fn, metrics=["accuracy"])
model.fit(x_train, y_train, epochs=5, batch_size=64)
```

## Install Tensorcircuit

```bash
pip install tensorcircuit
```

## Benchmarking

This data is collected by running `benchmarks/scripts/vqe_tc.py` 10 times and average results.

<table>
  <tr>
    <th></th>
    <th>Vanilla Tensorflow</th>
    <th>Apple Tensorflow</th>
    <th>Apple Tensorflow with Metal Plugin</th>
  </tr>
  <tr>
    <td>Construction Time</td>
    <td>11.49241641s</td>
    <td>11.31878941s</td>
    <td>11.6103961s</td>
  </tr>
  <tr>
    <td>Iteration time</td>
    <td>0.002313011s</td>
    <td>0.002333004s</td>
    <td>0.046412581s</td>
  </tr>
  <tr>
    <td>Total time</td>
    <td>11.72371747s</td>
    <td>11.55208979s</td>
    <td>16.25165417s</td>
  </tr>
</table>


Until July 2023, this has been tested on Intel Macs running Ventura, M1 Macs running Ventura, M2 Macs running Ventura, and M2 Macs running Sonoma beta.


================================================
FILE: docs/source/contribs/development_Mac_cn.md
================================================
# MacOS Tensorcircuit 安装教程

[_Mark (Zixuan) Song_](https://marksong.tech) 撰写

由于苹果更新了Tensorflow，因此M系列（直到M2）和英特尔系列Mac上的安装可以遵循完全相同的过程。

## 从头开始

对于全新的Macos或未安装Xcode的Macos。

若您已安装Xcode，请跳转到安装TC后端。

### 安装Xcode命令行工具

<font color=gray><em>需要对机器的图形访问</em></font>

如果网络良好，请运行`xcode-select --install`进行安装。

或者，如果网络连接不理想，请从[苹果](https://developer.apple.com/download/more/)下载命令行工具安装映像，然后进行安装。

## 安装TC后端

有四个后端可供选择，Numpy，Tensorflow，Jax和Torch。

### 安装Jax、Pytorch（可选）

```bash
pip install [Package Name]
```

### 安装Tensorflow（可选 - 推荐）

#### 安装miniconda（可选 - 推荐）

若您希望使用苹果为MacOS优化的Tensorflow（`tensorflow-macos`）或使用Tensorflow GPU优化（`tensorflow-metal`）请安装mimiconda。

若您希望使Google开发的原版Tensorflow（`tensorflow`）请跳过此步骤。

```bash
curl -o ~/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash ~/miniconda.sh -b -p $HOME/miniconda
source ~/miniconda/bin/activate
conda install -c apple tensorflow-deps
```

#### 安装步骤

```bash
pip install tensorflow
```

若您希望使用苹果为Tensorflow优化的Metal后端，请继续运行（不建议）：

```bash
pip install tensorflow-metal
```

#### 验证Tensorflow安装

```python
import tensorflow as tf

cifar = tf.keras.datasets.cifar100
(x_train, y_train), (x_test, y_test) = cifar.load_data()
model = tf.keras.applications.ResNet50(
    include_top=True,
    weights=None,
    input_shape=(32, 32, 3),
    classes=100,)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer="adam", loss=loss_fn, metrics=["accuracy"])
model.fit(x_train, y_train, epochs=5, batch_size=64)
```

## 安装Tensorcircuit

```bash
pip install tensorcircuit
```

## 测试与比较

以下数据由运行`benchmarks/scripts/vqe_tc.py` 10次并取平均值得到。

<table>
  <tr>
    <th></th>
    <th>原版Tensorflow</th>
    <th>苹果优化版Tensorflow</th>
    <th>苹果优化版Tensorflow并安装Tensorflow Metal插件</th>
  </tr>
  <tr>
    <td>构建时间</td>
    <td>11.49241641s</td>
    <td>11.31878941s</td>
    <td>11.6103961s</td>
  </tr>
  <tr>
    <td>迭代时间</td>
    <td>0.002313011s</td>
    <td>0.002333004s</td>
    <td>0.046412581s</td>
  </tr>
  <tr>
    <td>从时间</td>
    <td>11.72371747s</td>
    <td>11.55208979s</td>
    <td>16.25165417s</td>
  </tr>
</table>


直到2023年7月，这已在运行Ventura的英特尔i9 Mac、运行Ventura的M1 Mac、运行Ventura的M2 Mac、运行Sonoma测试版的M2 Mac上进行了测试。


================================================
FILE: docs/source/contribs/development_MacARM.md
================================================
# Tensorcircuit Installation Guide on MacOS

Contributed by Mark (Zixuan) Song

.. warning::
    This page is deprecated. Please visit `the update tutorial <development_Mac.html>`_ for the latest information.

## Starting From Scratch

For completely new macos or macos without xcode and brew

### Install Xcode Command Line Tools

<font color=gray><em>Need graphical access to the machine.</em></font>

Run `xcode-select --install` to install if on optimal internet.

Or Download from [Apple](https://developer.apple.com/download/more/) Command Line Tools installation image then install if internet connection is weak.

## Install Miniconda

Due to the limitation of MacOS and packages, the lastest version of python does not always function as desired, thus miniconda installation is advised to solve the issues.

```
curl -o ~/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash ~/miniconda.sh -b -p $HOME/miniconda
source ~/miniconda/bin/activate
conda install -c apple tensorflow-deps
```

## Install TC Prerequisites

```
pip install numpy scipy tensornetwork networkx
```

## Install TC Backends

There are four backends to choose from, Numpy, Tensorflow, Jax, Torch.

### Install Jax, Pytorch, Qiskit, Cirq (Optional)

```
pip install [Package Name]
```

### Install Tensorflow (Optional)

#### Install Tensorflow without MacOS optimization

```
conda config --add channels conda-forge
conda config --set channel_priority strict
conda create -n tc_venv python tensorflow=2.7
```

#### Verify Tensorflow Installation

```
import tensorflow as tf

cifar = tf.keras.datasets.cifar100
(x_train, y_train), (x_test, y_test) = cifar.load_data()
model = tf.keras.applications.ResNet50(
    include_top=True,
    weights=None,
    input_shape=(32, 32, 3),
    classes=100,)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer="adam", loss=loss_fn, metrics=["accuracy"])
model.fit(x_train, y_train, epochs=5, batch_size=64)
```

#### Install Tensorflow with MacOS optimization (Recommended)

For tensorflow version 2.13 or later:
```
pip install tensorflow
pip install tensorflow-metal
```

For tensorflow version 2.12 or earlier:
```
pip install tensorflow-macos
pip install tensorflow-metal
```

#### Verify Tensorflow Installation

```
import tensorflow as tf

cifar = tf.keras.datasets.cifar100
(x_train, y_train), (x_test, y_test) = cifar.load_data()
model = tf.keras.applications.ResNet50(
    include_top=True,
    weights=None,
    input_shape=(32, 32, 3),
    classes=100,)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer="adam", loss=loss_fn, metrics=["accuracy"])
model.fit(x_train, y_train, epochs=5, batch_size=64)
```

## Install Tensorcircuit

```
pip install tensorcircuit
```

Testing Platform (Tested Jun 2023)

- Platform 1:
  - MacOS Ventura 13.1 (Build version 22C65)
  - M1 Ultra
- Platform 2:
  - MacOS Ventura 13.2 (Build version 22D49)
  - M1 Ultra (Virtual)
- Platform 4:
  - MacOS Sonoma 14.0 Beta 2 (Build version 23A5276g)
  - M2 Max


================================================
FILE: docs/source/contribs/development_MacM1.rst
================================================
Run TensorCircuit on TensorlowBackend with Apple M1
========================================================
Contributed by (Yuqin Chen)


.. warning::
    This page is deprecated. Please visit `the update tutorial <development_Mac.html>`_ for the latest information.


Why We Can't Run TensorCircuit on TensorlowBackend with Apple M1
-----------------------------------------------------------------------
TensorCircuit requires Tensorflow to support TensorflowBackend. However for Apple M1, Tensorflow package cannot be properly installed by a usual method like "pip install tensorflow". As well, the TensorCircuit package cannot be properly installed by a usual method "pip install tensorcircuit"
All we need is to properly install tensorflow on Apple M1 Pro and then download the TensorCircuit package to the local and install it. 

Install tensorflow on Apple M1
------------------------------------
According to the instructions below or the installation manual on Apple's official website `tensorflow-metal PluggableDevice <https://developer.apple.com/metal/tensorflow-plugin/>`_, you can install tensorflow step by step.

**Step1: Environment setup**

x86 : AMD
Create virtual environment (recommended):

.. code-block:: bash

    python3 -m venv ~/tensorflow-metal

    source ~/tensorflow-metal/bin/activate

    python -m pip install -U pip

NOTE: python version 3.8 required

arm64 : Apple Silicon

Download and install Conda env:

.. code-block:: bash

    chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh

    sh ~/Downloads/Miniforge3-MacOSX-arm64.sh

    source ~/miniforge3/bin/activate
    
Install the TensorFlow dependencies:

.. code-block:: bash

    conda install -c apple tensorflow-deps

- When upgrading to new base TensorFlow version, recommend:

 .. code-block:: bash

    # uninstall existing tensorflow-macos and tensorflow-metal

    python -m pip uninstall tensorflow-macos

    python -m pip uninstall tensorflow-metal

    # Upgrade tensorflow-deps

    conda install -c apple tensorflow-deps --force-reinstall

    # or point to specific conda environment

    conda install -c apple tensorflow-deps --force-reinstall -n my_env

- tensorflow-deps versions are following base TensorFlow versions so:

for v2.5

.. code-block:: bash

    conda install -c apple tensorflow-deps==2.5.0



for v2.6

.. code-block:: bash

    conda install -c apple tensorflow-deps==2.6.0


**Step2: Install base TensorFlow**

.. code-block:: bash

    python -m pip install tensorflow-macos

**Step3: Install tensorflow-metal plugin**

.. code-block:: bash

    python -m pip install tensorflow-metal


Install TensorCircuit on Apple M1
-----------------------------------
After properly install tensorflow, you can continue install TensorCircuit. 
Up to now, for Apple M1, the Tensorcircuit package can not be installed by simply
conducting "pip install tensorcircuit", which will lead to improper way for Tensorflow installation.
One need to download the installation package to the local, only in this way the installation proceess can recognize the Apple M1 environment. 

One should download the TensorCircuit package to local at first. 

.. code-block:: bash

    git clone https://github.com/tencent-quantum-lab/tensorcircuit.git


Then unpackage it, and cd into the folder with "setup.py". Conducting

.. code-block:: bash

    python setup.py build

    python setup.py install






================================================
FILE: docs/source/contribs/development_MacM2.md
================================================
# Tensorcircuit Installation Guide on MacOS

Contributed by [Hong-Ye Hu](https://github.com/hongyehu)

.. warning::
    This page is deprecated. Please visit `the update tutorial <development_Mac.html>`_ for the latest information.

The key issue addressed in this document is **how to install both TensorFlow and Jax on a M2 chip MacOS without conflict**. 

## Starting From Scratch

### Install Xcode Command Line Tools

<font color=gray><em>Need graphical access to the machine.</em></font>

Run `xcode-select --install` to install if on optimal internet.

Or Download from [Apple](https://developer.apple.com/download/more/) Command Line Tools installation image then install if internet connection is weak.

## Install Miniconda

Due to the limitation of MacOS and packages, the lastest version of python does not always function as desired, thus miniconda installation is advised to solve the issues. And use anaconda virtual environment is always a good habit.

```
curl -o ~/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash ~/miniconda.sh -b -p $HOME/miniconda
source ~/miniconda/bin/activate
```

## Install Packages
First, create a virtual environment, and make sure the python version is 3.8.5 by
```
conda create --name NewEnv python==3.8.5
conda activate NewEnv
```
Then, install the TensorFlow from `.whl` file (file can be downloaded from this [URL](https://drive.google.com/drive/folders/1oSipZLnoeQB0Awz8U68KYeCPsULy_dQ7)). This will install TensorFlow version 2.4.1
```
pip install ~/Downloads/tensorflow-2.4.1-py3-none-any.whl
```
Next, one need to install **Jax** and **Optax** by
```
conda install jax==0.3.0
conda install optax==0.1.4
```
Now, hopefully, you should be able to use both Jax and TensorFlow in this environment. But sometimes, it may give you an error "ERROR: package Chardet not found.". 
If that is the case, you can install it by `conda install chardet`.
Lastly, install tensorcircuit
```
pip install tensorcircuit
```
This is the solution that seems to work for M2-chip MacOS. Please let me know if there is a better solution!





================================================
FILE: docs/source/contribs/development_windows.rst
================================================
Run TensorCircuit on Windows Machine with Docker
========================================================

Contributed by `SexyCarrots <https://github.com/SexyCarrots>`_ (Xinghan Yang)

(For linux machines, please review the `Docker README for linux <https://github.com/tencent-quantum-lab/tensorcircuit/blob/master/docker/README.md>`_ )

This note is only a step-by-step tutorial to help you build and run a Docker Container for Windows Machine users with the given dockerfile. 
If you want to have a deeper dive in to Docker, please check the official `Docker Orientation <https://docs.docker.com/get-started/>`_
and free courses on `YouTube <https://www.youtube.com/results?search_query=docker+tutorial>`_.

Why We Can't Run TensorCircuit on Windows Machine
---------------------------------------------------------------

Due to the compatability issue with the `JAX <https://jax.readthedocs.io/en/latest/index.html>`_ backend on Windows,
we could not directly use jax backend for TensorCircuit on Windows machines. Please be aware that it is possible to `install
JAX on Windows <https://jax.readthedocs.io/en/latest/developer.html>`_, but it is tricky and not recommended unless
you have solid understanding of Windows environment and C++ tools. Virtual machine is also an option for development if
you are familiar with it. In this tutorial we would discuss the deployment of Docker for TensorCircuit since it use 
the most convenient and workable solution for beginners.

What Is Docker
------------------

Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly.
With Docker, you can manage your infrastructure in the same way as you manage your applications. By taking advantage of Docker's methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.

(Source: https://docs.docker.com/get-started/overview/) 

For more information and tutorials on Docker, you could check the `Docker Documentation <https://docs.docker.com/get-started/overview/>`_.

Install Docker and Docker Desktop
---------------------------------------------

`Download Docker Desktop for Windows <https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe>`_ for and install it by following its instructions.

*Following information is from the official Docker Doc: https://docs.docker.com/desktop/windows/install/*

**Install interactively**

- If you haven't already downloaded the installer (Docker Desktop Installer.exe), you can get it from Docker Hub. It typically downloads to your Downloads folder, or you can run it from the recent downloads bar at the bottom of your web browser.

- When prompted, ensure the Use WSL 2 instead of Hyper-V option on the Configuration page is selected or not depending on your choice of backend.

- If your system only supports one of the two options, you will not be able to select which backend to use.

- Follow the instructions on the installation wizard to authorize the installer and proceed with the install.

- When the installation is successful, click Close to complete the installation process.

- If your admin account is different to your user account, you must add the user to the docker-users group.
Run Computer Management as an administrator and navigate to Local Users and Groups > Groups > docker-users. Right-click to add the user to the group. Log out and log back in for the changes to take effect.

**Install from the command line**

After downloading Docker Desktop Installer.exe, run the following command in a terminal to install Docker Desktop:

.. code-block:: bash

    "Docker Desktop Installer.exe" install

If you're using PowerShell you should run it as:

.. code-block:: bash

    Start-Process '.\win\build\Docker Desktop Installer.exe' -Wait install

If using the Windows Command Prompt:

.. code-block:: bash

    start /w "" "Docker Desktop Installer.exe" install

Build Image in through PyCharm or Command Line Interface
--------------------------------------------------------

**First of all**, run docker desktop.

**For CLI command:**

Go to your local ``./tensorcircuit/docker`` directory, then open your local CLI.

.. code-block:: bash

    cd ./tensorcircuit/docker

Use the command:

.. code-block:: bash

    docker build .

It could take more than fifteen minutes to build the docker image, depending on your internet and computer hardware.
Please keep your computer active while building the docker image. You need to build the image again from scratch if
there is any interruption during the building.

**For PyCharm:**

Install the docker plugin within Pycharm, than open the dockerfile in the ``./tensorcircuit/docker`` directory.
Choose Dockerfile to be the configuration, then run the dockerfile.
Please keep your computer active while building the docker image. You need to build the image again from scratch if
there is any interruption during the building.

Run Docker Image and Examples in TensorCircuit
--------------------------------------------------------

Open your CLI

Find your local images by:

.. code-block:: bash

    docker images

Run image as a container by:

.. code-block:: bash

    docker run [image name]

List existing containers by:

.. code-block:: bash

    docker ps

Then, open docker desktop and open docker CLI:

.. code-block:: bash

    ls

You would see all files and directories in ``./tensorcircuit/`` listed.

Go to the dir where all examples are:

.. code-block:: bash

    cd examples

Again, to see all the examples:

.. code-block:: bash

    ls

We would run noisy_qml.py to see what would happen:

.. code-block:: bash

    python noisy_qml.py

See the result and play with other example for a while. Latter you could start developing your own projects within
the docker container we just built. Enjoy your time with TensorCircuit.

*Please don't hesitate to create a New issue in GitHub if you find problems or have anything for discussion with other contributors*



================================================
FILE: docs/source/contribs/development_wsl2.rst
================================================
Run TensorCirit on Windows with WSL2 (Windows Subsystem for Linux 2)
===========================================================================

Contributed by `YHPeter <https://github.com/YHPeter>`_ (Peter Yu)

Reminder, if you are not supposed to use JAX, you can still use Numpy/Tensorflow/Pytorch backend to run demonstrations.

Step 1.
Install WSL2, follow the official installation instruction: https://docs.microsoft.com/en-us/windows/wsl/install

Step 2.
Install CUDA for GPU support, if you want to used GPU accelerator.
The official CUDA installation for WSL2: https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch02-getting-started

Step 3.
Follow the Linux Installation Instructions to finish installing.

.. list-table:: **System Support Summary**
   :header-rows: 1

   * - Backend
     - Numpy
     - TensorFlow
     - JAX
     - Pytorch
   * - Suggested Package Version
     - >= 1.20.0
     - >= 2.7.0
     - >= 0.3.0
     - >= 1.12
   * - OS Support without GPU Accelerator
     - Windows/MacOS/Linux
     - Windows/MacOS/Linux
     - Windows/MacOS/Linux
     - Windows/MacOS/Linux
   * - OS Support with GPU Accelerator
     - No Support for GPU
     - Windows(WSL2, docker)/`MacOS <https://developer.apple.com/metal/tensorflow-plugin>`_/Linux
     - Windows(WSL2, docker)/MacOS/Linux
     - Windows(WSL2, docker)/MacOS(torch>=1.12)/Linux
   * - Platform with TPU Accelerator
     - No Support for TPU
     - `GCP - Tensorflow with TPU <https://cloud.google.com/tpu/docs/run-calculation-tensorflow>`_
     - `GCP - JAX with TPU <https://cloud.google.com/tpu/docs/run-calculation-jax>`_
     - `GCP - Pytorch with TPU <https://cloud.google.com/tpu/docs/run-calculation-pytorch>`_

Tips: Currently, we don't suggest you to use TPU accelerator.


================================================
FILE: docs/source/locale/zh/LC_MESSAGES/advance.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-01-13 11:04+0800\n"
"PO-Revision-Date: 2022-04-11 07:50+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/advance.rst:3
msgid "Advanced Usage"
msgstr "高级用法"

#: ../../source/advance.rst:6
msgid "MPS Simulator"
msgstr "MPS 模拟器"

#: ../../source/advance.rst:8
msgid "(Still experimental support)"
msgstr "施工中"

#: ../../source/advance.rst:10
msgid ""
"Very simple, we provide the same set of API for ``MPSCircuit`` as "
"``Circuit``, the only new line is to set the bond dimension for the new "
"simulator."
msgstr ""

#: ../../source/advance.rst:18
msgid ""
"The larger bond dimension we set, the better approximation ratio (of "
"course the more computational cost we pay)"
msgstr ""

#: ../../source/advance.rst:21
msgid "Split Two-qubit Gates"
msgstr "分解双量子比特门"

#: ../../source/advance.rst:23
msgid ""
"The two-qubit gates applied on the circuit can be decomposed via SVD, "
"which may further improve the optimality of the contraction pathfinding."
msgstr "应用在电路上的双量子比特门可以通过 SVD 进行分解，这可以进一步提高收缩路径查找的最优性。"

#: ../../source/advance.rst:25
msgid "`split` configuration can be set at circuit-level or gate-level."
msgstr "`split` 配置可以在电路级或门级设置。"

#: ../../source/advance.rst:46
msgid ""
"Note ``max_singular_values`` must be specified to make the whole "
"procedure static and thus jittable."
msgstr "请注意 ``max_singular_values`` 必须被指定明以使整个过程成为静态的，因此是可即时编译的。"

#: ../../source/advance.rst:50
msgid "Jitted Function Save/Load"
msgstr "即时编译函的保存/加载"

#: ../../source/advance.rst:52
msgid ""
"To reuse the jitted function, we can save it on the disk via support from"
" the TensorFlow `SavedModel "
"<https://www.tensorflow.org/guide/saved_model>`_. That is to say, only "
"jitted quantum function on the TensorFlow backend can be saved on the "
"disk."
msgstr ""
"要重新使用可即时编译函数，我们可以通过 TensorFlow `SavedModel "
"<https://www.tensorflow.org/guide/saved_model>`_. 的帮助将其保存在磁盘上。也就是说，只有 "
"TensorFlow 后端的可即时编译量子函数才能保存在磁盘上。"

#: ../../source/advance.rst:54
msgid ""
"For the JAX-backend quantum function, one can first transform them into "
"the tf-backend function via JAX experimental support: `jax2tf "
"<https://github.com/google/jax/tree/main/jax/experimental/jax2tf>`_."
msgstr ""
"对于 jax-backend 量子函数，可以先通过 jax 实验支持将它们转换为 tf-backend 函数: `jax2tf "
"<https://github.com/google/jax/tree/main/jax/experimental/jax2tf>`_。"

#: ../../source/advance.rst:56
msgid ""
"We wrap the tf-backend `SavedModel` as very easy-to-use function "
":py:meth:`tensorcircuit.keras.save_func` and "
":py:meth:`tensorcircuit.keras.load_func`."
msgstr ""
"我们将 tf-backend `SavedModel` 包装为非常易于使用的函数 "
":py:meth:`tensorcircuit.keras.save_func` 和 "
":py:meth:`tensorcircuit.keras.load_func`。"

#: ../../source/advance.rst:59
msgid "Parameterized Measurements"
msgstr "参数化测量"

#: ../../source/advance.rst:61
msgid ""
"For plain measurements API on a ``tc.Circuit``, eg. `c = "
"tc.Circuit(n=3)`, if we want to evaluate the expectation "
":math:`<Z_1Z_2>`, we need to call the API as "
"``c.expectation((tc.gates.z(), [1]), (tc.gates.z(), [2]))``."
msgstr ""
"对于 ``tc.Circuit`` 上的普通测量 API, 例如 `c = tc.Circuit(n=3)`, 如果我们要评估期望 "
":math:`<Z_1Z_2>`, 我们需要调用API为 ``c.expectation((tc.gates.z(), [1]), "
"(tc.gates.z(), [2]))``。"

#: ../../source/advance.rst:63
msgid ""
"In some cases, we may want to tell the software what to measure but in a "
"tensor fashion. For example, if we want to get the above expectation, we "
"can use the following API: "
":py:meth:`tensorcircuit.templates.measurements.parameterized_measurements`."
msgstr ""
"在某些情况下，我们可能希望以张量形式告诉软件要测量什么。例如，如果我们想获得上述期望，我们可以使用以下 API ： "
":py:meth:`tensorcircuit.templates.measurements.parameterized_measurements`。"

#: ../../source/advance.rst:70
msgid ""
"This API corresponds to measure :math:`I_0Z_1Z_2I_3` where 0, 1, 2, 3 are"
" for local I, X, Y, and Z operators respectively."
msgstr "此 API 对应于测量 :math:`I_0Z_1Z_2I_3`， 其中 0、1、2、3 分别用于 I、X、Y、Z 局部运算符。"

#: ../../source/advance.rst:73
msgid "Sparse Matrix"
msgstr "稀疏矩阵"

#: ../../source/advance.rst:75
msgid ""
"We support COO format sparse matrix as most backends only support this "
"format, and some common backend methods for sparse matrices are listed "
"below:"
msgstr "我们只支持 COO 格式的稀疏矩阵，因为大多数后端只支持这种格式，下面列出了一些常用的稀疏矩阵后端方法："

#: ../../source/advance.rst:90
msgid ""
"The sparse matrix is specifically useful to evaluate Hamiltonian "
"expectation on the circuit, where sparse matrix representation has a good"
" tradeoff between space and time. Please refer to "
":py:meth:`tensorcircuit.templates.measurements.sparse_expectation` for "
"more detail."
msgstr ""
"稀疏矩阵对于评估电路上的哈密顿期望特别有用，其中稀疏矩阵表示在空间和时间之间具有良好的效率。请参阅 "
":py:meth:`tensorcircuit.templates.measurements.sparse_expectation` "
"了解更多详细信息。"

#: ../../source/advance.rst:93
msgid ""
"For different representations to evaluate Hamiltonian expectation in "
"tensorcircuit, please refer to :doc:`tutorials/tfim_vqe_diffreph`."
msgstr "对于在张量电路中评估哈密顿期望的不同表示，请参阅 :doc:`tutorials/tfim_vqe_diffreph` 。"

#: ../../source/advance.rst:96
msgid "Randoms, Jit, Backend Agnostic, and Their Interplay"
msgstr "随机数，即时编译，后端无关特性，和他们的相互作用"

#: ../../source/advance.rst:139
msgid ""
"Therefore, a unified jittable random infrastructure with backend agnostic"
" can be formulated as"
msgstr "因此，一个与后端无关并且统一可即时编译的随机基础设施可以表述为"

#: ../../source/advance.rst:167
msgid "And a more neat approach to achieve this is as follows:"
msgstr "实现这一目标的更简洁的方法如下："

#: ../../source/advance.rst:182
msgid ""
"It is worth noting that since ``Circuit.unitary_kraus`` and "
"``Circuit.general_kraus`` call ``implicit_rand*`` API, the correct usage "
"of these APIs is the same as above."
msgstr ""
"值得注意的是，由于 ``Circuit.unitary_kraus`` 和 ``Circuit.general_kraus`` 调用 "
"``implicit_rand*`` API，这些 API 的正确用法与上面相同。"

#: ../../source/advance.rst:184
msgid ""
"One may wonder why random numbers are dealt in such a complicated way, "
"please refer to the `Jax design note "
"<https://github.com/google/jax/blob/main/docs/design_notes/prng.md>`_ for"
" some hints."
msgstr ""
"有人可能想知道为什么以如此复杂的方式处理随机数，请参阅 `Jax 设计说明 "
"<https://github.com/google/jax/blob/main/docs/design_notes/prng.md>`_  "
"提示。"

#: ../../source/advance.rst:186
msgid ""
"If vmap is also involved apart from jit, I currently find no way to "
"maintain the backend agnosticity as TensorFlow seems to have no support "
"of vmap over random keys (ping me on GitHub if you think you have a way "
"to do this). I strongly recommend the users using Jax backend in the "
"vmap+random setup."
msgstr ""




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/contribs.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-07-14 15:43+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/contribs/development_Mac.md:1
#: ../../source/contribs/development_MacARM.md:1
#: ../../source/contribs/development_MacM2.md:1
msgid "Tensorcircuit Installation Guide on MacOS"
msgstr ""

#: ../../source/contribs/development_Mac.md:3
msgid "Contributed by [_Mark (Zixuan) Song_](https://marksong.tech)"
msgstr ""

#: ../../source/contribs/development_Mac.md:5
msgid ""
"Apple has updated Tensorflow (for MacOS) so that installation on M-series"
" (until M2) and Intel-series Mac can follow the exact same procedure."
msgstr ""

#: ../../source/contribs/development_Mac.md:7
#: ../../source/contribs/development_MacARM.md:8
#: ../../source/contribs/development_MacM2.md:10
msgid "Starting From Scratch"
msgstr ""

#: ../../source/contribs/development_Mac.md:9
msgid "For completely new Macos or Macos without Xcode and Homebrew installed."
msgstr ""

#: ../../source/contribs/development_Mac.md:11
#: ../../source/contribs/development_MacARM.md:12
#: ../../source/contribs/development_MacM2.md:12
msgid "Install Xcode Command Line Tools"
msgstr ""

#: ../../source/contribs/development_Mac.md:13
#: ../../source/contribs/development_MacARM.md:14
#: ../../source/contribs/development_MacM2.md:14
msgid "<font color=gray><em>Need graphical access to the machine.</em></font>"
msgstr ""

#: ../../source/contribs/development_Mac.md:15
#: ../../source/contribs/development_MacARM.md:16
#: ../../source/contribs/development_MacM2.md:16
msgid "Run `xcode-select --install` to install if on optimal internet."
msgstr ""

#: ../../source/contribs/development_Mac.md:17
msgid ""
"Or Download it from [Apple](https://developer.apple.com/download/more/) "
"Command Line Tools installation image then install it if the internet "
"connection is weak."
msgstr ""

#: ../../source/contribs/development_Mac.md:19
#: ../../source/contribs/development_MacARM.md:20
#: ../../source/contribs/development_MacM2.md:20
msgid "Install Miniconda"
msgstr ""

#: ../../source/contribs/development_Mac.md:21
msgid ""
"Due to the limitation of MacOS and packages, the latest version of Python"
" does not always function as desired, thus miniconda installation is "
"advised to solve the issues."
msgstr ""

#: ../../source/contribs/development_Mac.md:30
#: ../../source/contribs/development_MacARM.md:37
msgid "Install TC Backends"
msgstr ""

#: ../../source/contribs/development_Mac.md:32
msgid "There are four backends to choose from, Numpy, Tensorflow, Jax, and Torch."
msgstr ""

#: ../../source/contribs/development_Mac.md:34
#: ../../source/contribs/development_MacARM.md:41
msgid "Install Jax, Pytorch, Qiskit, Cirq (Optional)"
msgstr ""

#: ../../source/contribs/development_Mac.md:40
#: ../../source/contribs/development_MacARM.md:47
msgid "Install Tensorflow (Optional)"
msgstr ""

#: ../../source/contribs/development_Mac.md:42
msgid "Installation"
msgstr ""

#: ../../source/contribs/development_Mac.md:44
msgid "For Tensorflow version 2.13 or later:"
msgstr ""

#: ../../source/contribs/development_Mac.md:50
msgid "For Tensorflow version 2.12 or earlier:"
msgstr ""

#: ../../source/contribs/development_Mac.md:56
#: ../../source/contribs/development_MacARM.md:57
#: ../../source/contribs/development_MacARM.md:89
msgid "Verify Tensorflow Installation"
msgstr ""

#: ../../source/contribs/development_Mac.md:74
#: ../../source/contribs/development_MacARM.md:107
msgid "Install Tensorcircuit"
msgstr ""

#: ../../source/contribs/development_Mac.md:80
msgid ""
"Until July 2023, this has been tested on Intel Macs running Ventura, M1 "
"Macs running Ventura, M2 Macs running Ventura, and M2 Macs running Sonoma"
" beta."
msgstr ""

#: ../../source/contribs/development_MacARM.md:3
msgid "Contributed by Mark (Zixuan) Song"
msgstr ""

#: ../../source/contribs/development_MacARM.md:5
#: ../../source/contribs/development_MacM2.md:5
msgid ""
".. warning::     This page is deprecated. Please visit `the update "
"tutorial <development_Mac.html>`_ for the latest information."
msgstr ""

#: ../../source/contribs/development_MacARM.md:10
msgid "For completely new macos or macos without xcode and brew"
msgstr ""

#: ../../source/contribs/development_MacARM.md:18
#: ../../source/contribs/development_MacM2.md:18
msgid ""
"Or Download from [Apple](https://developer.apple.com/download/more/) "
"Command Line Tools installation image then install if internet connection"
" is weak."
msgstr ""

#: ../../source/contribs/development_MacARM.md:22
msgid ""
"Due to the limitation of MacOS and packages, the lastest version of "
"python does not always function as desired, thus miniconda installation "
"is advised to solve the issues."
msgstr ""

#: ../../source/contribs/development_MacARM.md:31
msgid "Install TC Prerequisites"
msgstr ""

#: ../../source/contribs/development_MacARM.md:39
msgid "There are four backends to choose from, Numpy, Tensorflow, Jax, Torch."
msgstr ""

#: ../../source/contribs/development_MacARM.md:49
msgid "Install Tensorflow without MacOS optimization"
msgstr ""

#: ../../source/contribs/development_MacARM.md:75
msgid "Install Tensorflow with MacOS optimization (Recommended)"
msgstr ""

#: ../../source/contribs/development_MacARM.md:77
msgid "For tensorflow version 2.13 or later:"
msgstr ""

#: ../../source/contribs/development_MacARM.md:83
msgid "For tensorflow version 2.12 or earlier:"
msgstr ""

#: ../../source/contribs/development_MacARM.md:113
msgid "Testing Platform (Tested Jun 2023)"
msgstr ""

#: ../../source/contribs/development_MacARM.md:115
msgid "Platform 1:"
msgstr ""

#: ../../source/contribs/development_MacARM.md:116
msgid "MacOS Ventura 13.1 (Build version 22C65)"
msgstr ""

#: ../../source/contribs/development_MacARM.md:117
msgid "M1 Ultra"
msgstr ""

#: ../../source/contribs/development_MacARM.md:118
msgid "Platform 2:"
msgstr ""

#: ../../source/contribs/development_MacARM.md:119
msgid "MacOS Ventura 13.2 (Build version 22D49)"
msgstr ""

#: ../../source/contribs/development_MacARM.md:120
msgid "M1 Ultra (Virtual)"
msgstr ""

#: ../../source/contribs/development_MacARM.md:121
msgid "Platform 4:"
msgstr ""

#: ../../source/contribs/development_MacARM.md:122
msgid "MacOS Sonoma 14.0 Beta 2 (Build version 23A5276g)"
msgstr ""

#: ../../source/contribs/development_MacARM.md:123
msgid "M2 Max"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:2
msgid "Run TensorCircuit on TensorlowBackend with Apple M1"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:3
msgid "Contributed by (Yuqin Chen)"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:7
msgid ""
"This page is deprecated. Please visit `the update tutorial "
"<development_Mac.html>`_ for the latest information."
msgstr ""

#: ../../source/contribs/development_MacM1.rst:11
msgid "Why We Can't Run TensorCircuit on TensorlowBackend with Apple M1"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:12
msgid ""
"TensorCircuit requires Tensorflow to support TensorflowBackend. However "
"for Apple M1, Tensorflow package cannot be properly installed by a usual "
"method like \"pip install tensorflow\". As well, the TensorCircuit "
"package cannot be properly installed by a usual method \"pip install "
"tensorcircuit\" All we need is to properly install tensorflow on Apple M1"
" Pro and then download the TensorCircuit package to the local and install"
" it."
msgstr ""

#: ../../source/contribs/development_MacM1.rst:16
msgid "Install tensorflow on Apple M1"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:17
msgid ""
"According to the instructions below or the installation manual on Apple's"
" official website `tensorflow-metal PluggableDevice "
"<https://developer.apple.com/metal/tensorflow-plugin/>`_, you can install"
" tensorflow step by step."
msgstr ""

#: ../../source/contribs/development_MacM1.rst:19
msgid "**Step1: Environment setup**"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:21
msgid "x86 : AMD Create virtual environment (recommended):"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:32
msgid "NOTE: python version 3.8 required"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:34
msgid "arm64 : Apple Silicon"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:36
msgid "Download and install Conda env:"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:46
msgid "Install the TensorFlow dependencies:"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:52
msgid "When upgrading to new base TensorFlow version, recommend:"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:70
msgid "tensorflow-deps versions are following base TensorFlow versions so:"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:72
msgid "for v2.5"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:80
msgid "for v2.6"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:87
msgid "**Step2: Install base TensorFlow**"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:93
msgid "**Step3: Install tensorflow-metal plugin**"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:101
msgid "Install TensorCircuit on Apple M1"
msgstr ""

#: ../../source/contribs/development_MacM1.rst:102
msgid ""
"After properly install tensorflow, you can continue install "
"TensorCircuit. Up to now, for Apple M1, the Tensorcircuit package can not"
" be installed by simply conducting \"pip install tensorcircuit\", which "
"will lead to improper way for Tensorflow installation. One need to "
"download the installation package to the local, only in this way the "
"installation proceess can recognize the Apple M1 environment."
msgstr ""

#: ../../source/contribs/development_MacM1.rst:107
msgid "One should download the TensorCircuit package to local at first."
msgstr ""

#: ../../source/contribs/development_MacM1.rst:114
msgid "Then unpackage it, and cd into the folder with \"setup.py\". Conducting"
msgstr ""

#: ../../source/contribs/development_MacM2.md:3
msgid "Contributed by [Hong-Ye Hu](https://github.com/hongyehu)"
msgstr ""

#: ../../source/contribs/development_MacM2.md:8
msgid ""
"The key issue addressed in this document is **how to install both "
"TensorFlow and Jax on a M2 chip MacOS without conflict**."
msgstr ""

#: ../../source/contribs/development_MacM2.md:22
msgid ""
"Due to the limitation of MacOS and packages, the lastest version of "
"python does not always function as desired, thus miniconda installation "
"is advised to solve the issues. And use anaconda virtual environment is "
"always a good habit."
msgstr ""

#: ../../source/contribs/development_MacM2.md:30
msgid "Install Packages"
msgstr ""

#: ../../source/contribs/development_MacM2.md:31
msgid ""
"First, create a virtual environment, and make sure the python version is "
"3.8.5 by"
msgstr ""

#: ../../source/contribs/development_MacM2.md:36
msgid ""
"Then, install the TensorFlow from `.whl` file (file can be downloaded "
"from this "
"[URL](https://drive.google.com/drive/folders/1oSipZLnoeQB0Awz8U68KYeCPsULy_dQ7))."
" This will install TensorFlow version 2.4.1"
msgstr ""

#: ../../source/contribs/development_MacM2.md:40
msgid "Next, one need to install **Jax** and **Optax** by"
msgstr ""

#: ../../source/contribs/development_MacM2.md:45
msgid ""
"Now, hopefully, you should be able to use both Jax and TensorFlow in this"
" environment. But sometimes, it may give you an error \"ERROR: package "
"Chardet not found.\".  If that is the case, you can install it by `conda "
"install chardet`. Lastly, install tensorcircuit"
msgstr ""

#: ../../source/contribs/development_MacM2.md:51
msgid ""
"This is the solution that seems to work for M2-chip MacOS. Please let me "
"know if there is a better solution!"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:1
msgid "MacOS Tensorcircuit 安装教程"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:3
msgid "[_Mark (Zixuan) Song_](https://marksong.tech) 撰写"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:5
msgid "由于苹果更新了Tensorflow，因此M系列（直到M2）和英特尔系列Mac上的安装可以遵循完全相同的过程。"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:7
msgid "从头开始"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:9
msgid "对于全新的Macos或未安装Xcode和Homebrew的Macos。"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:11
msgid "安装Xcode命令行工具"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:13
msgid "<font color=gray><em>需要对机器的图形访问。</em></font>"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:15
msgid "如果网络良好，请运行`xcode-select --install`进行安装。"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:17
msgid "或者，如果网络连接较弱，请从[苹果](https://developer.apple.com/download/more/)下载命令行工具安装映像，然后进行安装。"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:19
msgid "安装Miniconda"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:21
msgid "由于MacOS和软件包的限制，因此建议安装miniconda以解决问题。"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:30
msgid "安装TC后端"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:32
msgid "有四个后端可供选择，Numpy，Tensorflow，Jax和Torch。"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:34
msgid "安装Jax，Pytorch，Qiskit，Cirq（可选）"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:40
msgid "安装Tensorflow（可选）"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:42
msgid "安装步骤"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:44
msgid "Tensorflow版本2.13或之后："
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:50
msgid "Tensorflow版本2.12或之前："
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:56
msgid "验证Tensorflow安装"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:74
msgid "安装Tensorcircuit"
msgstr ""

#: ../../source/contribs/development_Mac_cn.md:80
msgid ""
"直到2023年7月，这已在运行Ventura的英特尔i9 Mac、运行Ventura的M1 Mac、运行Ventura的M2 "
"Mac、运行Sonoma测试版的M2 Mac上进行了测试。"
msgstr ""

#: ../../source/contribs/development_windows.rst:2
msgid "Run TensorCircuit on Windows Machine with Docker"
msgstr ""

#: ../../source/contribs/development_windows.rst:4
msgid ""
"Contributed by `SexyCarrots <https://github.com/SexyCarrots>`_ (Xinghan "
"Yang)"
msgstr ""

#: ../../source/contribs/development_windows.rst:6
msgid ""
"(For linux machines, please review the `Docker README for linux "
"<https://github.com/tencent-quantum-"
"lab/tensorcircuit/blob/master/docker/README.md>`_ )"
msgstr ""

#: ../../source/contribs/development_windows.rst:8
msgid ""
"This note is only a step-by-step tutorial to help you build and run a "
"Docker Container for Windows Machine users with the given dockerfile. If "
"you want to have a deeper dive in to Docker, please check the official "
"`Docker Orientation <https://docs.docker.com/get-started/>`_ and free "
"courses on `YouTube "
"<https://www.youtube.com/results?search_query=docker+tutorial>`_."
msgstr ""

#: ../../source/contribs/development_windows.rst:13
msgid "Why We Can't Run TensorCircuit on Windows Machine"
msgstr ""

#: ../../source/contribs/development_windows.rst:15
msgid ""
"Due to the compatability issue with the `JAX "
"<https://jax.readthedocs.io/en/latest/index.html>`_ backend on Windows, "
"we could not directly use jax backend for TensorCircuit on Windows "
"machines. Please be aware that it is possible to `install JAX on Windows "
"<https://jax.readthedocs.io/en/latest/developer.html>`_, but it is tricky"
" and not recommended unless you have solid understanding of Windows "
"environment and C++ tools. Virtual machine is also an option for "
"development if you are familiar with it. In this tutorial we would "
"discuss the deployment of Docker for TensorCircuit since it use the most "
"convenient and workable solution for beginners."
msgstr ""

#: ../../source/contribs/development_windows.rst:23
msgid "What Is Docker"
msgstr ""

#: ../../source/contribs/development_windows.rst:25
msgid ""
"Docker is an open platform for developing, shipping, and running "
"applications. Docker enables you to separate your applications from your "
"infrastructure so you can deliver software quickly. With Docker, you can "
"manage your infrastructure in the same way as you manage your "
"applications. By taking advantage of Docker's methodologies for shipping,"
" testing, and deploying code quickly, you can significantly reduce the "
"delay between writing code and running it in production."
msgstr ""

#: ../../source/contribs/development_windows.rst:28
msgid "(Source: https://docs.docker.com/get-started/overview/)"
msgstr ""

#: ../../source/contribs/development_windows.rst:30
msgid ""
"For more information and tutorials on Docker, you could check the `Docker"
" Documentation <https://docs.docker.com/get-started/overview/>`_."
msgstr ""

#: ../../source/contribs/development_windows.rst:33
msgid "Install Docker and Docker Desktop"
msgstr ""

#: ../../source/contribs/development_windows.rst:35
msgid ""
"`Download Docker Desktop for Windows "
"<https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe>`_"
" for and install it by following its instructions."
msgstr ""

#: ../../source/contribs/development_windows.rst:37
msgid ""
"*Following information is from the official Docker Doc: "
"https://docs.docker.com/desktop/windows/install/*"
msgstr ""

#: ../../source/contribs/development_windows.rst:39
msgid "**Install interactively**"
msgstr ""

#: ../../source/contribs/development_windows.rst:41
msgid ""
"If you haven't already downloaded the installer (Docker Desktop "
"Installer.exe), you can get it from Docker Hub. It typically downloads to"
" your Downloads folder, or you can run it from the recent downloads bar "
"at the bottom of your web browser."
msgstr ""

#: ../../source/contribs/development_windows.rst:43
msgid ""
"When prompted, ensure the Use WSL 2 instead of Hyper-V option on the "
"Configuration page is selected or not depending on your choice of "
"backend."
msgstr ""

#: ../../source/contribs/development_windows.rst:45
msgid ""
"If your system only supports one of the two options, you will not be able"
" to select which backend to use."
msgstr ""

#: ../../source/contribs/development_windows.rst:47
msgid ""
"Follow the instructions on the installation wizard to authorize the "
"installer and proceed with the install."
msgstr ""

#: ../../source/contribs/development_windows.rst:49
msgid ""
"When the installation is successful, click Close to complete the "
"installation process."
msgstr ""

#: ../../source/contribs/development_windows.rst:51
msgid ""
"If your admin account is different to your user account, you must add the"
" user to the docker-users group."
msgstr ""

#: ../../source/contribs/development_windows.rst:52
msgid ""
"Run Computer Management as an administrator and navigate to Local Users "
"and Groups > Groups > docker-users. Right-click to add the user to the "
"group. Log out and log back in for the changes to take effect."
msgstr ""

#: ../../source/contribs/development_windows.rst:54
msgid "**Install from the command line**"
msgstr ""

#: ../../source/contribs/development_windows.rst:56
msgid ""
"After downloading Docker Desktop Installer.exe, run the following command"
" in a terminal to install Docker Desktop:"
msgstr ""

#: ../../source/contribs/development_windows.rst:62
msgid "If you're using PowerShell you should run it as:"
msgstr ""

#: ../../source/contribs/development_windows.rst:68
msgid "If using the Windows Command Prompt:"
msgstr ""

#: ../../source/contribs/development_windows.rst:75
msgid "Build Image in through PyCharm or Command Line Interface"
msgstr ""

#: ../../source/contribs/development_windows.rst:77
msgid "**First of all**, run docker desktop."
msgstr ""

#: ../../source/contribs/development_windows.rst:79
msgid "**For CLI command:**"
msgstr ""

#: ../../source/contribs/development_windows.rst:81
msgid ""
"Go to your local ``./tensorcircuit/docker`` directory, then open your "
"local CLI."
msgstr ""

#: ../../source/contribs/development_windows.rst:87
msgid "Use the command:"
msgstr ""

#: ../../source/contribs/development_windows.rst:93
msgid ""
"It could take more than fifteen minutes to build the docker image, "
"depending on your internet and computer hardware. Please keep your "
"computer active while building the docker image. You need to build the "
"image again from scratch if there is any interruption during the "
"building."
msgstr ""

#: ../../source/contribs/development_windows.rst:97
msgid "**For PyCharm:**"
msgstr ""

#: ../../source/contribs/development_windows.rst:99
msgid ""
"Install the docker plugin within Pycharm, than open the dockerfile in the"
" ``./tensorcircuit/docker`` directory. Choose Dockerfile to be the "
"configuration, then run the dockerfile. Please keep your computer active "
"while building the docker image. You need to build the image again from "
"scratch if there is any interruption during the building."
msgstr ""

#: ../../source/contribs/development_windows.rst:105
msgid "Run Docker Image and Examples in TensorCircuit"
msgstr ""

#: ../../source/contribs/development_windows.rst:107
msgid "Open your CLI"
msgstr ""

#: ../../source/contribs/development_windows.rst:109
msgid "Find your local images by:"
msgstr ""

#: ../../source/contribs/development_windows.rst:115
msgid "Run image as a container by:"
msgstr ""

#: ../../source/contribs/development_windows.rst:121
msgid "List existing containers by:"
msgstr ""

#: ../../source/contribs/development_windows.rst:127
msgid "Then, open docker desktop and open docker CLI:"
msgstr ""

#: ../../source/contribs/development_windows.rst:133
msgid "You would see all files and directories in ``./tensorcircuit/`` listed."
msgstr ""

#: ../../source/contribs/development_windows.rst:135
msgid "Go to the dir where all examples are:"
msgstr ""

#: ../../source/contribs/development_windows.rst:141
msgid "Again, to see all the examples:"
msgstr ""

#: ../../source/contribs/development_windows.rst:147
msgid "We would run noisy_qml.py to see what would happen:"
msgstr ""

#: ../../source/contribs/development_windows.rst:153
msgid ""
"See the result and play with other example for a while. Latter you could "
"start developing your own projects within the docker container we just "
"built. Enjoy your time with TensorCircuit."
msgstr ""

#: ../../source/contribs/development_windows.rst:156
msgid ""
"*Please don't hesitate to create a New issue in GitHub if you find "
"problems or have anything for discussion with other contributors*"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:2
msgid "Run TensorCirit on Windows with WSL2 (Windows Subsystem for Linux 2)"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:4
msgid "Contributed by `YHPeter <https://github.com/YHPeter>`_ (Peter Yu)"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:6
msgid ""
"Reminder, if you are not supposed to use JAX, you can still use "
"Numpy/Tensorflow/Pytorch backend to run demonstrations."
msgstr ""

#: ../../source/contribs/development_wsl2.rst:8
msgid ""
"Step 1. Install WSL2, follow the official installation instruction: "
"https://docs.microsoft.com/en-us/windows/wsl/install"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:11
msgid ""
"Step 2. Install CUDA for GPU support, if you want to used GPU "
"accelerator. The official CUDA installation for WSL2: "
"https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch02-getting-"
"started"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:15
msgid "Step 3. Follow the Linux Installation Instructions to finish installing."
msgstr ""

#: ../../source/contribs/development_wsl2.rst:18
msgid "**System Support Summary**"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:21
msgid "Backend"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:22
msgid "Numpy"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:23
msgid "TensorFlow"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:24
msgid "JAX"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:25
msgid "Pytorch"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:26
msgid "Suggested Package Version"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:27
msgid ">= 1.20.0"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:28
msgid ">= 2.7.0"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:29
msgid ">= 0.3.0"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:30
msgid ">= 1.12"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:31
msgid "OS Support without GPU Accelerator"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:32
#: ../../source/contribs/development_wsl2.rst:33
#: ../../source/contribs/development_wsl2.rst:34
#: ../../source/contribs/development_wsl2.rst:35
msgid "Windows/MacOS/Linux"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:36
msgid "OS Support with GPU Accelerator"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:37
msgid "No Support for GPU"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:38
msgid ""
"Windows(WSL2, docker)/`MacOS <https://developer.apple.com/metal"
"/tensorflow-plugin>`_/Linux"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:39
msgid "Windows(WSL2, docker)/MacOS/Linux"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:40
msgid "Windows(WSL2, docker)/MacOS(torch>=1.12)/Linux"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:41
msgid "Platform with TPU Accelerator"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:42
msgid "No Support for TPU"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:43
msgid ""
"`GCP - Tensorflow with TPU <https://cloud.google.com/tpu/docs/run-"
"calculation-tensorflow>`_"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:44
msgid ""
"`GCP - JAX with TPU <https://cloud.google.com/tpu/docs/run-calculation-"
"jax>`_"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:45
msgid ""
"`GCP - Pytorch with TPU <https://cloud.google.com/tpu/docs/run-"
"calculation-pytorch>`_"
msgstr ""

#: ../../source/contribs/development_wsl2.rst:47
msgid "Tips: Currently, we don't suggest you to use TPU accelerator."
msgstr ""

#~ msgid ""
#~ "Or Download from "
#~ "[Apple](https://developer.apple.com/download/more/)  Command "
#~ "Line Tools installation image then "
#~ "install if internet connection is weak."
#~ msgstr ""

#~ msgid "There are four backends to choose from, Tensorflow, Jax, Torch."
#~ msgstr ""

#~ msgid ""
#~ "FYI:  Error can occur when machine "
#~ "learning training or gpu related code"
#~ " is involved."
#~ msgstr ""

#~ msgid "Testing Platform"
#~ msgstr ""

#~ msgid "Install Tensorflow (Recommended Approach)"
#~ msgstr ""

#~ msgid ""
#~ "❗️ Tensorflow with MacOS optimization "
#~ "would not function correctly in version"
#~ " 2.11.0 and before. Do not use "
#~ "this version of tensorflow if you "
#~ "intented to train any machine learning"
#~ " model."
#~ msgstr ""

#~ msgid ""
#~ "FYI: Error can occur when machine "
#~ "learning training or gpu related code"
#~ " is involved."
#~ msgstr ""

#~ msgid ""
#~ "⚠️ Tensorflow without macos optimization "
#~ "does not support Metal API and "
#~ "utilizing GPU (both intel chips and "
#~ "M-series chips) until at least "
#~ "tensorflow 2.11. Tensorflow-macos would "
#~ "fail when running `tc.backend.to_dense()`"
#~ msgstr ""

#~ msgid "Testing Platform (Tested Feb 2023)"
#~ msgstr ""

#~ msgid ""
#~ "This page is deprecated. Please visit"
#~ " `the update tutorial <development_MacARM.html>`_"
#~ " for latest information."
#~ msgstr ""




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/contribution.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  TensorCircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-06-27 20:10+0800\n"
"PO-Revision-Date: 2022-04-12 09:18+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: zh\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/contribution.rst:2
msgid "Guide for Contributors"
msgstr "开发者指南"

#: ../../source/contribution.rst:4
#, fuzzy
msgid ""
"We welcome everyone’s contributions! The development of TensorCircuit is "
"open-sourced and centered on `GitHub <https://github.com/tencent-quantum-"
"lab/tensorcircuit>`_."
msgstr ""
"我们欢迎每一位贡献者！TensorCircuit 的开发是开源的，并以 `GitHub <https://github.com/quclub"
"/tensorcircuit-dev>`_ 为中心。"

#: ../../source/contribution.rst:6
msgid "There are various ways to contribute:"
msgstr "参与贡献有很多方式："

#: ../../source/contribution.rst:8
msgid "Answering questions on the discussions page or issue page."
msgstr "回答讨论区或是 issue page 上的问题。"

#: ../../source/contribution.rst:10
msgid "Raising issues such as bug reports or feature requests on the issue page."
msgstr "在 issue page 报告漏洞或是提出新的功能需求。"

#: ../../source/contribution.rst:12
msgid "Improving the documentation (docstrings/tutorials) by pull requests."
msgstr "通过拉取请求改进文档（文档字符串/教程）。"

#: ../../source/contribution.rst:14
msgid "Contributing to the codebase by pull requests."
msgstr "通过拉取请求为代码库做出贡献。"

#: ../../source/contribution.rst:19
msgid "Pull Request Guidelines"
msgstr "拉取请求指南"

#: ../../source/contribution.rst:21
msgid ""
"We welcome pull requests from everyone. For large PRs involving feature "
"enhancement or API changes, we ask that you first open a GitHub issue to "
"discuss your proposal."
msgstr "我们欢迎大家提出拉取请求。对于涉及功能增强或 API 更改的大型拉取请求，我们需要您首先打开一个 GitHub issue 来讨论您的企划。"

#: ../../source/contribution.rst:23
msgid "The following git workflow is recommended for contribution by PR:"
msgstr "拉取请求推荐使用以下 git 工作流进行贡献： "

#: ../../source/contribution.rst:25
msgid ""
"Configure your git username and email so that they match your GitHub "
"account if you haven't."
msgstr "配置您的 git 用户名和电子邮件，以使它们与您的 GitHub 帐户匹配（如果没有）。"

#: ../../source/contribution.rst:32
msgid ""
"Fork the TensorCircuit repository by clicking the Fork button on GitHub. "
"This will create an independent version of the codebase in your own "
"GitHub account."
msgstr "通过点击 GitHub 上的 Fork 按钮来分叉 TensorCircuit 库。 这将在您自己的 GitHub 帐户中创建一个独立版本的代码库。"

#: ../../source/contribution.rst:34
msgid ""
"Clone your forked repository and set up an ``upstream`` reference to the "
"official TensorCircuit repository."
msgstr "克隆您的分叉存储库并设置对官方 TensorCircuit 存储库的 ``upstream`` 引用。"

#: ../../source/contribution.rst:42
msgid ""
"Configure the python environment locally for development. The following "
"commands are recommended:"
msgstr "在本地配置 python 环境以进行开发。推荐使用以下命令："

#: ../../source/contribution.rst:49
msgid "Extra packages may be required for specific development tasks."
msgstr "特定的开发任务可能需要额外的软件包。"

#: ../../source/contribution.rst:51
msgid ""
"Pip installing your fork from the source. This allows you to modify the "
"code locally and immediately test it out."
msgstr "从源代码 pip 安装你的 fork。这允许您在本地修改代码并立即对其进行测试。"

#: ../../source/contribution.rst:57
msgid ""
"Create a feature branch where you can make modifications and "
"developments. DON'T open PR from your master/main branch."
msgstr "创建一个功能分支，您可以在其中进行修改和开发、不要从你的 ``master/main`` 分支打开拉取请求。"

#: ../../source/contribution.rst:63
msgid ""
"Make sure your changes can pass all checks by running: "
"``./check_all.sh``. (See the :ref:`Checks` section below for details)"
msgstr "运行 ``./check_all.sh`` 以确保你的修改能够通过所有检查（参考 :ref:`Checks` 部分以获取更多细节）。"

#: ../../source/contribution.rst:65
msgid "Once you are satisfied with your changes, create a commit as follows:"
msgstr "一旦您对更改感到满意，请按如下方式创建提交："

#: ../../source/contribution.rst:72
msgid "You should sync your code with the official repo:"
msgstr "您应该将您的代码与官方库进行同步："

#: ../../source/contribution.rst:79
msgid ""
"Note that PRs typically comprise a single git commit, you should squash "
"all your commits in the feature branch. Using ``git rebase -i`` for "
"commits squash, see `instructions <https://www.internalpointers.com/post"
"/squash-commits-into-one-git>`_"
msgstr ""
"请注意，拉取请求通常包含一个 git 提交，您应该在功能分支中压缩所有提交。使用  ``git rebase -i`` 压缩修改，参考  `教程 "
"<https://www.internalpointers.com/post/squash-commits-into-one-git>`_。"

#: ../../source/contribution.rst:81
msgid ""
"Push your commit from your feature branch. This will create a remote "
"branch in your forked repository on GitHub, from which you will raise a "
"PR."
msgstr "从您的功能分支上传您的提交。这将在您在 GitHub 上的分叉存储库中创建一个远程分支，您将从该分支中提出拉取请求。"

#: ../../source/contribution.rst:87
msgid ""
"Create a PR from the official TensorCircuit repository and send it for "
"review. Some comments and remarks attached with the PR are recommended. "
"If the PR is not finally finished, please add [WIP] at the beginning of "
"the title of your PR."
msgstr ""
"从官方 TensorCircuit "
"存储库创建拉取请求并将其发送以供审核。建议在拉取请求中附上一些评论和备注。如果拉取请求没有最终完成，请在您的拉取请求的标题前加上 [WIP]。"

#: ../../source/contribution.rst:89
msgid ""
"The PR will be reviewed by the developers and may get approved or change "
"requested. In the latter case, you can further revise the PR according to"
" suggestions and feedback from the code reviewers."
msgstr "拉取请求将由开发人员审核，并可能获得批准或被要求完善。在后一种情况下，您可以根据代码审查者的建议和反馈进一步修改拉取请求。"

#: ../../source/contribution.rst:91
msgid ""
"The PR you opened can be automatically updated once you further push "
"commits to your forked repository. Please remember to ping the code "
"reviewers in the PR conversation soon."
msgstr "一旦您进一步将提交上传到您的分叉存储库，您打开的拉取请求就会自动更新。请记得尽快在拉取请求对话中提醒代码审阅者。"

#: ../../source/contribution.rst:93
msgid ""
"Please always include new docs and tests for your PR if possible and "
"record your changes on CHANGELOG."
msgstr "请尽可能始终为您的拉取请求包含新的文档和测试，并在 CHANGELOG 上记录您的更改。"

#: ../../source/contribution.rst:97
msgid "Checks"
msgstr "检查"

#: ../../source/contribution.rst:99
msgid ""
"The simplest way to ensure the codebase is ok with checks and tests is to"
" run one-in-all scripts ``./check_all.sh`` (you may need to ``chmod +x "
"check_all.sh`` to grant permissions on this file)."
msgstr ""
"确保代码库可以进行检查和测试的最简单方法是运行一体化脚本  ``./check_all.sh`` （你可能需要 ``chmod +x "
"check_all.sh`` 以在文件中获得批准）。"

#: ../../source/contribution.rst:101
msgid "The scripts include the following components:"
msgstr "此脚本包含下面的部分："

#: ../../source/contribution.rst:103
msgid "black"
msgstr "black 库"

#: ../../source/contribution.rst:105
msgid ""
"mypy: configure file is ``mypy.ini``, results strongly correlated with "
"the version of numpy, we fix ``numpy==1.21.5`` as mypy standard in CI."
msgstr ""
"mypy: 设置文件是 ``mypy.ini``, 结果与 numpy 的版本强烈相关，我们设置 ``numpy==1.21.5`` "
"作为持续集成中的 mypy 标准。"

#: ../../source/contribution.rst:107
msgid "pylint: configure file is ``.pylintrc``"
msgstr "pylint: 设置文件是 ``.pylintrc``。"

#: ../../source/contribution.rst:109
msgid "pytest: see :ref:`Pytest` sections for details."
msgstr "pytest: 阅读 :ref:`Pytest` 部分以获取更多细节。"

#: ../../source/contribution.rst:111
msgid "sphinx doc builds: see :ref:`Docs` section for details."
msgstr "sphinx 文档产生: 参考 :ref:`Docs` 部分以获取更多细节。"

#: ../../source/contribution.rst:113
msgid "Make sure the scripts check are successful by 💐."
msgstr "通过💐确保脚本检查成功。"

#: ../../source/contribution.rst:115
msgid ""
"Similar tests and checks are also available via GitHub action as CI "
"infrastructures."
msgstr "类似的测试和检查也可以通过 GitHub 持续集成基础设的施操作来实现。"

#: ../../source/contribution.rst:117
msgid ""
"Please also include corresponding changes for CHANGELOG.md and docs for "
"the PR."
msgstr "还请在 CHANGELOG.md 和拉取请求的文档中包括相应更改。"

#: ../../source/contribution.rst:121
msgid "Pytest"
msgstr "Pytest"

#: ../../source/contribution.rst:123
msgid ""
"For pytest, one can speed up the test by ``pip install pytest-xdist``, "
"and then run parallelly as ``pytest -v -n [number of processes]``. We "
"also have included some micro-benchmark tests, which work with ``pip "
"install pytest-benchmark``."
msgstr ""
"关于 pytest，你可以通过 ``pip install pytest-xdist``，然后并行运行 ``pytest -v -n "
"[number of processes]`` 来加快测试速度。我们还包含了一些与 ``pip install pytest-"
"benchmark`` 一起使用的微基准测试。"

#: ../../source/contribution.rst:126
msgid "**Fixtures:**"
msgstr "**Fixtures:**"

#: ../../source/contribution.rst:128
msgid ""
"There are some pytest fixtures defined in the conftest file, which are "
"for customization on backends and dtype in function level. ``highp`` is a"
" fixture for complex128 simulation. While ``npb``, ``tfb``, ``jaxb`` and "
"``torchb`` are fixtures for global numpy, tensorflow, jax and pytorch "
"backends, respectively. To test different backends in one function, we "
"need to use the parameterized fixture, which is enabled by ``pip install "
"pytest-lazy-fixture``. Namely, we have the following approach to test "
"different backends in one function."
msgstr ""
"我们在 conftest 文件中定义了一些 pytest fixture，用于后端自定义和函数级别的 dtype 。``highp`` 是 "
"complex128 模拟的 fixture 。 而 ``npb``、``tfb``、``jaxb`` 和 ``torchb`` 分别是全局 "
"numpy、tensorflow、jax 和 pytorch 后端的 fixtures 。 要在一个函数中测试不同的后端，我们需要使用参数化的 "
"fixtures，它由 ``pip install pytest-lazy-fixture`` "
"启动。也就是说，我们有以下方法在一个函数中测试不同的后端。"

#: ../../source/contribution.rst:143
msgid "Docs"
msgstr "文档"

#: ../../source/contribution.rst:145
msgid ""
"We use `sphinx <https://www.sphinx-doc.org/en/master/>`__ to manage the "
"documentation."
msgstr "我们使用 `sphinx <https://www.sphinx-doc.org/en/master/>`__ 来管理文档。"

#: ../../source/contribution.rst:147
msgid "The source files for docs are .rst file in docs/source."
msgstr "文档的源文件是在 ``docs/source`` 文件夹中的 .rst 文件。"

#: ../../source/contribution.rst:149
msgid ""
"For English docs, ``sphinx-build source build/html`` in docs dir is "
"enough. The html version of the docs are in docs/build/html."
msgstr ""
"对于英文文档, ``sphinx-build source build/html`` 在 docs dir 中就足够了。HTML 版本在储存在 "
"``docs/build/html`` 文件夹中。"

#: ../../source/contribution.rst:151
msgid "**Auto Generation of API Docs:**"
msgstr "**自动生成 API 文档：**"

#: ../../source/contribution.rst:153
msgid ""
"We utilize a python script to generate/refresh all API docs rst files "
"under /docs/source/api based on the codebase /tensorcircuit."
msgstr ""
"我们利用 python 脚本根据代码库 /tensorcircuit 生成/刷新 /docs/source/api 下的所有 API rst "
"文档文件。"

#: ../../source/contribution.rst:160
msgid "**i18n:**"
msgstr "**国际化**"

#: ../../source/contribution.rst:162
#, fuzzy
msgid ""
"For Chinese docs, we refer to the standard i18n workflow provided by "
"sphinx, see `here <https://www.sphinx-"
"doc.org/en/master/usage/advanced/intl.html>`__."
msgstr ""
"对于中文版文档, 我们参考 sphinx 提供的国际化标准工作流，请参考 `这里 <https://www.sphinx-"
"doc.org/en/master/usage/advanced/intl.html>`__ 。"

#: ../../source/contribution.rst:164
msgid "To update the po file from updated English rst files, using"
msgstr "为从更新的英文 rst 文件中更新 .po 文件，使用"

#: ../../source/contribution.rst:173
#, fuzzy
msgid ""
"Edit these .po files to add translations (`poedit "
"<https://poedit.net/>`__ recommended). These files are in "
"docs/source/locale/zh/LC_MESSAGES."
msgstr ""
"通过编辑这些 .po 文件来翻译文档，推荐使用 (`poedit <https://poedit.net/>`__ )。 这些文件在 "
"``docs/source/locale/cn/LC_MESSAGES`` 中。"

#: ../../source/contribution.rst:175
msgid ""
"To generate the Chinese version of the documentation: ``sphinx-build "
"source -D language=\"zh\" build/html_cn`` which is in the separate "
"directory ``.../build/html_cn/index.html``, whereas English version is in"
" the directory ``.../build/html/index.html``."
msgstr ""

#: ../../source/contribution.rst:179
msgid "Releases"
msgstr ""

#: ../../source/contribution.rst:181
msgid ""
"Firstly, ensure that the version numbers in __init__.py and CHANGELOG are"
" correctly updated."
msgstr ""

#: ../../source/contribution.rst:183
#, fuzzy
msgid "**GitHub Release**"
msgstr "**Fixtures:**"

#: ../../source/contribution.rst:191
msgid "And from GitHub page choose draft a release from tag."
msgstr ""

#: ../../source/contribution.rst:193
msgid "**PyPI Release**"
msgstr ""

#: ../../source/contribution.rst:202
msgid "**DockerHub Release**"
msgstr ""

#: ../../source/contribution.rst:204
msgid "Make sure the DockerHub account is logged in via ``docker login``."
msgstr ""

#: ../../source/contribution.rst:212
#, fuzzy
msgid "**Binder Release**"
msgstr "**Fixtures:**"

#: ../../source/contribution.rst:214
msgid ""
"One may need to update the tensorcirucit version for binder environment "
"by pushing new commit in refraction-ray/tc-env repo with new version "
"update in its ``requriements.txt``. See `mybind setup "
"<https://discourse.jupyter.org/t/tip-speed-up-binder-launches-by-pulling-"
"github-content-in-a-binder-link-with-nbgitpuller/922>`_ for speed up via "
"nbgitpuller."
msgstr ""

#~ msgid ""
#~ "To generate the Chinese version of "
#~ "the documentation: ``sphinx-build source "
#~ "-D language=\"zh\" -D master_doc=index_cn "
#~ "build/html_cn`` which is in the separate"
#~ " directory ``.../build/html_cn/index_cn.html``, whereas"
#~ " English version is in the directory"
#~ " ``.../build/html/index.html``."
#~ msgstr ""
#~ "生成简体中文文档： ``sphinx-build source -D "
#~ "language=\"cn\" build/html_cn`` 储存在单独的文件夹 "
#~ "``.../build/html_cn`` 中, 而英文版本则储存在 "
#~ "``.../build/html`` 文件夹中。"




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/faq.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-01-13 11:04+0800\n"
"PO-Revision-Date: 2022-05-11 17:52+0800\n"
"Last-Translator: Xinghan Yang <yang-xinghan@outlook.com>\n"
"Language: cn\n"
"Language-Team: cn <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/faq.rst:2
msgid "Frequently Asked Questions"
msgstr "常见问题"

#: ../../source/faq.rst:5
msgid "How can I run TensorCircuit on GPU?"
msgstr "如何在 GPU 上运行 TensorCircuit"

#: ../../source/faq.rst:7
msgid ""
"This is done directly through the ML backend. GPU support is determined "
"by whether ML libraries are can run on GPU, we don't handle this within "
"tensorcircuit. It is the users' responsibility to configure a GPU-"
"compatible environment for these ML packages. Please refer to the "
"installation documentation for these ML packages and directly use the "
"official dockerfiles provided by TensorCircuit. With GPU compatible "
"environment, we can switch the use of GPU or CPU by a backend agnostic "
"environment variable ``CUDA_VISIBLE_DEVICES``."
msgstr ""
"这是直接通过 ML 后端完成的。GPU 支持是直接取决于 ML 库是否可以在 GPU 上运行，我们不在 tensorcircuit "
"中处理这个问题。为这些 ML 包配置与 GPU 兼容的环境是用户的责任。请参考这些 ML 包的安装文档，或直接使用 TensorCircuit "
"提供的官方 dockerfile。在 GPU 兼容的环境下，我们可以通过后端无关的环境变量 ``CUDA_VISIBLE_DEVICES`` "
"来切换使用 GPU 或 CPU。"

#: ../../source/faq.rst:13
msgid "When should I use GPU for the quantum simulation?"
msgstr ""

#: ../../source/faq.rst:15
msgid ""
"In general, for a circuit with qubit count larger than 16 or for circuit "
"simulation with large batch dimension more than 16, GPU simulation will "
"be faster than CPU simulation. That is to say, for very small circuits "
"and the very small batch dimensions of vectorization, GPU may show worse "
"performance than CPU. But one have to carry out detailed benchmarks on "
"the hardware choice, since the performance is determined by the hardware "
"and task details."
msgstr ""

#: ../../source/faq.rst:21
msgid "When should I jit the function?"
msgstr "什么时候该使用 jit？"

#: ../../source/faq.rst:23
msgid ""
"For a function with \"tensor in and tensor out\", wrapping it with jit "
"will greatly accelerate the evaluation. Since the first time of "
"evaluation takes longer time (staging time), jit is only good for "
"functions which have to be evaluated frequently."
msgstr ""

#: ../../source/faq.rst:28
msgid ""
"Be caution that jit can be easily misused if the users are not familiar "
"with jit mechanism, which may lead to:"
msgstr ""

#: ../../source/faq.rst:30
msgid "very slow performance due to recompiling/staging for each run,"
msgstr ""

#: ../../source/faq.rst:31
msgid "error when run function with jit,"
msgstr ""

#: ../../source/faq.rst:32
msgid "or wrong results without any warning."
msgstr ""

#: ../../source/faq.rst:34
msgid "The most possible reasons for each problem are:"
msgstr ""

#: ../../source/faq.rst:36
msgid "function input are not all in the tensor form,"
msgstr ""

#: ../../source/faq.rst:37
msgid ""
"the output shape of all ops in the function may require the knowledge of "
"the input value more than the input shape, or use mixed ops from numpy "
"and ML framework"
msgstr ""

#: ../../source/faq.rst:38
msgid ""
"subtle interplay between random number generation and jit (see "
":ref:`advance:Randoms, Jit, Backend Agnostic, and Their Interplay` for "
"the correct solution), respectively."
msgstr ""

#: ../../source/faq.rst:42
msgid "Which ML framework backend should I use?"
msgstr "我应该使用哪个 ML 框架后端？"

#: ../../source/faq.rst:44
msgid ""
"Since the Numpy backend has no support for AD, if you want to evaluate "
"the circuit gradient, you must set the backend as one of the ML "
"frameworks beyond Numpy."
msgstr "由于 Numpy 后端不支持自动微分，如果要评估电路梯度，必须将后端设置为 Numpy 之外的 ML 框架之一。"

#: ../../source/faq.rst:46
msgid ""
"Since PyTorch has very limited support for vectorization and jit while "
"our package strongly depends on these features, it is not recommended to "
"use. Though one can always wrap a quantum function on another backend "
"using a PyTorch interface, say "
":py:meth:`tensorcircuit.interfaces.torch_interface`."
msgstr ""
"由于 PyTorch 对矢量化和 jit 的支持非常有限，而我们的包强烈依赖于这些特性，因此不建议使用它。尽管总是可以使用 PyTorch "
"接口将量子函数包装在另一个后端，例如：py:meth:` 张量电路.interfaces.torch_interface`。"

#: ../../source/faq.rst:48
msgid ""
"In terms of the choice between TensorFlow and Jax backend, the better one"
" may depend on the use cases and one may want to benchmark both to pick "
"the better one. There is no one-for-all recommendation and this is why we"
" maintain the backend agnostic form of our software."
msgstr ""
"就 TensorFlow 和 Jax "
"后端之间的选择而言，后端的好坏可能取决于用例，并且需要对两者进行基准测试以可能选择更好的后端。没有一种万能的建议，这就是为什么我们维持我们的软件支持不同的后端的形式。"

#: ../../source/faq.rst:50
msgid "Some general rules of thumb:"
msgstr "一些一般的经验法则："

#: ../../source/faq.rst:52
msgid ""
"On both CPU and GPU, the running time of a jitted function is faster for "
"jax backend."
msgstr "在 CPU 和 GPU 上，对于 jax 后端来说，jitted 函数的运行时间更快。"

#: ../../source/faq.rst:54
msgid "But on GPU, jit staging time is usually much longer for jax backend."
msgstr "但在 GPU 上，jax 后端的 jit 时间通常要长得多。"

#: ../../source/faq.rst:56
msgid ""
"For hybrid machine learning tasks, TensorFlow has a better ML ecosystem "
"and reusable classical ML models."
msgstr "对于混合机器学习任务，TensorFlow 拥有更好的 ML 生态系统和可重用的经典 ML 模型。"

#: ../../source/faq.rst:58
msgid ""
"Jax has some built-in advanced features that are lacking in TensorFlow, "
"such as checkpoint in AD and pmap for distributed computing."
msgstr "Jax 具有一些 TensorFlow 所缺乏的内置高级功能，例如自动微分中的检查点和用于分布式计算的 pmap。"

#: ../../source/faq.rst:60
msgid ""
"Jax is much insensitive to dtype where type promotion is handled "
"automatically which means easier debugging."
msgstr "Jax 对自动处理类型提升的 dtype 非常不敏感，这意味着更容易调试。"

#: ../../source/faq.rst:62
msgid ""
"TensorFlow can cache the jitted function on the disk via SavedModel, "
"which further amortizes the staging time."
msgstr "TensorFlow 可以通过 SavedModel 将 jitted 函数缓存在磁盘上，从而进一步摊销编译时间。"

#: ../../source/faq.rst:66
msgid "What is the counterpart of ``QuantumLayer`` for PyTorch and Jax backend?"
msgstr "PyTorch 和 Jax 后端的 QuantumLayer 对应的是什么？"

#: ../../source/faq.rst:68
msgid ""
"Since PyTorch doesn't have mature vmap and jit support and Jax doesn't "
"have native classical ML layers, we highly recommend TensorFlow as the "
"backend for quantum-classical hybrid machine learning tasks, where "
"``QuantumLayer`` plays an important role. For PyTorch, we can in "
"principle wrap the corresponding quantum function into a PyTorch module, "
"we currently have the built-in support for this wrapper as "
"``tc.TorchLayer``. In terms of the Jax backend, we highly suggested "
"keeping the functional programming paradigm for such machine learning "
"tasks. Besides, it is worth noting that, jit and vmap are automatically "
"taken care of in ``QuantumLayer``."
msgstr ""

#: ../../source/faq.rst:74
msgid "When do I need to customize the contractor and how?"
msgstr "我什么时候需要定制 contractor 以及如何定制？"

#: ../../source/faq.rst:76
msgid ""
"As a rule of thumb, for the circuit with qubit counts larger than 16 and "
"circuit depth larger than 8, customized contraction may outperform the "
"default built-in greedy contraction strategy."
msgstr "根据经验，对于量子比特数大于 16 且电路深度大于 8 的电路，自定义收缩可能优于默认的内置贪婪收缩策略。"

#: ../../source/faq.rst:78
msgid ""
"To set up or not set up the customized contractor is about a trade-off "
"between the time on contraction pathfinding and the time on the real "
"contraction via matmul."
msgstr "设置或不设置定制 contractor 是在收缩寻路时间和通过 matmul 实际收缩时间之间进行权衡。"

#: ../../source/faq.rst:80
msgid ""
"The customized contractor costs much more time than the default "
"contractor in terms of contraction path searching, and via the path it "
"finds, the real contraction can take less time and space."
msgstr ""
"在收缩路径搜索方面，定制 contractor 比默认 contractor "
"花费的时间要多得多。并且通过它找到的路径比真正的收缩找到的路径花费更少的时间和空间。"

#: ../../source/faq.rst:82
msgid ""
"If the circuit simulation time is the bottleneck of the whole workflow, "
"one can always try customized contractors to see whether there is some "
"performance improvement."
msgstr "如果电路仿真时间是整个工作流程的瓶颈，总是可以尝试定制 contractor，看看是否有一些性能改进。"

#: ../../source/faq.rst:84
msgid ""
"We recommend to using `cotengra library "
"<https://cotengra.readthedocs.io/en/latest/index.html>`_ to set up the "
"contractor, since there are lots of interesting hyperparameters to tune, "
"we can achieve a better trade-off between the time on contraction path "
"search and the time on the real tensor network contraction."
msgstr ""
"我们建议使用 `cotengra library "
"<https://cotengra.readthedocs.io/en/latest/index.html>`_ 来设置 "
"contractor，因为有很多有趣的超参数需要调整，我们可以实现更好的收缩路径搜索时间和真实张量网络收缩时间之间的权衡。"

#: ../../source/faq.rst:86
msgid ""
"It is also worth noting that for jitted function which we usually use, "
"the contraction path search is only called at the first run of the "
"function, which further amortizes the time and favors the use of a highly"
" customized contractor."
msgstr ""
"\"“另外值得注意的是，对于我们通常使用的 jitted 函数，收缩路径搜索只在函数第一次运行时调用，这进一步摊销了时间，有利于使用高度定制的 "
"contractor。"

#: ../../source/faq.rst:88
msgid ""
"In terms of how-to on contractor setup, please refer to "
":ref:`quickstart:Setup the Contractor`."
msgstr "关于如何设置 contractor，请参阅 :ref:`quickstart:Setup the Contractor`。"

#: ../../source/faq.rst:91
msgid "Is there some API less cumbersome than ``expectation`` for Pauli string?"
msgstr "对于 Pauli 字符串，有没有比 ``expectation`` 更简单的 API？"

#: ../../source/faq.rst:93
msgid ""
"Say we want to measure something like :math:`\\langle X_0Z_1Y_2Z_4 "
"\\rangle` for a six-qubit system, the general ``expectation`` API may "
"seem to be cumbersome. So one can try one of the following options:"
msgstr ""
"假设我们要为六量子位系统测量  :math:`\\langle X_0Z_1Y_2Z_4 \\rangle` ，一般的 "
"``expectation`` API 可能看起来很麻烦。所以可以尝试以下选项之一 :"

#: ../../source/faq.rst:96
msgid "``c.expectation_ps(x=[0], y=[2], z=[1, 4])``"
msgstr "``c.expectation_ps(x=[0], y=[2], z=[1, 4])``"

#: ../../source/faq.rst:98
msgid ""
"``tc.templates.measurements.parameterized_measurements(c, np.array([1, 3,"
" 2, 0, 3, 0]), onehot=True)``"
msgstr ""
"``tc.templates.measurements.parameterized_measurements(c, np.array([1, 3,"
" 2, 0, 3, 0]), onehot=True)``"

#: ../../source/faq.rst:101
msgid ""
"Can I apply quantum operation based on previous classical measurement "
"results in TensorCircuit?"
msgstr "我可以根据线路中的经典测量结果来决定后续的量子门操作吗？"

#: ../../source/faq.rst:103
msgid "Try the following: (the pipeline is even fully jittable!)"
msgstr "尝试以下方法：（方法甚至完全可以即时编译！）"

#: ../../source/faq.rst:112
msgid ""
"``cond_measurement`` will return 0 or 1 based on the measurement result "
"on z-basis, and ``conditional_gate`` applies gate_list[r] on the circuit."
msgstr ""
"``cond_measurement`` 将根据 z 基础上的测量结果返回 0 或 1，并且 ``conditional_gate`` "
"在电路上应用 gate_list[r]。"

#: ../../source/faq.rst:115
msgid ""
"How to understand the difference between different measurement methods "
"for ``Circuit``?"
msgstr ""

#: ../../source/faq.rst:117
msgid ""
":py:meth:`tensorcircuit.circuit.Circuit.measure` : used at the end of the"
" circuit execution, return bitstring based on quantum amplitude "
"probability (can also with the probability), the circuit and the output "
"state are unaffected (no collapse). The jittable version is "
"``measure_jit``."
msgstr ""

#: ../../source/faq.rst:119
msgid ""
":py:meth:`tensorcircuit.circuit.Circuit.cond_measure`: also with alias "
"``cond_measurement``, usually used in the middle of the circuit "
"execution. Apply a POVM on z basis on the given qubit, the state is "
"collapsed and nomarlized based on the measurement projection. The method "
"returns an integer Tensor indicating the measurement result 0 or 1 based "
"on the quantum amplitude probability."
msgstr ""

#: ../../source/faq.rst:121
msgid ""
":py:meth:`tensorcircuit.circuit.Circuit.post_select`: also with alia "
"``mid_measurement``, usually used in the middle of the circuit execution."
" The measurement result is fixed as given from ``keep`` arg of this "
"method. The state is collapsed but unnormalized based on the given "
"measurement projection."
msgstr ""

#: ../../source/faq.rst:123
msgid "Please refer to the following demos:"
msgstr ""

#: ../../source/faq.rst:155
msgid ""
"How to understand difference between ``tc.array_to_tensor`` and "
"``tc.backend.convert_to_tensor``?"
msgstr ""

#: ../../source/faq.rst:157
msgid ""
"``tc.array_to_tensor`` convert array to tensor as well as automatically "
"cast the type to the default dtype of TensorCircuit, i.e. ``tc.dtypestr``"
" and it also support to specify dtype as ``tc.array_to_tensor( , "
"dtype=\"complex128\")``. Instead, ``tc.backend.convert_to_tensor`` keeps "
"the dtype of the input array, and to cast it as complex dtype, we have to"
" explicitly call ``tc.backend.cast`` after conversion. Besides, "
"``tc.array_to_tensor`` also accepts multiple inputs as ``a_tensor, "
"b_tensor = tc.array_to_tensor(a_array, b_array)``."
msgstr ""

#: ../../source/faq.rst:165
msgid ""
"How to arrange the circuit gate placement in the visualization from "
"``c.tex()``?"
msgstr ""

#: ../../source/faq.rst:167
msgid ""
"Try ``lcompress=True`` or ``rcompress=True`` option in "
":py:meth:`tensorcircuit.circuit.Circuit.tex` API to make the circuit "
"align from the left or from the right."
msgstr ""

#: ../../source/faq.rst:169
msgid ""
"Or try ``c.unitary(0, unitary=tc.backend.eye(2), name=\"invisible\")`` to"
" add placeholder on the circuit which is invisible for circuit "
"visualization."
msgstr ""

#: ../../source/faq.rst:172
msgid "How to get the entanglement entropy from the circuit output?"
msgstr ""

#: ../../source/faq.rst:174
msgid "Try the following:"
msgstr ""

#~ msgid "When should I use GPU for the quantum simulation?"
#~ msgstr "我什么时候应该使用 GPU 进行量子模拟？"

#~ msgid ""
#~ "In general, for circuit with qubit "
#~ "count larger than 16 or for "
#~ "circuit simulation with large batch "
#~ "dimension more than 16, GPU simulation"
#~ " will be faster than CPU simulation."
#~ " That is to say, for very small"
#~ " circuit and very small batch "
#~ "dimension of vectorization, GPU may show"
#~ " worse performance than CPU. But one"
#~ " have to carry out detailed "
#~ "benchmarks on the hardware choice, since"
#~ " the performance is determined by the"
#~ " hardware and task details."
#~ msgstr ""
#~ "一般来说，对于量子比特数大于 16 的电路，或者对于大批量维度大于 16 的电路仿真，GPU"
#~ " 仿真会比 CPU 仿真快。也就是说，对于非常小的电路和非常小的批量维度的向量化 "
#~ "，GPU可能表现出比CPU更差的性能。但是因为性能是由硬件和任务细节决定的，所有必须对硬件选择进行详细的基准测试。"

#~ msgid "When should I jit the function?"
#~ msgstr "我应该什么时候即时编译函数？"

#~ msgid ""
#~ "For function with \"tensor in and "
#~ "tensor out\", wrap it with jit "
#~ "will greatly accelerate the evaluation. "
#~ "Since the first time of evaluation "
#~ "takes longer time (staging time), jit"
#~ " is only good for functions which "
#~ "has to be evaluated frequently."
#~ msgstr ""
#~ "对于一个有 “tensor in and tensor out” "
#~ "的函数，用 jit 包裹起来会大大加快求值速度。由于第一次求值时间较长（staging "
#~ "time），jit只适用于需要频繁地求值的函数。"

#~ msgid ""
#~ "Be caution that jit can be easily"
#~ " misused if the users are not "
#~ "familiar with jit mechnism, which may"
#~ " lead to:"
#~ msgstr ""

#~ msgid "very slow performance due to recompiling/staging for each run,"
#~ msgstr "由于每次运行都重新编译/暂存，性能非常慢，"

#~ msgid "error when run function with jit,"
#~ msgstr "使用 jit 运行函数时出错"

#~ msgid "or wrong results without any warning."
#~ msgstr "或没有任何警告的错误结果。"

#~ msgid "The most possible reasons for each problem are:"
#~ msgstr "每个问题最可能的原因是："

#~ msgid "function input are not all in the tensor form,"
#~ msgstr "函数输入不都是张量形式，"

#~ msgid ""
#~ "the output shape of all ops in "
#~ "the function may require the knowledge"
#~ " of the input value more than "
#~ "the input shape, or use mixed ops"
#~ " from numpy and ML framework"
#~ msgstr "函数中所有操作的输出形状可能需要比输入形状更多的输入值知识，或者使用来自 numpy 和 ML 框架的混合操作"

#~ msgid ""
#~ "subtle interplay between random number "
#~ "generation and jit (see :ref:`advance:Randoms,"
#~ " Jit, Backend Agnostic, and Their "
#~ "Interplay` for the correct solution), "
#~ "respectively."
#~ msgstr ""
#~ "随机数生成和 jit 之间的微妙相互作用（分别参见 :ref:`advance:Randoms, "
#~ "Jit, Backend Agnostic, and their "
#~ "Interplay`  以获得正确的解决方案）。"

#~ msgid ""
#~ "How to understand the difference between"
#~ " different measurement methods for "
#~ "``Circuit``?"
#~ msgstr "如何理解 ``电路`` 不同测量方式的区别？"

#~ msgid ""
#~ ":py:meth:`tensorcircuit.circuit.Circuit.measure` : used"
#~ " at the end of the circuit "
#~ "execution, return bitstring based on "
#~ "quantum amplitude probability (can also "
#~ "with the probability), the circuit and"
#~ " the output state are unaffected (no"
#~ " collapse). The jittable version is "
#~ "``measure_jit``."
#~ msgstr ""
#~ ":py:meth:`tensorcircuit.circuit.Circuit.measure` : "
#~ "在电路执行结束时使用，根据量子幅度概率返回bitstring（也可以带概率），电路和输出状态不受影响（无崩溃）。 "
#~ "可即时编译的版本是 ``measure_jit``。"

#~ msgid ""
#~ ":py:meth:`tensorcircuit.circuit.Circuit.cond_measure`: also "
#~ "with alias ``cond_measurement``, usually used"
#~ " in the middle of the circuit "
#~ "execution. Apply a POVM on z basis"
#~ " on the given qubit, the state "
#~ "is collapsed and nomarlized based on "
#~ "the measurement projection. The method "
#~ "returns an integer Tensor indicating the"
#~ " measurement result 0 or 1 based "
#~ "on the quantum amplitude probability."
#~ msgstr ""
#~ ":py:meth:`tensorcircuit.circuit.Circuit.cond_measure`: 也有别名 "
#~ "`cond_measurement`，通常用在电路执行的中间。在给定的量子比特上基于 z 应用一个 "
#~ "POVM，状态被折叠并基于测量投影进行归一化。该方法返回一个整数张量，表示基于量子幅度概率的测量结果 0 或 "
#~ "1。"

#~ msgid ""
#~ ":py:meth:`tensorcircuit.circuit.Circuit.post_select`: also "
#~ "with alia ``mid_measurement``, usually used"
#~ " in the middle of the circuit "
#~ "execution. The measurement result is "
#~ "fixed as given from ``keep`` arg "
#~ "of this method. The state is "
#~ "collapsed but unnormalized based on the"
#~ " given measurement projection."
#~ msgstr ""
#~ ":py:meth:`tensorcircuit.circuit.Circuit.post_select`: 也有别名 "
#~ "``mid_measurement`，通常用于电路执行的中间。测量结果是固定的，由 ``keep`` arg "
#~ "给出 这种方法。根据给定的测量投影，状态已折叠但未归一化。"

#~ msgid "Please refer to the following demos:"
#~ msgstr "请参考以下演示："

#~ msgid ""
#~ "How to arange the circuit gate "
#~ "placement in visualization from ``c.tex()``?"
#~ msgstr "如何在 ``c.tex()`` 的可视化中安排电路门布局？"

#~ msgid ""
#~ "Try ``lcompress=True`` or ``rcompress=True`` "
#~ "option in :py:meth:`tensorcircuit.circuit.Circuit.tex` "
#~ "API to make the circuit align from"
#~ " left or from right."
#~ msgstr ""
#~ "尝试 ``lcompress=True`` 或 ``rcompress=True`` 选项在"
#~ " :py:meth:`tensorcircuit.circuit.Circuit.tex` API "
#~ "中使电路从左对齐或从右对齐。"

#~ msgid ""
#~ "Or try ``c.unitary(0, unitary=tc.backend.eye(2), "
#~ "name=\"invisible\")`` to add placeholder on"
#~ " the circuit which is invisible for"
#~ " circuit visualization."
#~ msgstr ""
#~ "或者尝试 ``c.unitary(0, unitary=tc.backend.eye(2), "
#~ "name=\"invisible\")`` 在电路上添加占位符，这对于电路可视化来说是不可见的。"

#~ msgid "How to get the entanglement entropy of from the circuit output?"
#~ msgstr "如何从电路输出中得到纠缠熵？"

#~ msgid "Try the following:"
#~ msgstr "尝试以下方法："

#~ msgid ""
#~ "Since PyTorch doesn't have mature vmap"
#~ " and jit support and Jax doesn't "
#~ "have native classical ML layers, we "
#~ "highly recommend TensorFlow as the "
#~ "backend for quantum-classical hybrid "
#~ "machine learning tasks, where ``QuantumLayer``"
#~ " plays an important role. For "
#~ "PyTorch, we can in principle wrap "
#~ "the corresponding quantum function into "
#~ "a PyTorch module, but we currently "
#~ "have no built-in support for this"
#~ " wrapper. In terms of the Jax "
#~ "backend, we highly suggested keeping the"
#~ " functional programming paradigm for such"
#~ " machine learning tasks. Besides, it "
#~ "is worth noting that, jit and vmap"
#~ " are automatically taken care of in"
#~ " ``QuantumLayer``."
#~ msgstr ""
#~ "由于 PyTorch 没有成熟的 vmap 和 jit 支持，而且"
#~ " Jax 没有原生的经典 ML 层，我们强烈推荐 TensorFlow "
#~ "作为量子经典混合机器学习任务的后端，其中 QuantumLayer 起着重要作用。 对于 "
#~ "PyTorch，我们原则上可以将相应的量子函数包装到 PyTorch 模块中，但我们目前没有内置支持这个包装器。在"
#~ " Jax 后端方面，我们强烈建议保留函数式编程范式来处理此类机器学习任务。此外，值得注意的是，jit 和"
#~ " vmap 在 QuantumLayer 中是自动处理的。"




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/index.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# Xinghan Yang <yang-xinghan@outlook.com>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-28 14:36+0800\n"
"PO-Revision-Date: 2023-05-28 14:39+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language-Team: \n"
"Language: cn\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Generator: Poedit 3.2.2\n"

#: ../../source/index.rst:2
msgid "TensorCircuit Documentation"
msgstr "参考文档"

#: ../../source/index.rst:8
msgid "**Welcome and congratulations! You have found TensorCircuit.** 👏"
msgstr "**祝贺你发现了 TensorCircuit！** 👏"

#: ../../source/index.rst:11
msgid "Introduction"
msgstr "介绍"

#: ../../source/index.rst:13
msgid ""
"TensorCircuit is an open-source high-performance quantum computing software "
"framework in Python."
msgstr "TensorCircuit 是基于 Python 的开源高性能量子计算软件框架。"

#: ../../source/index.rst:15
msgid "It is built for humans. 👽"
msgstr "适合人类。👽"

#: ../../source/index.rst:17
msgid "It is designed for speed, flexibility and elegance. 🚀"
msgstr "速度，灵活，优雅。🚀"

#: ../../source/index.rst:19
msgid "It is empowered by advanced tensor network simulator engine. 🔋"
msgstr "先进张量网络引擎赋能。🔋"

#: ../../source/index.rst:21
msgid ""
"It is ready for quantum hardware access with CPU/GPU/QPU (local/cloud) hybrid "
"solutions. 🖥"
msgstr "量子硬件支持，优雅 CPU/GPU/QPU 混合部署方案。 🖥"

#: ../../source/index.rst:23
msgid ""
"It is implemented with industry-standard machine learning frameworks: "
"TensorFlow, JAX, and PyTorch. 🤖"
msgstr "业界标准机器学习框架 TensorFlow，JAX，PyTorch 实现。🤖"

#: ../../source/index.rst:25
msgid ""
"It is compatible with machine learning engineering paradigms: automatic "
"differentiation, just-in-time compilation, vectorized parallelism and GPU "
"acceleration. 🛠"
msgstr "与机器学习工程实践兼容：自动微分，即时编译，向量并行化和 GPU 加速。🛠"

#: ../../source/index.rst:27
msgid ""
"With the help of TensorCircuit, now get ready to efficiently and elegantly "
"solve interesting and challenging quantum computing problems: from academic "
"research prototype to industry application deployment."
msgstr ""
"有了 TensorCircuit，你现在可以高效优雅地解决量子计算中的各种问题：从学术研究的原"
"型开发到工业应用的部署。"

#: ../../source/index.rst:33
msgid "Relevant Links"
msgstr "相关链接"

#: ../../source/index.rst:35
msgid ""
"TensorCircuit is created and maintained by `Shi-Xin Zhang <https://github.com/"
"refraction-ray>`_ and this version is released by `Tencent Quantum Lab <https://"
"quantum.tencent.com/>`_."
msgstr ""
"TensorCircuit 由 `Shi-Xin Zhang <https://github.com/refraction-ray>`_ 创建和维"
"护；此版本由 `腾讯量子实验室 <https://quantum.tencent.com/>`_ 发布。"

#: ../../source/index.rst:37
msgid ""
"The current core authors of TensorCircuit are `Shi-Xin Zhang <https://github."
"com/refraction-ray>`_ and `Yu-Qin Chen <https://github.com/yutuer21>`_. We also "
"thank `contributions <https://github.com/tencent-quantum-lab/tensorcircuit/"
"graphs/contributors>`_ from the lab and the open source community."
msgstr ""
"TensorCircuit 当前主要作者为 `Shi-Xin Zhang <https://github.com/refraction-"
"ray>`_，`Yu-Qin Chen <https://github.com/yutuer21>`_。同时感谢来自实验室和开源社"
"区的 `贡献 <https://github.com/tencent-quantum-lab/tensorcircuit/graphs/"
"contributors>`_。"

#: ../../source/index.rst:40
msgid ""
"If you have any further questions or collaboration ideas, please use the issue "
"tracker or forum below, or send email to shixinzhang#tencent.com."
msgstr ""
"如果关于 TensorCircuit 有任何问题咨询或合作意向，请在 issue 或 discussion 提问，"
"或发送邮件到 shixinzhang#tencent.com。"

#: ../../source/index.rst:45
msgid "Source code"
msgstr "源代码"

#: ../../source/index.rst:49
msgid "GitHub"
msgstr ""

#: ../../source/index.rst:52
msgid "Documentation"
msgstr "参考文档"

#: ../../source/index.rst:56
msgid "Readthedocs"
msgstr ""

#: ../../source/index.rst:59
msgid "Whitepaper"
msgstr "白皮书"

#: ../../source/index.rst:63
msgid "*Quantum* journal"
msgstr "Quantum 期刊"

#: ../../source/index.rst:66
msgid "Issue Tracker"
msgstr "问题跟踪"

#: ../../source/index.rst:70
msgid "GitHub Issues"
msgstr ""

#: ../../source/index.rst:73
msgid "Forum"
msgstr "论坛"

#: ../../source/index.rst:77
msgid "GitHub Discussions"
msgstr ""

#: ../../source/index.rst:80
msgid "PyPI"
msgstr ""

#: ../../source/index.rst:84
msgid "``pip install``"
msgstr ""

#: ../../source/index.rst:87
msgid "DockerHub"
msgstr ""

#: ../../source/index.rst:91
msgid "``docker pull``"
msgstr ""

#: ../../source/index.rst:94
msgid "Application"
msgstr "应用"

#: ../../source/index.rst:98
msgid "Research using TC"
msgstr "研究项目"

#: ../../source/index.rst:101
msgid "Cloud"
msgstr "量子云"

#: ../../source/index.rst:104
msgid "Tencent Quantum Cloud"
msgstr "腾讯量子云平台"

#: ../../source/index.rst:131
msgid "Unified Quantum Programming"
msgstr "统一量子编程"

#: ../../source/index.rst:133
msgid ""
"TensorCircuit is unifying infrastructures and interfaces for quantum computing."
msgstr "TensorCircuit 尝试统一量子计算的基础设施和编程界面。"

#: ../../source/index.rst:140
msgid "Unified Backends"
msgstr "统一后端"

#: ../../source/index.rst:144
msgid "Jax/TensorFlow/PyTorch/Numpy/Cupy"
msgstr ""

#: ../../source/index.rst:146
msgid "Unified Devices"
msgstr "统一设备"

#: ../../source/index.rst:150
msgid "CPU/GPU/TPU"
msgstr ""

#: ../../source/index.rst:152
msgid "Unified Providers"
msgstr "统一平台"

#: ../../source/index.rst:156
msgid "QPUs from different vendors"
msgstr "不同供应商的 QPU"

#: ../../source/index.rst:158
msgid "Unified Resources"
msgstr "统一资源"

#: ../../source/index.rst:162
msgid "local/cloud/HPC"
msgstr "本地/云/集群"

#: ../../source/index.rst:170
msgid "Unified Interfaces"
msgstr "统一接口"

#: ../../source/index.rst:174
msgid "numerical sim/hardware exp"
msgstr "数值模拟/硬件实验"

#: ../../source/index.rst:176
msgid "Unified Engines"
msgstr "统一引擎"

#: ../../source/index.rst:180
msgid "ideal/noisy/approximate simulation"
msgstr "理想/含噪/近似模拟"

#: ../../source/index.rst:182
msgid "Unified Representations"
msgstr "统一表示"

#: ../../source/index.rst:186
msgid "from/to_IR/qiskit/openqasm/json"
msgstr ""

#: ../../source/index.rst:188
msgid "Unified Pipelines"
msgstr "统一流程"

#: ../../source/index.rst:192
msgid "stateless functional programming/stateful ML models"
msgstr "函数式编程/面向对象模型"

#: ../../source/index.rst:198
msgid "Reference Documentation"
msgstr "参考文档"

#: ../../source/index.rst:200
msgid ""
"The following documentation sections briefly introduce TensorCircuit to the "
"users and developpers."
msgstr "以下文档向用户和开发者简要介绍了 TensorCircuit 软件。"

#: ../../source/index.rst:213
msgid "Tutorials"
msgstr "教程"

#: ../../source/index.rst:215
msgid ""
"The following documentation sections include integrated examples in the form of "
"Jupyter Notebook."
msgstr ""
"以下 Jupyter Notebook 格式的文档包括了一系列使用 TensorCircuit 的集成案例。"

#: ../../source/index.rst:229
msgid "API References"
msgstr "API 参考"

#: ../../source/index.rst:238
msgid "Indices and Tables"
msgstr "索引和表格"

#: ../../source/index.rst:240
msgid ":ref:`genindex`"
msgstr ":ref:`genindex`"

#: ../../source/index.rst:241
msgid ":ref:`modindex`"
msgstr ":ref:`modindex`"

#: ../../source/index.rst:242
msgid ":ref:`search`"
msgstr ":ref:`search`"

#~ msgid ""
#~ "Binder online: https://mybinder.org/v2/gh/refraction-ray/tc-env/master?"
#~ "urlpath=git-pull?repo=https://github.com/tencent-quantum-lab/"
#~ "tensorcircuit&urlpath=lab/tree/tensorcircuit/&branch=master"
#~ msgstr ""
#~ "在线 Binder Jupyter: https://mybinder.org/v2/gh/refraction-ray/tc-env/master?"
#~ "urlpath=git-pull?repo=https://github.com/tencent-quantum-lab/"
#~ "tensorcircuit&urlpath=lab/tree/tensorcircuit/&branch=master"

#~ msgid "Software Whitepaper: https://arxiv.org/abs/2205.10091"
#~ msgstr "白皮书文章: https://arxiv.org/abs/2205.10091"

#~ msgid "Guide to TensorCircuit"
#~ msgstr "TensorCircuit 指南"

#~ msgid "Links"
#~ msgstr "重要链接"

#~ msgid "Source code: https://github.com/tencent-quantum-lab/tensorcircuit"
#~ msgstr "源代码: https://github.com/tencent-quantum-lab/tensorcircuit"

#~ msgid "Documentation: https://tensorcircuit.readthedocs.io"
#~ msgstr "文档: https://tensorcircuit.readthedocs.io"

#~ msgid ""
#~ "Software Whitepaper (published in Quantum): https://quantum-journal.org/"
#~ "papers/q-2023-02-02-912/"
#~ msgstr ""
#~ "软件白皮书 (发表于 Quantum): https://quantum-journal.org/papers/"
#~ "q-2023-02-02-912/"

#~ msgid ""
#~ "Issue Tracker: https://github.com/tencent-quantum-lab/tensorcircuit/issues"
#~ msgstr "问题跟踪: https://github.com/tencent-quantum-lab/tensorcircuit/issues"

#~ msgid "Forum: https://github.com/tencent-quantum-lab/tensorcircuit/discussions"
#~ msgstr ""
#~ "论坛社区: https://github.com/tencent-quantum-lab/tensorcircuit/discussions"

#~ msgid "PyPI page: https://pypi.org/project/tensorcircuit"
#~ msgstr "PyPI 页面: https://pypi.org/project/tensorcircuit"

#~ msgid ""
#~ "DockerHub page: https://hub.docker.com/repository/docker/tensorcircuit/"
#~ "tensorcircuit"
#~ msgstr ""
#~ "DockerHub 页面: https://hub.docker.com/repository/docker/tensorcircuit/"
#~ "tensorcircuit"

#~ msgid ""
#~ "Research and projects based on TensorCircuit: https://github.com/tencent-"
#~ "quantum-lab/tensorcircuit#research-and-applications"
#~ msgstr ""
#~ "基于 TensorCircuit 的研究和项目: https://github.com/tencent-quantum-lab/"
#~ "tensorcircuit#research-and-applications"

#~ msgid "Tencent Quantum Cloud Service: https://quantum.tencent.com/cloud/"
#~ msgstr "腾讯量子云服务: https://quantum.tencent.com/cloud/"

#~ msgid "Research based on TC"
#~ msgstr "基于 TC 的研究项目"



================================================
FILE: docs/source/locale/zh/LC_MESSAGES/index_cn.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-08 21:05+0800\n"
"PO-Revision-Date: 2022-04-16 22:35+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Language: cn\n"
"X-Generator: Poedit 1.6.11\n"

#: ../../source/index_cn.rst:2
msgid "Guide to TensorCircuit"
msgstr "TensoCircuit 指南"

#: ../../source/index_cn.rst:16
msgid "API References"
msgstr "API 参考"

#: ../../source/index_cn.rst:25
msgid "Indices and Tables"
msgstr "索引和表格"

#: ../../source/index_cn.rst:27
msgid ":ref:`genindex`"
msgstr ":ref:`genindex`"

#: ../../source/index_cn.rst:28
msgid ":ref:`modindex`"
msgstr ":ref:`modindex`"

#: ../../source/index_cn.rst:29
msgid ":ref:`search`"
msgstr ":ref:`search`"



================================================
FILE: docs/source/locale/zh/LC_MESSAGES/infras.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-27 18:52+0800\n"
"PO-Revision-Date: 2022-04-18 20:44+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/infras.rst:3
msgid "TensorCircuit: What is inside?"
msgstr "TensorCircuit：里面有什么？"

#: ../../source/infras.rst:5
msgid ""
"This part of the documentation is mainly for advanced users and "
"developers who want to learn more about what happened behind the scene "
"and delve into the codebase."
msgstr "这部分文档主要面向希望了解更多后端原理与并深入研究代码库的高级用户和开发人员。"

#: ../../source/infras.rst:9
msgid "Overview of Modules"
msgstr "模块概述"

#: ../../source/infras.rst:11
msgid "**Core Modules:**"
msgstr "**核心模块：**"

#: ../../source/infras.rst:13
msgid ""
":py:mod:`tensorcircuit.abstractcircuit` and "
":py:mod:`tensorcircuit.basecircuit`: Hierarchical abstraction of circuit "
"class."
msgstr ""

#: ../../source/infras.rst:15
msgid ""
":py:mod:`tensorcircuit.circuit`: The core object "
":py:obj:`tensorcircuit.circuit.Circuit`. It supports circuit "
"construction, simulation, representation, and visualization without noise"
" or with noise using the Monte Carlo trajectory approach."
msgstr ""
":py:mod:`tensorcircuit.circuit`: 核心对象 "
":py:obj:`tensorcircuit.circuit.Circuit`.它支持使用蒙特卡洛轨迹方法的无噪声或有噪声的电路构建、仿真、表示和可视化。"

#: ../../source/infras.rst:17
msgid ""
":py:mod:`tensorcircuit.cons`: Runtime ML backend, dtype and contractor "
"setups. We provide three sets of set methods for global setup, function "
"level setup using function decorators, and context setup using ``with`` "
"context managers. We also include customized contractor infrastructures "
"in this module."
msgstr ""
":py:mod:`tensorcircuit.cons`: 运行时的机器学习后端、数据类型和 contractor 设置。 "
"我们为全局设置、使用函数装饰器的函数级别设置和使用 ``with`` 上下文管理器的上下文设置提供了三组设置方法.我们还在此模块中提供了定制的 "
"contractor 基础设施。"

#: ../../source/infras.rst:19
msgid ""
":py:mod:`tensorcircuit.gates`: Definition of quantum gates, either fixed "
"ones or parameterized ones, as well as "
":py:obj:`tensorcircuit.gates.GateF` class for gates."
msgstr ""
":py:mod:`tensorcircuit.gates`: 固定或参数化的量子门的定义，以及用于门的 "
":py:obj:`tensorcircuit.gates.GateF` 类。"

#: ../../source/infras.rst:21
msgid "**Backend Agnostic Abstraction:**"
msgstr "**后端无关抽象：**"

#: ../../source/infras.rst:23
msgid ""
":py:mod:`tensorcircuit.backends` provides a set of backend API and the "
"corresponding implementation on Numpy, Jax, TensorFlow, and PyTorch "
"backends. These backends are inherited from the TensorNetwork package and"
" are highly customized."
msgstr ""
":py:mod:`tensorcircuit.backends` 提供了一组后端 API 以及在 Numpy、Jax、TensorFlow 和 "
"PyTorch 后端上的对应实现。这些后端继承自 TensorNetwork 包并且是高度定制的。"

#: ../../source/infras.rst:25
msgid "**Noisy Simulation Related Modules:**"
msgstr "**噪声模拟相关模块：**"

#: ../../source/infras.rst:27
msgid ":py:mod:`tensorcircuit.channels`: Definition of quantum noise channels."
msgstr ":py:mod:`tensorcircuit.channels`: 量子噪声通道的定义。"

#: ../../source/infras.rst:29
#, fuzzy
msgid ""
":py:mod:`tensorcircuit.densitymatrix`: Referenced and highly efficient "
"implementation of ``tc.DMCircuit`` class, with similar set API of "
"``tc.Circuit`` while simulating the noise in the full form of the density"
" matrix."
msgstr ""
":py:mod:`tensorcircuit.densitymatrix`: Referenced implementation of "
"``tc.DMCircuit`` 类的引用实现，具有 ``tc.Circuit`` 的类似集合 API，同时以密度矩阵的完整形式模拟噪声。"

#: ../../source/infras.rst:31
#, fuzzy
msgid ""
":py:mod:`tensorcircuit.noisemodel`: The global noise configuration and "
"circuit noisy method APIs"
msgstr ":py:mod:`tensorcircuit.vis`: 用于电路可视化的代码"

#: ../../source/infras.rst:33
msgid "**ML Interfaces Related Modules:**"
msgstr "**机器学习接口相关模块：**"

#: ../../source/infras.rst:35
#, fuzzy
msgid ""
":py:mod:`tensorcircuit.interfaces`: Provide interfaces when quantum "
"simulation backend is different from neural libraries. Currently include "
"PyTorch, TensorFlow, NumPy and SciPy optimizer interfaces."
msgstr ""
":py:mod:`tensorcircuit.interfaces`: 当量子模拟后端与神经库不同时提供接口。 目前包括 PyTorch 和 "
"scipy 优化器接口。"

#: ../../source/infras.rst:37
msgid ""
":py:mod:`tensorcircuit.keras`: Provide TensorFlow Keras layers, as well "
"as wrappers of jitted function, save/load from tf side."
msgstr ""
":py:mod:`tensorcircuit.keras`: 提供 TensorFlow Keras 层，以及可及时编译函数的包装器，从 "
"TensorFlow 端保存/加载."

#: ../../source/infras.rst:39
msgid ":py:mod:`tensorcircuit.torchnn`: Provide PyTorch nn Modules."
msgstr ""

#: ../../source/infras.rst:41
msgid "**MPS and MPO Utiliy Modules:**"
msgstr "**MPS 和 MPO 实用模块：**"

#: ../../source/infras.rst:43
msgid ""
":py:mod:`tensorcircuit.quantum`: Provide definition and classes for "
"Matrix Product States as well as Matrix Product Operators, we also "
"include various quantum physics and quantum information quantities in "
"this module."
msgstr ""
":py:mod:`tensorcircuit.quantum`: "
"提供矩阵乘积状态以及矩阵乘积算子的定义和类，我们还在这个模块中包含了各种量子物理和量子信息量。"

#: ../../source/infras.rst:45
msgid "**MPS Based Simulator Modules:**"
msgstr "**基于 MPS 的模拟器模块：**"

#: ../../source/infras.rst:47
msgid ""
":py:mod:`tensorcircuit.mps_base`: Customized and jit/AD compatible MPS "
"class from TensorNetwork package."
msgstr ""
":py:mod:`tensorcircuit.mps_base`: 来自 TensorNetwork 包的自定义并且即时编译/自动微分兼容的 "
"MPS 类。"

#: ../../source/infras.rst:49
msgid ""
":py:mod:`tensorcircuit.mpscircuit`: "
":py:obj:`tensorcircuit.mpscircuit.MPSCircuit` class with similar (but "
"subtly different) APIs as ``tc.Circuit``, where the simulation engine is "
"based on MPS TEBD."
msgstr ""
":py:mod:`tensorcircuit.mpscircuit`: "
":py:obj:`tensorcircuit.mpscircuit.MPSCircuit` 类具有与 "
"``tc.Circuit``，类似（但略有不同）的 API，其中仿真引擎基于 MPS TEBD。"

#: ../../source/infras.rst:51
msgid "**Supplemental Modules:**"
msgstr "**支持模块：**"

#: ../../source/infras.rst:53
msgid ""
":py:mod:`tensorcircuit.simplify`: Provide tools and utility functions to "
"simplify the tensornetworks before the real contractions."
msgstr ":py:mod:`tensorcircuit.simplify`: 提供工具和实用函数以在真正收缩之前简化张量网络。"

#: ../../source/infras.rst:55
msgid ""
":py:mod:`tensorcircuit.experimental`: Experimental functions, long and "
"stable support is not guaranteed."
msgstr ":py:mod:`tensorcircuit.experimental`: 实验函数，不保证有持久且稳定的支持。"

#: ../../source/infras.rst:57
msgid ""
":py:mod:`tensorcircuit.utils`: Some general function tools that are not "
"quantum at all."
msgstr ":py:mod:`tensorcircuit.utils`: 一些与量子完全无关的通用工具函数。"

#: ../../source/infras.rst:59
msgid ":py:mod:`tensorcircuit.vis`: Visualization code for circuit drawing."
msgstr ":py:mod:`tensorcircuit.vis`: 用于电路可视化的代码"

#: ../../source/infras.rst:61
msgid ""
":py:mod:`tensorcircuit.translation`: Translate circuit object to circuit "
"object in other quantum packages."
msgstr ":py:mod:`tensorcircuit.translation`: 将电路对象转换为其他量子包中的电路对象。"

#: ../../source/infras.rst:63
msgid "**Processing and error mitigation on sample results:**"
msgstr ""

#: ../../source/infras.rst:65
msgid ""
":py:mod:`tensorcircuit.results`: Provide tools to process count dict and "
"to apply error mitigation."
msgstr ""

#: ../../source/infras.rst:67
msgid "**Cloud quantum hardware access module:**"
msgstr ""

#: ../../source/infras.rst:69
msgid ""
":py:mod:`tensorcircuit.cloud`: Provide quantum cloud SDK that can access "
"and program the real quantum hardware."
msgstr ""

#: ../../source/infras.rst:71
msgid ""
":py:mod:`tensorcircuit.compiler`: Provide compiler chains to compile and "
"transform quantum circuits."
msgstr ""

#: ../../source/infras.rst:73
msgid "**Shortcuts and Templates for Circuit Manipulation:**"
msgstr "**电路操作的快捷方式和模板：**"

#: ../../source/infras.rst:75
msgid ""
":py:mod:`tensorcircuit.templates`: provide handy shortcuts functions for "
"expectation or circuit building patterns."
msgstr ":py:mod:`tensorcircuit.templates`: 为期望或电路构建模式提供方便的快捷函数。"

#: ../../source/infras.rst:77
msgid "**Applications:**"
msgstr "**应用：**"

#: ../../source/infras.rst:79
msgid ""
":py:mod:`tensorcircuit.applications`: most code here is not maintained "
"and deprecated, use at your own risk."
msgstr ":py:mod:`tensorcircuit.applications`: 这里的大多数代码都没有维护并且被弃用了，使用风险自负。"

#: ../../source/infras.rst:83
msgid ""
"Recommend reading order -- only read the part of code you care about for "
"your purpose. If you want to get an overview of the codebase, please read"
" ``tc.circuit`` followed by ``tc.cons`` and ``tc.gates``."
msgstr ""
"推荐阅读顺序——只阅读你关心的部分代码。如果您想了解代码库的概述，之后可以阅读 ``tc.circuit`` 后面的 ``tc.cons`` 和 "
"``tc.gates``。"

#: ../../source/infras.rst:88
msgid "Relation between TensorCircuit and TensorNetwork"
msgstr "TensorCircuit 和 TensorNetwork 之间的关系"

#: ../../source/infras.rst:90
msgid ""
"TensorCircuit has a strong connection with the `TensorNetwork package "
"<https://github.com/google/TensorNetwork>`_ released by Google. Since the"
" TensorNetwork package has poor documentation and tutorials, most of the "
"time, we need to delve into the codebase of TensorNetwork to figure out "
"what happened. In other words, to read the TensorCircuit codebase, one "
"may have to frequently refer to the TensorNetwork codebase."
msgstr ""
"TensorCircuit 与谷歌发布的 `TensorNetwork "
"<https://github.com/google/TensorNetwork>`_ 有很强的联系。由于 TensorNetwork "
"包的文档和教程很差，大多数时候，我们需要深入研究 TensorNetwork 的代码库来弄清楚发生了什么。换句话说，要阅读 "
"TensorCircuit 代码库，可能需要经常参考 TensorNetwork 代码库。"

#: ../../source/infras.rst:92
msgid ""
"Inside TensorCircuit, we heavily utilize TensorNetwork-related APIs from "
"the TensorNetwork package and highly customized several modules from "
"TensorNetwork by inheritance and rewriting:"
msgstr ""
"在 TensorCircuit 内部，我们大量使用了 TensorNetwork 包中与 TensorNetwork 相关的 "
"API，并通过继承和重写从 TensorNetwork 中高度定制了几个模块："

#: ../../source/infras.rst:94
msgid ""
"We implement our own /backends from TensorNetwork's /backends by adding "
"much more APIs and fixing lots of bugs in TensorNetwork's implementations"
" on certain backends via monkey patching. (The upstream is inactive and "
"not that responsive anyhow.)"
msgstr ""
"我们从 TensorNetwork 的后端实现我们自己的后端，方法是添加更多 API，并通过猴子补丁修复 TensorNetwork "
"在某些后端的实现中的许多错误。（上游是不活跃的，反馈不够灵敏）"

#: ../../source/infras.rst:96
msgid ""
"We borrow TensorNetwork's code in /quantum to our ``tc.quantum`` module, "
"since TensorNetwork has no ``__init__.py`` file to export these MPO and "
"MPS related objects. Of course, we have made substantial improvements "
"since then."
msgstr ""
"我们将 /quantum 中的 TensorNetwork 代码借用到我们的 ``tc.quantum`` 模块中，因为 "
"TensorNetwork 没有 ``__init__.py`` 文件来导出这些 MPO 和 MPS "
"相关对象。当然，从那时起，我们已经取得了实质性的代码改进。"

#: ../../source/infras.rst:98
msgid ""
"We borrow the TensorNetwork's code in /matrixproductstates as "
"``tc.mps_base`` for bug fixing and jit/AD compatibility, so that we have "
"better support for our MPS based quantum circuit simulator."
msgstr ""
"我们借用 /matrixproductstates 中 TensorNetwork 的代码作为 ``tc.mps_base`` "
"用于错误修复和即时编译/自动微分兼容性，以便我们更好地支持基于 MPS 的量子电路模拟器。"

#: ../../source/infras.rst:102
msgid "Relations of Circuit-like classes"
msgstr ""

#: ../../source/infras.rst:114
msgid "QuOperator/QuVector and MPO/MPS"
msgstr "QuOperator/QuVector 和 MPO/MPS"

#: ../../source/infras.rst:116
msgid ""
":py:class:`tensorcircuit.quantum.QuOperator`, "
":py:class:`tensorcircuit.quantum.QuVector` and "
":py:class:`tensorcircuit.quantum.QuAdjointVector` are classes adopted "
"from TensorNetwork package. They behave like a matrix/vector (column or "
"row) when interacting with other ingredients while the inner structure is"
" maintained by the tensornetwork for efficiency and compactness."
msgstr ""
":py:class:`tensorcircuit.quantum.QuOperator`, "
":py:class:`tensorcircuit.quantum.QuVector` 和 "
":py:class:`tensorcircuit.quantum.QuAdjointVector` 是从 TensorNetwork "
"包中采用的类。它们的行为类似于与其他成分交互时的矩阵/向量（列或行），而内部结构由张量网络维护以提高效率和紧凑性。"

#: ../../source/infras.rst:119
msgid ""
"We use code examples and associated tensor diagrams to illustrate these "
"object abstractions."
msgstr "我们使用代码示例和相关的张量图来说明这些对象抽象。"

#: ../../source/infras.rst:123
msgid ""
"``QuOperator`` can express MPOs and ``QuVector`` can express MPSs, but "
"they can express more than these fixed structured tensor networks."
msgstr "``QuOperator`` 可以表达 MPO，``QuVector`` 可以表达 MPS，但它们可以表达的不仅仅是这些固定的结构化张量网络。"

#: ../../source/infras.rst:151
msgid ""
"Note how in this example, ``matrix`` is not a typical MPO but still can "
"be expressed as ``QuOperator``. Indeed, any tensor network with two sets "
"of dangling edges of the same dimension can be treated as ``QuOperator``."
" ``QuVector`` is even more flexible since we can treat all dangling edges"
" as the vector dimension."
msgstr ""
"请注意，在这个例子中，``matrix`` 不是一个典型的 MPO，但仍然可以表示为 "
"``QuOperator``。事实上，任何具有两组相同维度的悬边的张量网络都可以被视为 `` QuOperator``。``QuVector`` "
"更加灵活，因为我们可以将所有悬空边视为向量维度。"

#: ../../source/infras.rst:153
msgid ""
"Also, note how ``^`` is overloaded as ``tn.connect`` to connect edges "
"between different nodes in TensorNetwork. And indexing the node gives the"
" edges of the node, eg. ``n1[0]`` means the first edge of node ``n1``."
msgstr ""
"还要注意 ``^`` 是如何被重载为 ``tn.connect`` 以连接 TensorNetwork "
"中不同节点之间的边。索引节点给出了节点的边，例如 ``n1[0]`` 意味着 ``节点 n1``  的第一条边。"

#: ../../source/infras.rst:155
msgid ""
"The convention to define the ``QuOperator`` is firstly giving "
"``out_edges`` (left index or row index of the matrix) and then giving "
"``in_edges`` (right index or column index of the matrix). The edges list "
"contains edge objects from the TensorNetwork library."
msgstr ""
"定义 ``QuOperator`` 的惯例是首先给出 ``out_edges``（矩阵的左索引或行索引），然后给出 "
"``in_edges``（矩阵的右索引或列索引)。边列表包含来自 TensorNetwork 库的边对象。"

#: ../../source/infras.rst:157
msgid ""
"Such QuOperator/QuVector abstraction support various calculations only "
"possible on matrix/vectors, such as matmul (``@``), adjoint "
"(``.adjoint()``), scalar multiplication (``*``), tensor product (``|``), "
"and partial trace (``.partial_trace(subsystems_to_trace_out)``). To "
"extract the matrix information of these objects, we can use ``.eval()`` "
"or ``.eval_matrix()``, the former keeps the shape information of the "
"tensor network while the latter gives the matrix representation with "
"shape rank 2."
msgstr ""
"这样的 QuOperator/QuVector 抽象支持只能在矩阵/向量上进行的各种计算，例如 matmul (``@``)、伴随 "
"(``.adjoint()``)、标量乘法 "
"(``*``)、张量乘积（``|``）和偏迹（``.partial_trace(subsystems_to_trace_out)``）。要提取这些对象的矩阵信息，我们可以使用"
" ``.eval()`` 或 ``.eval_matrix() ``，前者保留了张量网络的形状信息，而后者给出了形状秩为2的矩阵表示。"

#: ../../source/infras.rst:162
msgid "Quantum Cloud SDK: Layerwise API design"
msgstr ""

#: ../../source/infras.rst:164
msgid "From lower level to higher level, a view of API layers invoking QPU calls"
msgstr ""

#: ../../source/infras.rst:166
msgid ""
"Vendor specific implementation of functional API in, e.g., "
":py:mod:`tensorcircuit.cloud.tencent`"
msgstr ""

#: ../../source/infras.rst:168
msgid ""
"Provider agnostic functional lower level API for task/device management "
"in :py:mod:`tensorcircuit.cloud.apis`"
msgstr ""

#: ../../source/infras.rst:170
msgid ""
"Object oriented abstraction for Provider/Device/Task in "
":py:mod:`tensorcircuit.cloud.abstraction`"
msgstr ""

#: ../../source/infras.rst:172
msgid ""
"Unified batch submission interface as standarized in "
":py:meth:`tensorcircuit.cloud.wrapper.batch_submit_template`"
msgstr ""

#: ../../source/infras.rst:174
msgid ""
"Numerical and experimental unified all-in-one interface as "
":py:meth:`tensorcircuit.cloud.wrapper.batch_expectation_ps`"
msgstr ""

#: ../../source/infras.rst:176
msgid ""
"Application level code with QPU calls built directly on "
"``batch_expectation_ps`` or more fancy algorithms can be built on "
"``batch_submit_func`` so that these algorithms can be reused as long as "
"one function ``batch_submit_func`` is defined for a given vendor (cheaper"
" than defining a new provider from lower level)."
msgstr ""

#: ../../source/infras.rst:181
msgid ""
"For compiler, error mitigation and results post-processing parts, they "
"can be carefully designed to decouple with the QPU calls, so they are "
"separately implemented in :py:mod:`tensorcircuit.compiler` and "
":py:mod:`tensorcircuit.results`, and they can be independently useful "
"even without tc's cloud access."
msgstr ""

#~ msgid ""
#~ ":py:mod:`tensorcircuit.densitymatrix2`: Highly efficient"
#~ " implementation of "
#~ ":py:obj:`tensorcircuit.densitymatrix2.DMCircuit2` class, "
#~ "always preferred than the referenced "
#~ "implementation."
#~ msgstr ""
#~ ":py:mod:`tensorcircuit.densitymatrix2`:  "
#~ ":py:obj:`tensorcircuit.densitymatrix2.DMCircuit2` "
#~ "类的高效实现，总是比参考的实现更适用。"

#~ msgid ""
#~ ":py:mod:`tensorcircuit.results`: Provide tools to"
#~ " process count dict and to apply "
#~ "error mitigation"
#~ msgstr ""




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/quickstart.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-07-14 15:43+0800\n"
"PO-Revision-Date: 2023-05-07 11:01+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: Xinghan Yang\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/quickstart.rst:3
msgid "Quick Start"
msgstr "快速上手"

#: ../../source/quickstart.rst:6
msgid "Installation"
msgstr "安装"

#: ../../source/quickstart.rst:8
msgid "For x86 Linux,"
msgstr "x64 Linux"

#: ../../source/quickstart.rst:10
msgid "``pip install tensorcircuit``"
msgstr "``pip install tensorcircuit``"

#: ../../source/quickstart.rst:12
msgid ""
"is in general enough. Either pip from conda or other python env managers "
"is fine."
msgstr ""

#: ../../source/quickstart.rst:15
msgid ""
"Since there are many optional packages for various features, the users "
"may need to install more pip packages when required."
msgstr ""

#: ../../source/quickstart.rst:18
msgid "For Linux with Nvidia GPU,"
msgstr ""

#: ../../source/quickstart.rst:19
msgid ""
"please refer to the GPU aware installation guide of corresponding machine"
" learning frameworks: `TensorFlow "
"<https://www.tensorflow.org/install/gpu>`_, `Jax "
"<https://github.com/google/jax#pip-installation-gpu-cuda>`_, or `PyTorch "
"<https://pytorch.org/get-started/locally/>`_."
msgstr ""

#: ../../source/quickstart.rst:24
msgid "Docker is also recommended (especially Linux + Nvidia GPU setup):"
msgstr ""

#: ../../source/quickstart.rst:26
msgid ""
"``sudo docker run -it --network host --gpus all "
"tensorcircuit/tensorcircuit``."
msgstr ""

#: ../../source/quickstart.rst:28
msgid ""
"For more details on docker setup, please refer to `docker readme "
"<https://github.com/tencent-quantum-"
"lab/tensorcircuit/tree/master/docker>`_."
msgstr ""

#: ../../source/quickstart.rst:30
msgid ""
"For Windows, due to the lack of support for Jax, we recommend to use "
"docker or WSL, please refer to `TC via windows docker "
"<contribs/development_windows.html>`_ or `TC via WSL "
"<contribs/development_wsl2.html>`_."
msgstr ""

#: ../../source/quickstart.rst:32
msgid "For MacOS, please refer to `TC on Mac <contribs/development_Mac.html>`_."
msgstr "For MacOS, please refer to `在Mac上安装TC <contribs/development_Mac_cn.html>`_."

#: ../../source/quickstart.rst:34
msgid ""
"Overall, the installation of TensorCircuit is simple, since it is purely "
"in Python and hence very portable. As long as the users can take care of "
"the installation of ML frameworks on the corresponding system, "
"TensorCircuit will work as expected."
msgstr ""

#: ../../source/quickstart.rst:37
msgid ""
"To debug the installation issue or report bugs, please check the "
"environment information by ``tc.about()``."
msgstr ""

#: ../../source/quickstart.rst:40
msgid ""
"We also provide a nightly build of tensorcircuit via PyPI which can be "
"accessed by ``pip uninstall tensorcircuit``, then ``pip install "
"tensorcircuit-nightly``"
msgstr ""

#: ../../source/quickstart.rst:46
msgid "Circuit Object"
msgstr "电路对象"

#: ../../source/quickstart.rst:48
msgid "The basic object for TensorCircuit is ``tc.Circuit``."
msgstr "TensorCircuit的基本对象是 ``tc.Circuit``。"

#: ../../source/quickstart.rst:50
msgid "Initialize the circuit with the number of qubits ``c=tc.Circuit(n)``."
msgstr "用量子比特数(n) ``c=tc.Circuit(n)`` 来初始化电路。"

#: ../../source/quickstart.rst:52
msgid "**Input States:**"
msgstr "**输入状态:**"

#: ../../source/quickstart.rst:54
msgid ""
"The default input function for the circuit is :math:`\\vert 0^n "
"\\rangle`. One can change this to other wavefunctions by directly feeding"
" the inputs state vectors w: ``c=tc.Circuit(n, inputs=w)``."
msgstr ""
"电路的默认输入函数是 :math:`\\vert 0^n \\rangle` 。可以通过直接输入输入状态向量 w 将其更改为其他波函数: "
"``c=tc.Circuit(n, inputs=w)``。"

#: ../../source/quickstart.rst:56
msgid ""
"One can also feed matrix product states as input states for the circuit, "
"but we leave MPS/MPO usage for future sections."
msgstr "也可以将矩阵乘积状态作为电路的输入状态，但我们将矩阵乘积状态/矩阵乘积算子的使用留待后续讲解。"

#: ../../source/quickstart.rst:58
msgid "**Quantum Gates:**"
msgstr "**量子门:**"

#: ../../source/quickstart.rst:60
msgid ""
"We can apply gates on circuit objects. For example, using ``c.H(1)`` or "
"``c.rx(2, theta=0.2)``, we can apply Hadamard gate on qubit 1 (0-based) "
"or apply Rx gate on qubit 2 as :math:`e^{-i\\theta/2 X}`."
msgstr ""
"我们可以将门应用于电路对象。 例如，使用 ``c.H(1)`` 或 ``c.rx(2, theta=0.2)``，我们可以将 Hadamard "
"门应用于量子比特1 （基于0）或将 Rx 门应用于量子比特2 :math:`e^{-i\\theta/2 X}`。"

#: ../../source/quickstart.rst:62
msgid "The same rule also applies to multi-qubit gates, such as ``c.cnot(0, 1)``."
msgstr "同样的规则亦适用于多量子比特门，例如  ``c.cnot(0, 1)`` 。"

#: ../../source/quickstart.rst:64
msgid "There are also highly customizable gates, two instances are:"
msgstr "这些量子门也是高度可定制的，下面是两个例子"

#: ../../source/quickstart.rst:66
msgid ""
"``c.exp1(0, 1, unitary=m, theta=0.2)`` which is for the exponential gate "
":math:`e^{i\\theta m}` of any matrix m as long as :math:`m^2=1`."
msgstr ""
"``c.exp1(0, 1, unitary=m, theta=0.2)`` 用于任何矩阵 m 的指数门 :math:`e^{i\\theta "
"m}`，只要 m 满足 :math:`m^2=1`。"

#: ../../source/quickstart.rst:68
msgid ""
"``c.any(0, 1, unitary=m)`` which is for applying the unitary gate m on "
"the circuit."
msgstr "``c.any(0, 1, unitary=m)`` 在电路上作用任意的幺正量子门。"

#: ../../source/quickstart.rst:70
msgid "These two examples are flexible and support gates on any number of qubits."
msgstr "这两个例子很灵活，支持任意数量的量子比特上的门。"

#: ../../source/quickstart.rst:72
msgid "**Measurements and Expectations:**"
msgstr "**测量与期望**"

#: ../../source/quickstart.rst:74
msgid ""
"The most straightforward way to get the output from the circuit object is"
" by getting the output wavefunction in vector form as ``c.state()``."
msgstr "从电路对象中获取输出的最直接的方法是通过 ``c.state()`` 以向量形式获取输出波函数。"

#: ../../source/quickstart.rst:76
msgid ""
"For bitstring sampling, we have ``c.perfect_sampling()`` which returns "
"the bitstring and the corresponding probability amplitude."
msgstr "对于位串采样，我们有 ``c.perfect_sampling()``，它返回位串和相应的概率幅度。"

#: ../../source/quickstart.rst:78
msgid ""
"To measure part of the qubits, we can use ``c.measure(0, 1)``, if we want"
" to know the corresponding probability of the measurement output, try "
"``c.measure(0, 1, with_prob=True)``. The measure API is by default non-"
"jittable, but we also have a jittable version as ``c.measure_jit(0, 1)``."
msgstr ""
"要测量部分量子比特，我们可以使用 ``c.measure(0, 1)``，如果我们想知道测量的结果的对应概率，可以尝试 "
"``c.measure(0, 1, with_prob=True)``。 测量 API 在默认情况下是不可即时编译的 "
"，但我们也有一个可即时编译的版本，如 ``c.measure_jit(0, 1)``。"

#: ../../source/quickstart.rst:80
msgid ""
"The measurement and sampling utilize advanced algorithms based on "
"tensornetwork and thus require no knowledge or space for the full "
"wavefunction."
msgstr "测量和采样使用了基于张量网络的高级算法，因此不需要任何相关知识或者空间来获取全波函数。"

#: ../../source/quickstart.rst:82
msgid "See the example below:"
msgstr "请看下面的例子："

#: ../../source/quickstart.rst:100
msgid ""
"To compute expectation values for local observables, we have "
"``c.expectation([tc.gates.z(), [0]], [tc.gates.z(), [1]])`` for "
":math:`\\langle Z_0Z_1 \\rangle` or ``c.expectation([tc.gates.x(), "
"[0]])`` for :math:`\\langle X_0 \\rangle`."
msgstr ""
"为了计算局部可观察量的期望值，我们有 ``c.expectation([tc.gates.z(), [0]], [tc.gates.z(), "
"[1]])`` 对应的期望为 :math:`\\langle Z_0Z_1 \\rangle` 时，或 "
"``c.expectation([tc.gates.x(), [0]])`` 对应的期望为 :math:`\\langle X_0 "
"\\rangle`时."

#: ../../source/quickstart.rst:102
msgid ""
"This expectation API is rather flexible, as one can measure an m on "
"several qubits as ``c.expectation([m, [0, 1, 2]])``."
msgstr "因为可以在几个量子比特上测量一个 m，这种计算期望值的 API 相当灵活：``c.expectation([m, [0, 1, 2]])``。"

#: ../../source/quickstart.rst:104
msgid ""
"We can also extract the unitary matrix underlying the whole circuit as "
"follows:"
msgstr "我们还可以提取整个电路下面的幺正矩阵，如下所示："

#: ../../source/quickstart.rst:117
msgid "**Circuit Transformations:**"
msgstr "**电路可视化**"

#: ../../source/quickstart.rst:119
msgid ""
"We currently support transform ``tc.Circuit`` from and to Qiskit "
"``QuantumCircuit`` object."
msgstr "我们目前支持 ``tc.Circuit`` 与 Qiskit ``QuantumCircuit`` 对象之间的互相转换。"

#: ../../source/quickstart.rst:121
msgid ""
"Export to Qiskit (possible for further hardware experiment, compiling, "
"and visualization): ``c.to_qiskit()``."
msgstr "导出到 Qiskit（可能用于进一步的硬件实验、编译和可视化）：``c.to_qiskit()``。"

#: ../../source/quickstart.rst:123
msgid ""
"Import from Qiskit: ``c = tc.Circuit.from_qiskit(QuantumCircuit, n)``. "
"Parameterized Qiskit circuit is supported by passing the parameters to "
"the ``binding_parameters`` argument of the ``from_qiskit`` function, "
"similar to the ``assign_parameters`` function in Qiskit."
msgstr ""

#: ../../source/quickstart.rst:127
msgid "**Circuit Visualization:**"
msgstr "**电路可视化**"

#: ../../source/quickstart.rst:129
msgid ""
"``c.vis_tex()`` can generate tex code for circuit visualization based on "
"LaTeX `quantikz <https://arxiv.org/abs/1809.03842>`__ package."
msgstr ""
"``c.vis_tex()`` 可以基于  `quantikz <https://arxiv.org/abs/1809.03842>`__ "
"package 生成用于电路可视化的 tex 代码。"

#: ../../source/quickstart.rst:131
msgid ""
"There are also some automatic pipeline helper functions to directly "
"generate figures from tex code, but they require extra installations in "
"the environment."
msgstr "还有一些自动辅助函数可以直接从 tex 代码生成图形，但它们需要在环境中进行额外安装。"

#: ../../source/quickstart.rst:133
msgid ""
"``render_pdf(tex)`` function requires full installation of LaTeX locally."
" And in the Jupyter environment, we may prefer ``render_pdf(tex, "
"notebook=True)`` to return jpg figures, which further require wand "
"magicwand library installed, see `here <https://docs.wand-"
"py.org/en/latest/>`__."
msgstr ""
"``render_pdf(tex)`` 函数需要在本地完全安装 LaTeX。 在 Jupyter 环境中，我们可能会偏好 "
"``render_pdf(tex, notebook=True)`` 来返回 jpg 图形，这需要安装 wand magicwand 库，请参阅 "
"`这里 <https://docs.wand-py.org/en/latest/>`__ 。"

#: ../../source/quickstart.rst:135
msgid ""
"Or since we can transform ``tc.Circuit`` into QuantumCircuit easily, we "
"have a simple pipeline to first transform ``tc.Circuit`` into Qiskit and "
"then call the visualization built in Qiskit. Namely, we have ``c.draw()``"
" API."
msgstr ""
"从 Qiskit 导入：``c = tc.Circuit.from_qiskit(QuantumCircuit, n)`` "
"或者因为我们可以轻松地将 ``tc.Circuit`` 转换为 QuantumCircuit，我们有一个简单的管道来首先转换 "
"``tc.Circuit`` 为 Qiskit，然后调用 Qiskit 中内置的可视化。 也就是说，我们有 ``c.draw()`` API。"

#: ../../source/quickstart.rst:137
msgid "**Circuit Intermediate Representation:**"
msgstr "**电路中间表示:**"

#: ../../source/quickstart.rst:139
msgid ""
"TensorCircuit provides its own circuit IR as a python list of dicts. This"
" IR can be further utilized to run compiling, generate serialization "
"qasm, or render circuit figures."
msgstr "TensorCircuit 提供自己的中间表示是元素是字典的列表。此中间表示可进一步用于运行编译、生成序列化 qasm 或渲染电路图。"

#: ../../source/quickstart.rst:141
msgid ""
"The IR is given as a list, each element is a dict containing information "
"on one gate that is applied to the circuit. Note gate attr in the dict is"
" a python function that returns the gate's node."
msgstr ""
"中间表示以列表形式给出，每个元素都是一个字典，其中包含应用于电路的一个量子门的信息。 注意字典中的 gate atrr "
"实际上是一个返回此量子门的节点的 python 函数。"

#: ../../source/quickstart.rst:153
msgid "Programming Paradigm"
msgstr "编程范式"

#: ../../source/quickstart.rst:155
msgid ""
"The most common case and the most typical programming paradigm for "
"TensorCircuit are to evaluate the circuit output and the corresponding "
"quantum gradients, which is common in variational quantum algorithms."
msgstr "TensorCircuit 最常见的情况和最典型的编程范式是评估电路的输出以及相应的量子梯度，这在变分量子算法中很常见。"

#: ../../source/quickstart.rst:182
#, fuzzy
msgid ""
"Also for a non-quantum example (linear regression) demonstrating the "
"backend agnostic feature, variables with pytree support, AD/jit/vmap "
"usage, and variational optimization loops. Please refer to the example "
"script: `linear regression example <https://github.com/tencent-quantum-"
"lab/tensorcircuit/blob/master/examples/universal_lr.py>`_. This example "
"might be more friendly to the machine learning community since it is "
"purely classical while also showcasing the main features and paradigms of"
" tensorcircuit."
msgstr ""
"同样对于演示后端不可知特性的非量子示例（线性回归），pytree 支持变量、自动微分/即时编译/矢量并行化 用法和变分优化循环。请参考示例脚本: "
"`线性回归示例 <https://github.com/quclub/tensorcircuit-"
"dev/blob/master/examples/universal_lr.py>`_ 。 "
"这个例子可能对机器学习的用户更友好，因为它纯粹是经典的，同时也展示了 TensorCircuit 的主要特征和范式。"

#: ../../source/quickstart.rst:185
msgid ""
"If the user has no intention to maintain the application code in a "
"backend agnostic fashion, the API for ML frameworks can be more handily "
"used and interleaved with the TensorCircuit API."
msgstr ""
"如果用户无意以与后端无关的方式维护应用程序代码，则可以更方便地使用用于机器学习框架的 API 并将其与 TensorCircuit API "
"交替使用。"

#: ../../source/quickstart.rst:220
msgid "Automatic Differentiation, JIT, and Vectorized Parallelism"
msgstr "自动微分、即时编译和矢量化并行 "

#: ../../source/quickstart.rst:222
msgid ""
"For concepts of AD, JIT and VMAP, please refer to `Jax documentation "
"<https://jax.readthedocs.io/en/latest/jax-101/index.html>`__ ."
msgstr ""
"关于自动微分、即时编译和向量并行化，请参考 `Jax 文档 "
"<https://jax.readthedocs.io/en/latest/jax-101/index.html>`__ 。"

#: ../../source/quickstart.rst:224
msgid ""
"The related API design in TensorCircuit closely follows the functional "
"programming design pattern in Jax with some slight differences. So we "
"strongly recommend users learn some basics about Jax no matter which ML "
"backend they intend to use."
msgstr ""
"TensorCircuit 中的相关 API 设计与 Jax 中的函数式编程的设计模式密切相关，但是略有不同。因此，我们强烈建议用户学习一些有关 "
"Jax 的基础知识，无论他们打算使用哪种机器学习后端。"

#: ../../source/quickstart.rst:226
msgid "**AD Support:**"
msgstr "**自动微分支持**"

#: ../../source/quickstart.rst:228
msgid ""
"Gradients, vjps, jvps, natural gradients, Jacobians, and Hessians. AD is "
"the base for all modern machine learning libraries."
msgstr "梯度、矢量雅可比乘积、自然梯度、 Jacobian 矩阵和 Hessian 矩阵。自动微分是所有现代机器学习库的基础。"

#: ../../source/quickstart.rst:232
msgid "**JIT Support:**"
msgstr "**自动微分支持**"

#: ../../source/quickstart.rst:234
msgid ""
"Parameterized quantum circuits can run in a blink. Always use jit if the "
"circuit will get evaluations multiple times, it can greatly boost the "
"simulation with two or three order time reduction. But also be cautious, "
"users need to be familiar with jit, otherwise, the jitted function may "
"return unexpected results or recompile on every hit (wasting lots of "
"time). To learn more about the jit mechanism, one can refer to "
"documentation or blogs on ``tf.function`` or ``jax.jit``, though these "
"two still have subtle differences."
msgstr ""
"参数化的量子电路可以在瞬间完成运行。如果电路将得到多次运行，请始终使用即时编译，它可以大大提高仿真速度，减少两到三个数量级的运行时间。但也要小心，用户需要熟悉"
" 即时编译，否则，即时编译的函数可能会返回意外结果或每次在点击时都重新编译（浪费大量时间）。要了解更多关于即时编译机制的信息，可以参考关于 "
"``tf.function`` 或 ``jax.jit`` 的文档或博客，即使这两者仍然存在细微差别。"

#: ../../source/quickstart.rst:238
msgid "**VMAP Support:**"
msgstr "**自动微分支持**"

#: ../../source/quickstart.rst:240
msgid ""
"Inputs, parameters, measurements, circuit structures, and Monte Carlo "
"noise can all be evaluated in parallel. To learn more about vmap "
"mechanism, one can refer to documentation or blogs on "
"``tf.vectorized_map`` or ``jax.vmap``."
msgstr ""
"输入、参数、测量、电路结构、蒙特卡洛噪声都可以并行测算。 要了解有关矢量并行化机制的更多信息，可以参考 ``tf.vectorized_map``"
" 或 ``jax.vmap`` 上的文档或博客。"

#: ../../source/quickstart.rst:245
msgid "Backend Agnosticism"
msgstr "后端无关特性"

#: ../../source/quickstart.rst:247
msgid ""
"TensorCircuit supports TensorFlow, Jax, and PyTorch backends. We "
"recommend using TensorFlow or Jax backend since PyTorch lacks advanced "
"jit and vmap features."
msgstr ""
"TensorCircuit 支持 TensorFlow、Jax 和 PyTorch 后端。 我们建议使用 TensorFlow 或 Jax "
"后端，因为 PyTorch 缺乏高级 jit 和 vmap 功能。"

#: ../../source/quickstart.rst:249
msgid ""
"The backend can be set as ``K=tc.set_backend(\"jax\")`` and ``K`` is the "
"backend with a full set of APIs as a conventional ML framework, which can"
" also be accessed by ``tc.backend``."
msgstr ""
"后端可以设置为 ``K=tc.set_backend(\"jax\")`` ，``K``作为常规机器学习框架的全套API的后端，也可以通过``tc"
" .backend`` 被访问。"

#: ../../source/quickstart.rst:272
#, fuzzy
msgid ""
"The supported APIs in the backend come from two sources, one part is "
"implemented in `TensorNetwork package "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/abstract_backend.py>`__"
" and the other part is implemented in `TensorCircuit package "
"<modules.html#module-tensorcircuit.backends>`__. To see all the backend "
"agnostic APIs, try:"
msgstr ""
"在后端支持的 APIs 有两个来源 , 一个来自 `TensorNetwork "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/abstract_backend.py>`__"
" 另一个来自 `TensorCircuit package <modules.html#module-"
"tensorcircuit.backends>`__。"

#: ../../source/quickstart.rst:427
msgid "​"
msgstr ""

#: ../../source/quickstart.rst:430
msgid "Switch the Dtype"
msgstr "转换 dtype"

#: ../../source/quickstart.rst:432
msgid ""
"TensorCircuit supports simulation using 32/64 bit precession. The default"
" dtype is 32-bit as \"complex64\". Change this by "
"``tc.set_dtype(\"complex128\")``."
msgstr ""
"TensorCircuit 支持使用 32/64 bit 精确度的模拟。默认的 dtype 是 32-bit 的 "
"\"complex64\"。可以通过  ``tc.set_dtype(\"complex128\")`` 把 dtype 改为 \"complex"
" 128\" 。"

#: ../../source/quickstart.rst:435
msgid ""
"``tc.dtypestr`` always returns the current dtype string: either "
"\"complex64\" or \"complex128\"."
msgstr "``tc.dtypestr`` 总会返回当前的 dtype 字符串: 不是 \"complex64\" 就是 \"complex128\"."

#: ../../source/quickstart.rst:439
msgid "Setup the Contractor"
msgstr "设置 contractor"

#: ../../source/quickstart.rst:441
msgid ""
"TensorCircuit is a tensornetwork contraction-based quantum circuit "
"simulator. A contractor is for searching for the optimal contraction path"
" of the circuit tensornetwork."
msgstr "TensorCircuit 是一个基于张量网络收缩的量子电路模拟器。 contractor 用于搜索电路张量网络的最佳收缩路径。"

#: ../../source/quickstart.rst:443
msgid ""
"There are various advanced contractors provided by third-party packages, "
"such as `opt-einsum <https://github.com/dgasmith/opt_einsum>`__ and "
"`cotengra <https://github.com/jcmgray/cotengra>`__."
msgstr ""
"有各种第三方包提供的高级 contractor ，例如 `opt-einsum "
"<https://github.com/dgasmith/opt_einsum>`__ 和 `cotengra "
"<https://github.com/jcmgray/cotengra>`__ 。"

#: ../../source/quickstart.rst:445
msgid ""
"`opt-einsum` is shipped with TensorNetwork package. To use cotengra, one "
"needs to pip install it; kahypar is also recommended to install with "
"cotengra."
msgstr ""
"`opt-einsum` 随 TensorNetwork 软件包一起。如要使用 cotengra，则需要 pip 安装它； 还建议安装 "
"cotengra 随 kahypar 一起使用。"

#: ../../source/quickstart.rst:447
msgid "Some setup cases:"
msgstr "一些设置案例："

#: ../../source/quickstart.rst:473
#, fuzzy
msgid ""
"For advanced configurations on cotengra contractors, please refer to "
"cotengra `doc "
"<https://cotengra.readthedocs.io/en/latest/advanced.html>`__ and more "
"fancy examples can be found at `contractor tutorial <https://github.com"
"/tencent-quantum-lab/tensorcircuit-"
"tutorials/blob/master/tutorials/contractors.ipynb>`__."
msgstr ""
"有关 cotengra contractor 的高级配置，请参阅 cotengra `doc "
"<https://cotengra.readthedocs.io/en/latest/advanced.html>`__ 更多精彩示例在  "
"`contractor 教程 <https:// github.com/quclub/tensorcircuit-"
"tutorials/blob/master/tutorials/contractors.ipynb>`__."

#: ../../source/quickstart.rst:475
msgid "**Setup in Function or Context Level**"
msgstr "**函数和上下文级别的设置**"

#: ../../source/quickstart.rst:477
msgid ""
"Beside global level setup, we can also setup the backend, the dtype, and "
"the contractor at the function level or context manager level:"
msgstr "除了全局级别设置，我们还可以在函数级别或上下文管理器级别设置后端、dtype 和contractor："

#: ../../source/quickstart.rst:495
msgid "Noisy Circuit Simulation"
msgstr "噪声电路模拟"

#: ../../source/quickstart.rst:497
msgid "**Monte Carlo State Simulator:**"
msgstr "**蒙特卡洛态模拟器**"

#: ../../source/quickstart.rst:499
msgid ""
"For the Monte Carlo trajectory noise simulator, the unitary Kraus channel"
" can be handled easily. TensorCircuit also supports fully jittable and "
"differentiable general Kraus channel Monte Carlo simulation, though."
msgstr ""
"对于蒙特卡洛轨迹噪声模拟器，可以轻松处理幺正的 Kraus 通道。 不过，TensorCircuit 还支持完全可即时编译和可微分的通用 "
"Kraus 通道蒙特卡罗模拟。"

#: ../../source/quickstart.rst:526
msgid "**Density Matrix Simulator:**"
msgstr "**密度矩阵模拟器**"

#: ../../source/quickstart.rst:528
msgid ""
"Density matrix simulator ``tc.DMCircuit`` simulates the noise in a full "
"form, but takes twice qubits to do noiseless simulation. The API is the "
"same as ``tc.Circuit``."
msgstr "密度矩阵模拟器``tc.DMCircuit`` 以完整形式模拟噪声，但需要两倍的量子比特。API 与 ``tc.Circuit`` 基本相同。"

#: ../../source/quickstart.rst:547
msgid "**Experiment with quantum errors:**"
msgstr ""

#: ../../source/quickstart.rst:549
msgid "Multiple quantum errors can be added on circuit."
msgstr ""

#: ../../source/quickstart.rst:565
msgid "**Experiment with readout error:**"
msgstr ""

#: ../../source/quickstart.rst:567
msgid ""
"Readout error can be added in experiments for sampling and expectation "
"value calculation."
msgstr ""

#: ../../source/quickstart.rst:593
msgid "MPS and MPO"
msgstr "矩阵乘积状态和矩阵乘积算子"

#: ../../source/quickstart.rst:595
msgid ""
"TensorCircuit has its class for MPS and MPO originally defined in "
"TensorNetwork as ``tc.QuVector``, ``tc.QuOperator``."
msgstr ""
"TensorCircuit 有自己的 MPS 和 MPO 类，起初在 TensorNetwork 中定义为“tc.QuVector” 和 "
"“tc.QuOperator”。"

#: ../../source/quickstart.rst:597
msgid ""
"``tc.QuVector`` can be extracted from ``tc.Circuit`` as the tensor "
"network form for the output state (uncontracted) by ``c.quvector()``."
msgstr ""
"作为``c.quvector()`` 的输出状态（未收缩）的张量网络形式，``tc.QuVector`` 可以从``tc.Circuit`` "
"中提取。"

#: ../../source/quickstart.rst:599
msgid ""
"The QuVector forms a wavefunction w, which can also be fed into Circuit "
"as the inputs state as ``c=tc.Circuit(n, mps_inputs=w)``."
msgstr ""
"QuVector 形成一个波函数 w，它也可以作为 ``c=tc.Circuit(n, mps_inputs=w)`` 的输入状态输入到 "
"Circuit 中。"

#: ../../source/quickstart.rst:601
msgid "MPS as input state for circuit"
msgstr "MPS 作为电路的输入状态"

#: ../../source/quickstart.rst:603
msgid ""
"The MPS/QuVector representation of the input state has a more efficient "
"and compact form."
msgstr "输入状态的 MPS/QuVector 表示具有更高效和紧凑的形式。"

#: ../../source/quickstart.rst:615
msgid "MPS as (uncomputed) output state for circuit"
msgstr "MPS 作为电路的（未计算的）输出状态"

#: ../../source/quickstart.rst:617
msgid ""
"For example, a quick way to calculate the wavefunction overlap without "
"explicitly computing the state amplitude is given as below:"
msgstr "例如，在不显式计算状态幅度的情况下，计算波函数重叠的快速方法如下："

#: ../../source/quickstart.rst:634
msgid "MPO as the gate on the circuit"
msgstr "MPO 作为电路上的门"

#: ../../source/quickstart.rst:636
msgid ""
"Instead of a common quantum gate in matrix/node format, we can directly "
"apply a gate in MPO/QuOperator format."
msgstr "代替矩阵/节点格式的普通量子门，我们可以直接应用 MPO/QuOperator 格式的门。"

#: ../../source/quickstart.rst:647
msgid ""
"The representative gate defined in MPO format is the ``multicontrol`` "
"gate."
msgstr "以 MPO 格式定义的代表门是 ``multicontrol`` 门。"

#: ../../source/quickstart.rst:649
msgid "MPO as the operator for expectation evaluation on a circuit"
msgstr "MPO作为电路期望估测算子"

#: ../../source/quickstart.rst:651
msgid ""
"We can also measure operator expectation on the circuit output state "
"where the operator is in MPO/QuOperator format."
msgstr "我们还可以测量运算符对 MPO/QuOperator 格式的电路输出状态的期望。"

#: ../../source/quickstart.rst:663
msgid "Interfaces"
msgstr "接口"

#: ../../source/quickstart.rst:665
msgid "**PyTorch Interface to Hybrid with PyTorch Modules:**"
msgstr "**与 PyTorch 模块混合的 PyTorch 接口：**"

#: ../../source/quickstart.rst:667
msgid ""
"As we have mentioned in the backend section, the PyTorch backend may lack"
" advanced features. This doesn't mean we cannot hybrid the advanced "
"circuit module with PyTorch neural module. We can run the quantum "
"function on TensorFlow or Jax backend while wrapping it with a Torch "
"interface."
msgstr ""
"正如我们在后端部分提到的，PyTorch 后端可能缺少高级功能。 这并不意味着我们不能将高级量子电路模块与 PyTorch 神经模块混合。 "
"我们可以在 TensorFlow 或 Jax 后端运行量子函数，同时使用 Torch 接口包装它。 "

#: ../../source/quickstart.rst:694
msgid ""
"For a GPU/CPU, torch/tensorflow, quantum/classical hybrid machine "
"learning pipeline enabled by tensorcircuit, see `example script "
"<https://github.com/tencent-quantum-"
"lab/tensorcircuit/blob/master/examples/hybrid_gpu_pipeline.py>`__."
msgstr ""

#: ../../source/quickstart.rst:696
msgid ""
"There is also a more flexible torch interface that support static non-"
"tensor inputs as keyword arguments, which can be utilized as below:"
msgstr ""

#: ../../source/quickstart.rst:710
msgid ""
"We also provider wrapper of quantum function for torch module as "
":py:meth:`tensorcircuit.TorchLayer` alias to "
":py:meth:`tensorcircuit.torchnn.QuantumNet`."
msgstr ""

#: ../../source/quickstart.rst:712
msgid ""
"For ``TorchLayer``, ``use_interface=True`` is by default, which natively "
"allow the quantum function defined on other tensorcircuit backends, such "
"as jax or tf for speed consideration."
msgstr ""

#: ../../source/quickstart.rst:714
msgid ""
"``TorchLayer`` can process multiple input arguments as multiple function "
"inputs, following torch practice."
msgstr ""

#: ../../source/quickstart.rst:742
msgid "**TensorFlow interfaces:**"
msgstr ""

#: ../../source/quickstart.rst:744
msgid ""
"Similar rules apply similar as torch interface. The interface can even be"
" used within jit environment outside. See "
":py:meth:`tensorcircuit.interfaces.tensorflow.tensorflow_interface`."
msgstr ""

#: ../../source/quickstart.rst:747
msgid ""
"We also provider ``enable_dlpack=True`` option in torch and tf "
"interfaces, which allow the tensor transformation happen without memory "
"transfer via dlpack, higher version of tf or torch package required."
msgstr ""

#: ../../source/quickstart.rst:750
msgid ""
"We also provider wrapper of quantum function for keras layer as "
":py:meth:`tensorcircuit.KerasLayer` alias to "
":py:meth:`tensorcircuit.keras.KerasLayer`."
msgstr ""

#: ../../source/quickstart.rst:752
msgid ""
"``KerasLayer`` can process multiple input arguments with the input as a "
"dict, following the common keras practice, see example below."
msgstr ""

#: ../../source/quickstart.rst:774
msgid "**Scipy Interface to Utilize Scipy Optimizers:**"
msgstr "**使用 scipy接口使用scipy优化器：**"

#: ../../source/quickstart.rst:776
msgid ""
"Automatically transform quantum functions as scipy-compatible values and "
"grad functions as provided for scipy interface with ``jac=True``."
msgstr "为带有 jac=True 的 scipy 接口自动将量子函数转换为与 scipy 兼容的 value 和 grad 函数。"

#: ../../source/quickstart.rst:802
msgid "Templates as Shortcuts"
msgstr "捷径模板"

#: ../../source/quickstart.rst:804
msgid "**Measurements:**"
msgstr "**测量**"

#: ../../source/quickstart.rst:806
msgid "Ising type Hamiltonian defined on a general graph"
msgstr "在一般图上定义的伊辛型哈密顿量"

#: ../../source/quickstart.rst:808
msgid ""
"See "
":py:meth:`tensorcircuit.templates.measurements.spin_glass_measurements`"
msgstr "参考 :py:meth:`tensorcircuit.templates.measurements.spin_glass_measurements`"

#: ../../source/quickstart.rst:810
msgid "Heisenberg Hamiltonian on a general graph with possible external fields"
msgstr "具有可能存在的外场的一般图上的海森堡哈密顿量"

#: ../../source/quickstart.rst:812
msgid ""
"See "
":py:meth:`tensorcircuit.templates.measurements.heisenberg_measurements`"
msgstr "参考 :py:meth:`tensorcircuit.templates.measurements.heisenberg_measurements`"

#: ../../source/quickstart.rst:814
msgid "**Circuit Blocks:**"
msgstr "**电路块**"

#~ msgid "**JIT support:**"
#~ msgstr "**即时编译支持**"

#~ msgid "**VMAP support:**"
#~ msgstr "**向量并行化支持**"

#~ msgid "Install from GitHub"
#~ msgstr "从GitHub安装"

#~ msgid ""
#~ "For beta version usage, one needs "
#~ "to install tensorcircuit package from "
#~ "GitHub. For development and PR workflow,"
#~ " please refer to `contribution "
#~ "<contribution.html>`__ instead."
#~ msgstr ""
#~ "如需使用测试版本，则需要从 GitHub 安装 tensorcircuit。对于开发和 PR"
#~ " 工作流程，请另外参考 `贡献 <contribution.html>`__ 。"

#~ msgid ""
#~ "For private tensorcircuit-dev repo, one"
#~ " needs to first configure the SSH "
#~ "key on GitHub and locally, please "
#~ "refer to `GitHub doc "
#~ "<https://docs.github.com/en/authentication/connecting-to-"
#~ "github-with-ssh>`__"
#~ msgstr ""
#~ "对于私有 tensorcircuit 开发库，首先需要在 GitHub 和本地配置 "
#~ "SSH 密钥, 请参考 `GitHub 文档 "
#~ "<https://docs.github.com/en/authentication/connecting-to-"
#~ "github-with-ssh>`__"

#~ msgid ""
#~ "Then try ``pip3 install --force-"
#~ "reinstall git+ssh://git@github.com/quclub/tensorcircuit-"
#~ "dev.git`` in shell."
#~ msgstr ""
#~ "然后尝试在命令行窗口中输入 ``pip3 install --force-reinstall"
#~ " git+ssh://git@github.com/quclub/tensorcircuit-dev.git`` "
#~ "。"

#~ msgid ""
#~ "Depending on one's need, one may "
#~ "further pip install tensorflow (for "
#~ "TensorFlow backend) or jax and jaxlib"
#~ " (for jax backend) or `cotengra "
#~ "<https://github.com/jcmgray/cotengra>`__ (for more "
#~ "advanced tensornetwork contraction path "
#~ "solver)."
#~ msgstr ""
#~ "基于个人情况，用户可能需要进一步安装 tensorflow, jax 或 jaxlib"
#~ " 或 `cotengra <https://github.com/jcmgray/cotengra>`_"
#~ " 以满足后端要求。"

#~ msgid ""
#~ "If one needs circuit visualization on"
#~ " JupyterLab, python package `wand "
#~ "<https://docs.wand-py.org/en/0.6.7/>`__ and its "
#~ "binary bindings, as well as LaTeX "
#~ "installation, are required."
#~ msgstr ""
#~ "如果需要在 JupyterLab 中进行电路可视化，则需要 python 库  "
#~ "`wand <https://docs.wand-py.org/en/0.6.7/>`__ "
#~ "及其二进制绑定以及 LaTeX 的安装。"

#~ msgid "Import from Qiskit: ``c = tc.Circuit.from_qiskit(QuantumCircuit, n)``"
#~ msgstr "从 Qiskit 导入：``c = tc.Circuit.from_qiskit(QuantumCircuit, n)``"

#~ msgid "For x86 Linux or Mac,"
#~ msgstr ""

#~ msgid ""
#~ "For Mac with M series chips (arm"
#~ " architecture), please refer to `TC "
#~ "on Mac M series "
#~ "<contribs/development_MacARM.html>`_."
#~ msgstr ""




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/sharpbits.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-07 10:47+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/sharpbits.rst:3
msgid "TensorCircuit: The Sharp Bits 🔪"
msgstr "TensorCircuit: 常见错误 🔪"

#: ../../source/sharpbits.rst:5
msgid ""
"Be fast is never for free, though much cheaper in TensorCircuit, but you "
"have to be cautious especially in terms of AD, JIT compatibility. We will"
" go through the main sharp edges 🔪 in this note."
msgstr "虽然在TensorCircuit中速度很快，但是你必须小心，尤其是在AD和JIT兼容性方面。"

#: ../../source/sharpbits.rst:9
msgid "Jit Compatibility"
msgstr "Jit 兼容性"

#: ../../source/sharpbits.rst:12
msgid "Non tensor input or varying shape tensor input"
msgstr "非向量输入或者变化形状的向量输入"

#: ../../source/sharpbits.rst:14
msgid ""
"The input must be in tensor form and the input tensor shape must be fixed"
" otherwise the recompilation is incurred which is time-consuming. "
"Therefore, if there are input args that are non-tensor or varying shape "
"tensors and frequently change, jit is not recommend."
msgstr "输入必须是张量形式，且输入张量的形状必须固定，否则会重新编译，这是非常耗时的。因此，如果有输入参数是非张量或者变化形状的张量，且经常变化，不建议使用jit。"

#: ../../source/sharpbits.rst:38
msgid "Mix use of numpy and ML backend APIs"
msgstr "混合使用numpy和ML后端API"

#: ../../source/sharpbits.rst:40
msgid ""
"To make the function jittable and ad-aware, every ops in the function "
"should be called via ML backend (``tc.backend`` API or direct API for the"
" chosen backend ``tf`` or ``jax``). This is because the ML backend has to"
" create the computational graph to carry out AD and JIT transformation. "
"For numpy ops, they will be only called in jit staging time (the first "
"run)."
msgstr ""
"为了使函数可jit和可AD，函数中的每个操作都应该通过ML后端（``tc.backend`` API或者直接调用后端API ``tf`` 或者 "
"``jax``）。这是因为ML后端必须创建计算图来\"进行AD和JIT转换。对于numpy操作，它们只会在jit编译阶段被调用（第一次运行）。"

#: ../../source/sharpbits.rst:54
msgid ""
"Numpy call inside jitted function can be helpful if you are sure of the "
"behavior is what you expect."
msgstr "如果你确定numpy调用的行为是你期望的，那么在jit函数中调用numpy是有帮助的。"

#: ../../source/sharpbits.rst:83
msgid "list append under if"
msgstr "if下的list append"

#: ../../source/sharpbits.rst:85
msgid ""
"Append something to a Python list within if whose condition is based on "
"tensor values will lead to wrong results. Actually values of both branch "
"will be attached to the list. See example below."
msgstr "在if条件基于张量值的情况下，将内容附加到Python列表中会导致错误的结果。实际上，两个分支的值都会被附加到列表中。参见下面的例子。"

#: ../../source/sharpbits.rst:108
msgid ""
"The above code raise ``ConcretizationTypeError`` exception directly for "
"Jax backend since Jax jit doesn't support tensor value if condition."
msgstr "上面的代码直接为Jax后端引发了``ConcretizationTypeError``异常，因为Jax jit不支持张量值if条件。"

#: ../../source/sharpbits.rst:110
msgid "Similarly, conditional gate application must be takend carefully."
msgstr "类似地，必须小心地应用条件门。"

#: ../../source/sharpbits.rst:145
msgid "Tensor variables consistency"
msgstr ""

#: ../../source/sharpbits.rst:148
msgid ""
"All tensor variables' backend (tf vs jax vs ..), dtype (float vs "
"complex), shape and device (cpu vs gpu) must be compatible/consistent."
msgstr ""

#: ../../source/sharpbits.rst:150
msgid "Inspect the backend, dtype, shape and device using the following codes."
msgstr ""

#: ../../source/sharpbits.rst:162
msgid ""
"If the backend is inconsistent, one can convert the tensor backend via "
":py:meth:`tensorcircuit.interfaces.tensortrans.general_args_to_backend`."
msgstr ""

#: ../../source/sharpbits.rst:173
msgid ""
"If the dtype is inconsistent, one can convert the tensor dtype using "
"``tc.backend.cast``."
msgstr ""

#: ../../source/sharpbits.rst:184
msgid ""
"Also note the jax issue on float64/complex128, see `jax gotcha "
"<https://github.com/google/jax#current-gotchas>`_."
msgstr ""

#: ../../source/sharpbits.rst:186
msgid ""
"If the shape is not consistent, one can convert the shape by "
"``tc.backend.reshape``."
msgstr ""

#: ../../source/sharpbits.rst:188
msgid ""
"If the device is not consistent, one can move the tensor between devices "
"by ``tc.backend.device_move``."
msgstr ""

#: ../../source/sharpbits.rst:192
msgid "AD Consistency"
msgstr "AD一致性"

#: ../../source/sharpbits.rst:194
msgid ""
"TF and JAX backend manage the differentiation rules differently for "
"complex-valued function (actually up to a complex conjuagte). See issue "
"discussion `tensorflow issue "
"<https://github.com/tensorflow/tensorflow/issues/3348>`_."
msgstr ""
"TF和JAX后端对复值函数的微分规则的管理方式不同（实际上是复共轭）。参见讨论 `tensorflow issue "
"<https://github.com/tensorflow/tensorflow/issues/3348>`_。"

#: ../../source/sharpbits.rst:196
msgid ""
"In TensorCircuit, currently we make the difference in AD transparent, "
"namely, when switching the backend, the AD behavior and result for "
"complex valued function can be different and determined by the nature "
"behavior of the corresponding backend framework. All AD relevant ops such"
" as ``grad`` or ``jacrev`` may be affected. Therefore, the user must be "
"careful when dealing with AD on complex valued function in a backend "
"agnostic way in TensorCircuit."
msgstr ""
"在TensorCircuit中，我们目前使AD的差异透明，即在切换后端时，复值函数的AD行为和结果可能不同，并由相应后端框架的本质行为决定。所有与AD相关的操作，如"
" ``grad`` 或者 ``jacrev`` "
"都可能受到影响。因此，用户在TensorCircuit中以后端无关的方式处理复值函数的AD时必须小心。"

#: ../../source/sharpbits.rst:199
msgid ""
"See example script on computing Jacobian with different modes on "
"different backends: `jacobian_cal.py <https://github.com/tencent-quantum-"
"lab/tensorcircuit/blob/master/examples/jacobian_cal.py>`_. Also see the "
"code below for a reference:"
msgstr ""
"参考不同后端的不同模式下计算Jacobian的示例脚本：`jacobian_cal.py <https://github.com/tencent-"
"quantum-"
"lab/tensorcircuit/blob/master/examples/jacobian_cal.py>`_。另外请参考下面的代码:"




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/tutorial.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-08 21:05+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/tutorial.rst:3
msgid "Jupyter Tutorials"
msgstr ""




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/tutorial_cn.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-08 21:05+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/tutorial_cn.rst:3
msgid "案例教程"
msgstr ""




================================================
FILE: docs/source/locale/zh/LC_MESSAGES/whitepapertoc.po
================================================
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-09 17:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/whitepapertoc.rst:3
msgid "Whitepaper Tutorials"
msgstr ""




================================================
FILE: docs/source/textbook/chap1.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 第一章 线性代数 (Linear Algebra)
"""

# 本章代码通过numpy展示，导入numpy库
import numpy as np
from scipy import linalg as la

"""
## 1 向量
### 1.1 向量 （vector）

具有大小和方向的量，用 $\vert v\rangle$ 表示列向量，$\langle v\vert$ 表示行向量。
"""

# 下面是一个二维向量的例子

print("行向量:")
print(np.array([2, 3]))

print()

print("列向量:")
print(np.array([[2], [3]]))
# Output:
#   行向量:

#   [2 3]

#   

#   列向量:

#   [[2]

#    [3]]


"""
### 1.2 线性无关（linear independent）

在线性代数里，一组向量中$A: a_1, a_2, ... a_n$，没有向量可以通过其他向量的线性组合而得到，我们称这一组向量为线性无关。

公式上，如果只存在$k_1,k_2, \cdots k_n$全为0的一组解满足

$$
k_1 a_1 + k_2 a_2 + \cdots k_n a_n = 0
$$

则向量组$A$是线性无关。
"""

A = (np.array([[1, 0, 0]]), np.array([[1, 1, 0]]), np.array([[1, 0, 1]]))

# 或者一般我们写成矩阵的形式

np.array([[1, 1, 0], [0, 1, 0], [0, 0, 1]])
# Output:
#   array([[1, 1, 0],

#          [0, 1, 0],

#          [0, 0, 1]])

"""
### 1.3 基 (Basis)

在线性空间中，一组向量 $\{ |v_0\rangle, |v_1\rangle, ...\} \in \mathbb{C}^m$ 的所有线性组合（span）的集合叫 $\mathbb{C}^m$的子空间(subspace).

如果这一组向量是线性无关的，那么这组向量是所有线性组合所构成的子空间中的基。

假设一组向量是基，那么基中的向量的个数 <= $m$，反之，向量个数大于所在空间维度则一定不是基。
"""

"""
## 2. 矩阵 （matrix）

数学上，一个$m \times n$的矩阵是一个由$m$行$n$列元素排列成的矩阵阵列。矩阵可以将向量进行线性变换：
$$
|v\rangle \rightarrow | v^{\prime} \rangle = M |v\rangle
$$
"""

M = np.array([[1, 2], [2, 1]])

v = np.array([1, 2])

np.matmul(M, v)
# Output:
#   array([5, 4])

"""
### 2.1 矩阵相乘

若$A$为$m \times n$的矩阵，$B$为$n \times p$的矩阵，则他们的乘积$AB$（有时记作$A \cdot B$）是一个$m \times p$的矩阵，记作$C = A \cdot B$，其中矩阵$C$中的第$i$行第$j$列元素可以表示为：
   $$
   C_{ij} = (AB)_{ij} = 
   a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj} =
   \sum_{r=1}^na_{ir}b_{rj}
   $$
"""

A = np.array([[1, 2], [2, 1]])

B = np.array([[0, 1], [1, 0]])

np.matmul(A, B)
# Output:
#   array([[2, 1],

#          [1, 2]])

"""
### 2.2 内积（inner product）

   內积可以看成是一种特殊的矩阵乘法。对于在希尔伯特空间中的两个向量$|a\rangle$和$|b\rangle$，$\langle a | b \rangle$表示它们的内积，$\langle a|$定义为向量$|a\rangle$的共轭转置（conjugate transpose），也可以记作$|a\rangle^\dagger$，有
   
$$\langle a|b \rangle = (a_1^*, a_2^*, \cdots, a_n^*)\left(
   	\begin{array}{c}
   		b_1 \\
   		b_2 \\
   		\vdots \\
   		b_n \\
   	\end{array}
   \right) = a_1^*b_1 + a_2^*b_2 + \cdots + a_n^*b_n$$

​此处$*$表示共轭复数（对于实数$a$和$b$，$a+ib$的共轭复数是$a-ib$）。

"""

base = np.arange(3)

a = base - 1j * base
b = base - 2j * base

print("向量 a:")
print(a)
print("向量 b:")
print(b)

print("a的共轭转置:")
print(a.conj().T)
np.inner(a.conj().T, b)
# Output:
#   向量 a:

#   [0.+0.j 1.-1.j 2.-2.j]

#   向量 b:

#   [0.+0.j 1.-2.j 2.-4.j]

#   a的共轭转置:

#   [0.-0.j 1.+1.j 2.+2.j]

#   (15-5j)

"""
### 2.3 外积（outer product）

   $| a \rangle \langle b |$表示希尔伯特空间中两个向量的外积，有
   $$
   |a \rangle \langle b| = 
   \begin{pmatrix}
   	a_1 \\
   	a_2 \\
   	\vdots \\
   	a_n \\
   \end{pmatrix}(b_1^*, b_2^*, \cdots, b_n^*) = 
   	\begin{pmatrix}
   	a_1b_1^* & a_1b_2^* & \cdots & a_1b_n^* \\
   	a_2b_1^* & a_2b_2^* & & \vdots \\
   	\vdots & & \ddots & \vdots \\
   	a_nb_1^* & \cdots & \cdots & a_nb_n^* \\
   \end{pmatrix}
   $$
"""

np.outer(a, b.conj().T)
# Output:
#   array([[ 0.+0.j,  0.+0.j,  0.+0.j],

#          [ 0.-0.j,  3.+1.j,  6.+2.j],

#          [ 0.-0.j,  6.+2.j, 12.+4.j]])

"""
### 2.4 张量积（tensor product）

   $| a \rangle \otimes | b \rangle$表示希尔伯特空间中两个向量的张量积，有
   $$
   |a \rangle \otimes |b \rangle = 
   \begin{pmatrix}
   	a_1 \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} \\
   	a_2 \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} \\
   \end{pmatrix} = 
   \begin{pmatrix}
   	a_1b_1 \\
   	a_1b_2 \\
   	a_2b_1 \\
   	a_2b_2 \\
   \end{pmatrix}
   $$
"""

np.kron(a, b)
# Output:
#   array([ 0. +0.j,  0. +0.j,  0. +0.j,  0. +0.j, -1. -3.j, -2. -6.j,

#           0. +0.j, -2. -6.j, -4.-12.j])

"""
   矩阵$A$和$B$的张量积，有
   $$
   A \otimes B = 
   \begin{pmatrix}
   	a_{11}B & \cdots & a_{1n}B \\
   	\vdots & \ddots & \vdots \\
   	a_{m1}B & \cdots & a_{mn}B \\
   \end{pmatrix}
   $$
   
"""

np.kron(np.eye(2), np.ones((2, 2)))
# Output:
#   array([[1., 1., 0., 0.],

#          [1., 1., 0., 0.],

#          [0., 0., 1., 1.],

#          [0., 0., 1., 1.]])

"""
### 2.5 正交完备集

1. 集合只含有单位向量（向量长度为1）
2. 两两向量之间的内积都为0

以下就是一组正交完备集。
"""

v = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
v
# Output:
#   array([[1, 0, 0],

#          [0, 1, 0],

#          [0, 0, 1]])

print(np.dot(v[0], v[1]))
print(np.dot(v[1], v[2]))
print(np.dot(v[0], v[2]))

print(np.linalg.norm(v[0]))
print(np.linalg.norm(v[1]))
print(np.linalg.norm(v[2]))
# Output:
#   0

#   0

#   0

#   1.0

#   1.0

#   1.0


"""
## 3. 特殊矩阵

### 3.1 可逆矩阵

给定一个$n$阶方阵$A$，若存在一$n$阶方阵$B$，使得$AB = BA = I_n$，其中$I_n$为$n$阶单位矩阵，则称$A$是可逆的，且$B$是$A$的逆矩阵，记作$A^{-1}$。
"""

A = np.array(np.arange(4).reshape((2, 2)))
B = np.linalg.inv(A)

print(A)
print(B)
print(A @ B)
# Output:
#   [[0 1]

#    [2 3]]

#   [[-1.5  0.5]

#    [ 1.   0. ]]

#   [[1. 0.]

#    [0. 1.]]


"""
### 3.2 转置（transpose）

矩阵$A$的转置是另一个矩阵$A^T$，由下列等价动作建立：

- 把$A$的横行写为$A^T$的纵列
- 把$A$的纵列写为$A^T$的横行

形式上说，$m \times n$矩阵的转置是$n \times m$的矩阵。
"""

print(A)
print(A.T)
# Output:
#   [[0 1]

#    [2 3]]

#   [[0 2]

#    [1 3]]


"""
### 3.3 共轭转置（conjugate transpose）

矩阵$A$的共轭转置$A^\dagger$是通过对矩阵A转置后再将所有元素替换为各自的共轭复数得到的。
"""

C = A + 1j * A
print(C)
print(C.conj().T)
# Output:
#   [[0.+0.j 1.+1.j]

#    [2.+2.j 3.+3.j]]

#   [[0.-0.j 2.-2.j]

#    [1.-1.j 3.-3.j]]


"""
### 3.4 厄米矩阵（Hermitian matrix)

$H$是一个厄米矩阵，满足

$$H = H^\dagger，$$

也就是 $H_{ij} = H_{ji}^*$。
"""

H = np.array([[3, 3 - 2j], [3 + 2j, 2]])

H.conj().T
# Output:
#   array([[3.-0.j, 3.-2.j],

#          [3.+2.j, 2.-0.j]])

"""
### 3.5 幺正矩阵（unitary matrix）

$U$ 是一个幺正矩阵，满足

$$UU^\dagger = I,$$

也就是

$$\sum_j U_{ij}U^*_{jk} = \delta_{ik}.$$

幺正矩阵的各行和各列可以看做正交完备向量组。
幺正矩阵作用在向量上，向量的长度不变。
"""

# 幺正矩阵例子
U = np.array([[1 + 1j, 1 - 1j], [1 - 1j, 1 + 1j]]) / 2

np.matmul(U, U.conj().T)
# Output:
#   array([[1.+0.j, 0.+0.j],

#          [0.+0.j, 1.+0.j]])

"""
## 4. 矩阵本征分解

$$A|v \rangle = \lambda|v \rangle $$

符合上式的向量$|v \rangle$是矩阵A的特征向量（eigenvector），$\lambda$是与特征向量对应的特征值（eigenvalue）。值得注意的是，任意满足$AA^\dagger = A^\dagger A$的矩阵可以进行以下分解：

$$A = \sum_j \lambda_j|v_j\rangle\langle v_j|$$

其中，$\lambda_j$ 是$A$的第$j$个本征值，$|v_j\rangle$ 是是其对应的本征向量，且$\{|v_j\rangle\}$构成正交归一的基。
"""

A = np.array([[0, -1j], [1j, 0]])

print(np.matmul(A, A.conj().T))
print(np.matmul(A.conj().T, A))

w, v = np.linalg.eigh(A)
# 验证第一对特征值和特征向量，第四部分会进行讲解
print(np.dot(A, v[:, 0]) - w[0] * v[:, 0])
# 验证第二对特征值和特征向量
print(np.dot(A, v[:, 1]) - w[1] * v[:, 1])
# Output:
#   [[1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j]]

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j]]

#   [0.+0.j 0.+0.j]

#   [0.+0.j 0.+0.j]


"""
## 
"""

"""
## 5. 矩阵函数

矩阵函数根据泰勒展开定义（对于矩阵，乘法和加法有定义，而泰勒展开中只有这两种计算）
特殊的对于 Hermitian 矩阵 A，我们有对角化 $A = U^\dagger \Lambda U$ ，其中 $\Lambda_{ij} = (\lambda_i)\delta_{ij}$ 是对角矩阵。那么我们有 $f(A) = U^\dagger f(\Lambda) U$, 其中 $f(\Lambda)_{ij} = f(\lambda_i)\delta_{ij}$ .

例子：根据以上矩阵函数定义，分析对于矩阵欧拉公式是否成立 何时成立 $e^{ix} = \cos(x)+i\sin(x)$

"""

X = np.array([[0, 1.0], [1.0, 0]])
Z = np.array([[1.0, 0], [0, -1.0]])

e, U = np.linalg.eigh(X)
print(U)
# Output:
#   [[-0.70710678  0.70710678]

#    [ 0.70710678  0.70710678]]


lbd = U.conj().T @ X @ U
np.testing.assert_allclose(np.diag(e), lbd, atol=1e-8)
print(lbd)
cos1 = la.cosm(X)
print(cos1)
cos2 = U @ np.diag(np.cos(e)) @ U.conj().T
print(cos2)
np.testing.assert_allclose(cos1, cos2, atol=1e-8)
# Output:
#   [[-1.00000000e+00 -2.23711432e-17]

#    [ 2.23711432e-17  1.00000000e+00]]

#   [[0.54030231 0.        ]

#    [0.         0.54030231]]

#   [[5.40302306e-01 2.28559205e-17]

#    [2.28559205e-17 5.40302306e-01]]


np.sin(Z), la.sinm(Z)
# Output:
#   (array([[ 0.84147098,  0.        ],

#           [ 0.        , -0.84147098]]),

#    array([[ 0.84147098,  0.        ],

#           [ 0.        , -0.84147098]]))



================================================
FILE: docs/source/textbook/chap2.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 第二章 量子力学 (Quantum Mechanics)
"""

"""
## 1. 量子力学基础

&emsp;&emsp;量子计算顾名思义，是使用量子力学规律进行计算的全新范式。目前理论和实验已经揭示，量子计算在计算能力上有远远超过传统计算机（也称经典计算）的潜力。量子力学是描述微观物理的最精确的理论，迄今为止得到了海量实验的验证。从数学上来说，量子力学的本质是希尔伯特空间（Hilbert space）及作用于其上的算子。当空间维数有限的情况下等价于在复数域上的线性空间。在本节中，我们将考虑有限维的线性空间及量子计算的基础。 相关线性代数的基础知识在附录中给出以供参考。

&emsp;&emsp;量子力学的基本原理是一组数学公理，我们先介绍这些公理。
"""

"""
### 1.1 公设1：希尔伯特空间（Hilbert space）

&emsp;&emsp;<b>任意封闭物理系统都具有一个相关联的具有內积结构的复向量空间（希尔伯特空间），称为量子态空间 $\mathcal{H}$ 。整个系统的物理性质完全由此空间中的向量来描述，称为态向量 $\vert\psi\rangle$ 。</b>

**说明**：

-  在量子力学中，通常使用狄拉克符号（Dirac notation） $|\cdot \rangle$ 来表示希尔伯特空间$\mathcal{H}$中的向量，读作“ket”。

-  数学上，我们可以用一个复向量簇（Ray）来表示量子态。这里，$|\psi\rangle$ 所在的向量簇指代任意向量$|\psi'\rangle = C|\psi\rangle, C\in \mathbb{C}, C\neq 0$，这些向量在物理上都是等价的。量子态代表物理系统的一种状态，例如具有某个特定的能量，动量，角动量等。我们可能听说过量子力学的叠加性原理（例如著名的薛定谔的猫）。 具体来说，对任意两个量子态$|u\rangle,|v\rangle \in \mathcal{H}$, $|u\rangle + |v\rangle \in \mathcal{H}$，即系统可以“同时”处于多种状态。 在我们关心的量子计算领域，$\mathcal{H}$维数是有限的(设为$N$)，那么我们可以选取一组线性无关的向量$\{|\phi_i\rangle\}=\{|\phi_i\rangle|i=0,\cdots,N-1\}$作为基向量（basis，例如日常的三维空间在$x$,$y$,$z$方向的三个单位向量$\{e_x, e_y,e_z\}$即为一组基向量）。$\mathcal{H}$中的任意量子态都可以用这组基线性展开：

$$|\psi\rangle = \sum_i c_i |\phi_i\rangle, \quad c_i \in \mathbb{C}，\forall i$$

这组基向量也称为完备基。方便起见，在这组基向量下，我们可以将$|\psi\rangle$表示成：

$$|\psi \rangle \cong (c_0, c_2,\cdots, c_{N-1})^T, \quad$$

称为$|\psi\rangle$在$\{|\phi\rangle\}_i$下的表示。这里的展开系数$c_i$称为几率幅，$T$代表向量和矩阵的转置。这就是我们所熟悉$n$维复数向量（列向量）。

- 正如在欧几里得空间可以定义向量內积一样，我们也可以定义希尔伯特空间的向量內积。我们可以定义$|v\rangle$ 的对偶向量 $\langle v|$（$\langle\cdot|$ 称为“bra”)。这样我们就有

$$|v\rangle \cong (v_0, \cdots, v_{N-1})^T, \quad \langle v| \cong (v_0^*, \cdots, v_{N-1}^*)。$$

使用Dirac notation方便的原因之一是我们可以将内积表示为“braket”:

$$\langle v|w\rangle = \sum_{i,j=0}^{N-1} v_i^* w_j \langle\phi_i|\phi_j \rangle。$$

可以看出，內积是一种线性算运算，且满足$\langle u|v\rangle=\langle v|u\rangle^*$和$\langle v|v \rangle\geq 0$。有了內积的概念，我们就可以定义向量的长度：$\|v \|_2 = \sqrt{\sum_{i=0}^{N-1} |v_i|^2} = \sqrt{\langle v|v\rangle}$。我们称长度为1的向量为单位长度向量。在接下来的讨论中，我们默认**描述系统**的量子态的长度为1。至此，我们对$N$维希尔伯特空间有

$$\mathcal{H}\cong \mathbb{C}^N。$$


- 如果两个向量$|v\rangle,|w\rangle$ 满足 $\langle w |v \rangle = 0$, 则我们称这两个向量是正交的。如果我们选定一组基$\{|i\rangle|i=0,\cdots,N-1\}$, 满足

$$\langle j | i\rangle = \delta_{ij} =  
\begin{cases}
& 1, \quad  i = j\\
& 0, \quad  i\neq j\\
\end{cases},$$

且$\|i\|_2=1, \forall i$, 那么这组基称为正交归一。如不明确说明，接下来我们都将使用正交归一基。这样$|\psi\rangle$就可以表示为:

$$
|\psi\rangle = \sum_i \psi_i |i\rangle \cong（\psi_0,\cdots, \psi_{N-1})^T, \quad \psi_i \in \mathbb{C}，\quad \sum_i |\psi_i|^2 = 1。
$$

* > 例子：考虑一个二维系统$\mathcal{H}\cong \mathbb{C}^2$，定义两个基向量为$|0\rangle$和$|1\rangle$。这样任意量子态可以表示为

$$
|\psi\rangle = \alpha|0\rangle+\beta|1\rangle,\quad |\alpha|^2+|\beta|^2 =1, 
$$

其中$\alpha,\beta$都是复数。

&emsp;&emsp;在正交归一基下，任意两个向量 $|u\rangle$，$|v\rangle$ 的內积可以简化为：

$$\langle v|w\rangle = (v_0^*, \cdots, v_{N-1}^*)\left(
  \begin{array}{c}
    w_0 \\
    \vdots \\
    w_{N-1}\\
  \end{array}
  \right) = \sum_j v^*_jw_j $$


- 类似地，我们可以定义两个向量之间的外积及其在基$\{|j\rangle\}$下的表示：

$$
|v\rangle \langle w| \cong \left(
  \begin{array}{c}
    v_0 \\
    \vdots \\
    v_{N-1}\\
  \end{array}
  \right) (w_0^*, \cdots, w_{N-1}^*) = \left(
  \begin{array}{ccc}
    v_0w_0^* &   \ldots &  v_0w_{N-1}^* \\
    \vdots &  \ddots &  \vdots  \\ 
    v_{N-1}w^*_0 &  \ldots  &  v_{N-1}w^*_{N-1}\\
  \end{array}
\right)。$$

注意，与內积不同，外积是一个作用在$\mathcal{H}$上的算子（矩阵）。

"""

"""
### 1.2 公设2：薛定谔方程(Schördinger equation)


&emsp;&emsp;<b>系统演化：封闭量子系统的演化由薛定谔方程描述：</b>

$$i \frac{d|\psi(t)\rangle}{dt} = H(t) |\psi(t)\rangle,$$

<b>其中$t$是时间，$H(t)$ 是一个厄米算子，称为系统的哈密顿量。</b>

**说明：**

1. 这里我们使用了原子单位，即约化普朗克常数$\hbar = 1$。

2. 对量子计算来说，我们可能不用太关心$H(t)$的具体细节（只需要知道他是一个厄米矩阵)，而更关心信息的载体——量子态的演化。这样，从时间$t_i$ 到$t_f$，由薛定谔方程，系统从$|\psi(t_i)\rangle$ 演化至 $|\psi(t_f)\rangle$可以等效地写为:

$$|\psi(t_f)\rangle = U(t_f, t_i) |\psi(t_i)\rangle，$$

这里$U(t_f, t_i)$是一个幺正算符（矩阵），具体推导可见Nielson and Chuang教科书第二章。

3. 原则上只要我们知道一个系统的哈密顿量，那么我们就可以完全了解系统的动力学。确定具体的哈密顿量就是在量子力学框架下理论物理的研究目标——不同的的物理系统会涉及到不同的哈密顿量，但这不是量子力学作为基本框架所关心的问题。

4. 在量子计算中，量子逻辑门正是一种幺正变换，其实现是靠工程上构建不同的哈密顿量来产生。详细的细节见第三章。

"""

"""
### 1.3 公设3: 量子态测量 (state measurement)



&emsp;&emsp;<b>量子态测量（Born 法则）：量子测量是由一组$\{M_k\}_{k=1}^l$的测量算子组成，这组算子满足 $\sum_k M_k^\dagger M_k = I$。 对任意量子态$|\psi\rangle\in \mathcal{H}$,在测量之后立刻以概率$p_k$变成</b>

$$|\psi\rangle\mapsto \frac{M_k|\psi\rangle}{\sqrt{p_k}} = |\psi_k\rangle$$ 

<b>其中 $p_k=\langle M_k^\dagger M_k\rangle \equiv \langle \psi| M_k^\dagger M_k| \psi\rangle = \|M_k|\psi\rangle\|_2^2\geq 0$。</b>

**说明：**

0. $M^\dagger$ 代表$M$的共轭转置，即$M^\dagger_{ij} = M^*_{ji}$。

1. 测量结果为指标$k$，同时量子态发生了“塌缩”, $|\psi\rangle$ “瞬间”变成了$|\psi_k\rangle$。我们可以验证

$$\sum_k p_k = \langle \psi | \sum_k M_k^\dagger M_k |\psi  \rangle = 1。$$


2. 冯-诺依曼（von Neumann）测量。在实际中，我们需要对一实际物理量进行测量（例如坐标，动量，角动量、能量等）。这些物理量在量子力学中都是厄米算子。原则上，在有限维$\mathcal{H}$上, 我们可以对任意厄米算子$O$进行测量。任意厄米算子$O$ 可以进行如下分解
    
$$O = \sum_j \lambda_j |j\rangle\langle j|,$$

其中，$\{|j\rangle\}$是一组正交归一基，称为$O$的本征向量（eigenstate）,$\lambda_j$ 称为 $|j\rangle$对应的本征值（eigenvalue）。对$O$进行测量，即设定

$$M_k=P_k= |k \rangle\langle k |,$$

那么我们测量到$k$ 的概率为 $p_k=\langle\psi|P_k|\psi\rangle$, 对应测到的物理量的值为$\lambda_k$。

> 例：假设$|\psi\rangle = \alpha|0\rangle + \beta|1\rangle$，我们需要测量厄米算子
    
$$Z=\left(
\begin{array}{cc}
    1 & 0  \\
    0 & -1 \\
\end{array}
\right)$$

那么，我们就有$M_0 = |0\rangle\langle0|$，$M_1=|1\rangle\langle1|$。我们就会以概率$|\alpha|^2$得到$0$，量子态塌缩到$|0\rangle$（测到相应的物理值$1$）；以概率$|\beta|^2$得到$1$，量子态塌缩到$|1\rangle$（测到相应的物理值$-1$）。
&emsp;&emsp;现在考虑对另一个厄米算子进行测量:

$$X=\left(
\begin{array}{cc}
    0 & 1  \\
    1 & 0 \\
\end{array}
\right)$$

注意$X$的本征是为$1$和$-1$，对应的本征向量为$|+\rangle=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$和$|-\rangle=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ (注意 $\langle-|+\rangle=0$ )。那么我们可以将量子态表示为

$$\begin{aligned}
|\psi\rangle &= \alpha|0\rangle + \beta|1\rangle = \frac{\alpha}{\sqrt{2}}(|+\rangle+|-\rangle) + \frac{\beta}{\sqrt{2}}(|+\rangle-|-\rangle)\\
&=\frac{\alpha+\beta}{\sqrt{2}}|+\rangle + \frac{\alpha-\beta}{\sqrt{2}}|-\rangle
\end{aligned}$$

这时，我们有$M_+ = |+\rangle\langle+|，M_- = |-\rangle\langle-|$，$P_+= |\alpha+\beta|^2/2$，$P_-= |\alpha-\beta|^2/2$。$|\psi\rangle$以概率$P_+$（$P_-$）塌缩到$|+\rangle$ ($|-\rangle$) ,得到测量结果$+$($-$)。

3. 公设3作为一条公设的基础地位非常具有争议性。如果将测量设备看成是量子力学系统，被测量的量子系统和测量设备一起形成一个更大的、孤立的、量子力学系统。 根据公设2，这个更大的孤立系统的演化可以用单一幺正变换来描述。是否有可能根据这种观点推导出公设3？即测量结果的随机性能否从确定的薛定谔方程推导出来？尽管沿着这些方向进行了大量研究，但仍然面临极大的困难，有些甚至是根本性的。 这里，我们将采取非常务实的方法，即在实践中很清楚何时应用公设2（只涉及微观系统），何时使用公设3（宏观的仪器参与测量的过程）。这样，我们就必须明确一条宏观和微观的界限。

"""

"""
### 1.4 公设4：多体量子系统



&emsp;&emsp;<b>给定两个具有各自希尔伯特空间的量子系统$\mathcal{H}_1$和$\mathcal{H}_2$ 那么他们组合量子系统对应的希尔伯特空间是这两个空间的直积：$\mathcal{H}_1\otimes \mathcal{H}_2$ . </b>

**说明**：

1. 这里两个希尔伯特空间可以代表完全不同的物理系统。例如：第一个系统可以是电子自旋，而第二系统可以是光子的极化。



2. 关于直积：定义$\mathcal{H}_1 = \text{span}\{|v_i\rangle\}_{i=0}^{d_1-1}$,  $\mathcal{H}_2 = \text{span}\{|w_i\rangle\}_{i=0}^{d_2-1}$， 那么$\mathcal{H} = \text{span}\{|v_i\rangle\otimes |w_j\rangle\}_{i=0,j=0}^{d_1-1,d_2-1}$ 。考虑两个量子态$|\psi\rangle\in\mathcal{H}_1$, $|\varphi\rangle\in \mathcal{H}_2$, 那么他们的直积是

$$|\psi\rangle\otimes |\varphi\rangle = (\psi_0, \cdots, \psi_{d_1-1})^T \otimes  (\varphi_0, \cdots, \varphi_{d_2-1})^T = (\psi_0\varphi_0\cdots, \psi_0\varphi_{d_2-1},\cdots,\psi_{d_1-1}\varphi_{d_2-1})^T.$$

3. 我们也可以定义矩阵的直积：如果

$$A = \left(
  \begin{array}{ccc}
    a_{0,0} &   \ldots &  a_{0,d_1-1} \\
    \vdots &  \ddots &  \vdots  \\ 
    a_{d_1-1,0} &  \ldots  & a_{d_1-1,d_1-1}\\
  \end{array}
\right), \quad 
B = \left(
  \begin{array}{ccc}
    b_{0,0} &   \ldots &  b_{0,d_2-1} \\
    \vdots &  \ddots &  \vdots  \\ 
    b_{d_2-1,0} &  \ldots  & b_{d_2-1,d_2-1}\\
  \end{array}
\right), \quad$$

那么

$$A\otimes B = \left(
  \begin{array}{ccc}
    a_{0,0}B &   \ldots &  a_{0,d_1-1}B \\
    \vdots &  \ddots &  \vdots  \\ 
    a_{d_1-1,0}B &  \ldots  & a_{d_1-1,d_1-1}B\\
  \end{array}
\right).$$

4. 更多的量子系统的可以通过类似地方式构造其希尔伯特空间：考虑$n$个系统，那么系统的希尔伯特空间是$\mathcal{H}=\mathcal{H}_1\otimes \mathcal{H}_2\cdots\otimes \mathcal{H}_n$。如果第$k$个系统的空间维数是$d_k$,那么整个系统的维数是$N=\Pi_{k=1}^n d_k$，需要同样数量的复数来描述。随着$n$的增加，复杂性是呈指数上升，这正是量子系统难以计算的原因，也是量子计算潜在计算能力的来源。

* > 例子：考虑一个二维系统和一个三维系统，那么$\mathcal{H}=\mathcal{H}_1\otimes \mathcal{H}_2=\mathbb{C}^{2}\otimes \mathbb{C}^3\cong \mathbb{C}^{6}$。如果$|\psi\rangle = \alpha|0\rangle+\beta|1\rangle\in \mathcal{H}_1$， $|\phi\rangle=\gamma|0\rangle+\delta|1\rangle + \xi |2\rangle\in \mathcal{H}_2$，那么

$$|\psi\rangle\otimes|\phi\rangle = \alpha\gamma|00\rangle + \alpha\delta|01\rangle+ \alpha\xi |02 \rangle +\beta\gamma|10\rangle + \beta\delta|11\rangle + \beta\xi |12\rangle.$$

"""

"""
### 1.5 量子概率与经典概率 


&emsp;&emsp;现在，我们已经了解了量子力学的全部原理。我们可以发现，量子力学的原理与牛顿力学和麦克斯韦方程描述的电磁学（两者统称为经典物理学）完全不同，体现在根本没有“力”这个概念。事实上，由于量子力学的几率本质，他与经典概率学的关系更为密切，而“力”的性质体现在决定系统演化的哈密顿量之中。考虑一个包含 $n$ 个可能取值的随机变量，我们可以定义其几率分布：

$$\vec{p} = (p_1,p_2, \cdots, p_N)^T, \quad  p_i \in \mathbb{R}_+, \sum_i {p_i} = 1。$$

经过一个经典信息处理过程后，概率分布发生了变化，我们可以用一个线性变换描述这个过程：

$$\vec{p}' = \mathcal{T} \vec{p},$$

这里矩阵$T$定义为：

$$\mathcal{T} = \left(
  \begin{array}{cccc}
    p(1|1) & p(1|2) & \ldots & p(1|N)    \\
    p(2|1) & p(2|2) & \ldots & p(N|N)     \\
      \vdots     &  \vdots    &  \ddots  &  \vdots         \\
    p(N|1) & p(N|2) &  \ldots & p(N|N)     \\
  \end{array}
\right)$$

称为概率转移矩阵，其中$p(i|j)\in \mathbb{R}$就是条件概率。我们可以简单地验证：$p'_j = \sum_i p(j|i)p(i)$。


&emsp;&emsp;类似地，在量子力学里，我们也有量子态的几率幅分布：

$$|\psi\rangle = (\psi_0, \psi_1, \cdots, \psi_{N-1})^T, \quad, \psi_i \in \mathbb{C}, \quad \sum_{i} |\psi_i|^2 =1$$

对量子态的信息处理也可以看成是一个变换：

$$|\phi\rangle = U|\psi\rangle= \sum_j \left(\sum_lU_{jl}\psi_l\right)|j\rangle =\sum_j\phi_j |j\rangle,$$

其中

$$U = \left(
  \begin{array}{cccc}
    U_{1|1} & U_{1|2} & \ldots & U_{1|N}    \\
    U_{2|1} & U_{2|2} & \ldots & U_{2|N}    \\
      \vdots     &  \vdots    &  \ddots  &  \vdots         \\
    U_{N|1} & U_{N|2} &  \ldots & U_{N|N}     \\
  \end{array}
\right).$$

&emsp;&emsp;这里，我们可以发现量子和经典概率的相似性——$U$可以看成是量子概率幅的转移矩阵。我们可以发现，正是由于Born法则的的要求可以验证，为了满足$\sum_j|\phi_j|^2 = 1$以保证测量结果的统计解释, $U$必须是一个幺正矩阵，这也正呼应了公设2的要求。我们可以对一个向量$v$定义$l_p$长度:

$$\| v\|_p = \left(\sum_i |v_i|^p\right)^{1/p} , \quad p\in N_+，$$

那么更抽象地说，经典信息处理是对$\mathbb{R}_+^{n}$ 进行保持$l_1$的变换， 而量子信息处理是对$\mathbb{C}^n$进行保持$l_2$的变换（$N$维幺正矩阵）。

</font>
"""

"""
## 2. 量子比特（qubit）


&emsp;&emsp;为了利用量子力学的性质进行信息处理，我们需要进行信息编码。量子比特就是这种编码的基本载体。数学上来看，量子比特只是量子系统的一种特殊形式（维数为2的希尔伯特空间）。在本节，我们将利用之前学到的量子力学原理来进一步了解量子比特。



### 2.1 单量子比特和Bloch球



&emsp;&emsp;简而言之，单个量子比特就是一个二维的希尔伯特空间 $\cong\mathbb{C}^{2}$ 。我们可以分别定义$|0\rangle$和$|1\rangle$态来编码信息0和1：

$$|0\rangle = \left(
  \begin{array}{c}
    1 \\
    0 \\
  \end{array}
  \right), \quad
|1\rangle = \left(
  \begin{array}{c}
    0 \\
    1 \\
  \end{array}
  \right), \quad$$

$|0\rangle$和$|1\rangle$同时也构成希尔伯特空间的基向量。任意单量子比特态可以表示为：

$$|\psi\rangle = \alpha|0\rangle+\beta|1\rangle = C(\cos(\theta/2)|0\rangle+e^{i\varphi}\sin(\theta/2)|1\rangle) \cong \cos(\theta/2)|0\rangle+e^{i\varphi}\sin(\theta/2)|1\rangle，$$

其中$C$作为一个常数没有物理意义。另外$\theta\in[-\pi, \pi)$, $\varphi\in[0,2\pi）$。这样,单个量子比特态仅仅需要2个角度就能完全刻画，而这两个角正好可以对应球坐标中的方位角($\theta$)和极角$(\varphi)$（如下图所示）。这样就赋予了每个量子态独特的几何意义：单个量子比特的量子态对应一个单位球面上的一点，这个球称为Bloch球。我们可以验证，$|0\rangle$和$|1\rangle$正位于Bloch球的北极和南极。

<div align = center>
<img src="./img/bloch.png" width="200" height="200" title="bloch" />
<figcaption align = "center"> Bloch 球 </figcaption>
</div>

</font>
"""

"""
### 2.2 多量子比特和量子纠缠（entanglement）

&emsp;&emsp;为了进行信息处理，我们也要考虑使用多个量子比特。考虑 $n$ 个量子比特，那么根据公设4, 我们需要处理的希尔伯特空间是$\mathbb{C}^{2^n}$，这个空间是由以下$2^n$个基向量张成：

$$\begin{aligned}
\{&|0_10_20_3\cdots0_{n-1}0_n\rangle, |0_10_20_3\cdots0_{n-1}1_n\rangle,  |0_10_20_3\cdots1_{n-1}0_n\rangle,  \\
&|0_10_20_3\cdots1_{n-1}1_n\rangle, \cdots \cdots， |1_1 1_2 1_3\cdots0_{n-1}0_n\rangle, |1_1 1_2 1_3\cdots0_{n-1}1_n\rangle，\\
&|1_1 1_2 1_3\cdots1_{n-1}0_n\rangle,|1_1 1_2 1_3\cdots 1_{n-1}1_n\rangle\}。
\end{aligned}$$

以 $n=2$ 为例，我们可以有一个最大**纠缠态**（也称为Bell态）：

$$|\Phi_+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle) = (1/\sqrt{2}, 0, 0, 1/\sqrt{2})^T。$$

这个量子态无法分解成两个量子态的直积（满足这种条件的量子态称为可分离态），具有经典概率不能描述的统计性质，在量子信息学中具有极其重要的意义。同样地，当$n=3$时，我们也能找到一类相似的纠缠态：

$$|{\rm GHZ}\rangle = \frac{1}{\sqrt{2}}(|000\rangle + |111\rangle)
=(1/\sqrt{2} , 0 , 0 , 0 , 0 , 0 , 0 , 1/\sqrt{2})^T。$$

&emsp;&emsp;事实上，与直觉相反，纠缠态无处不在，而可分离态却非常少。纠缠态的性质较为复杂，遗憾的是，对多量子比特系统，哪怕是在 $n=2$ 的情形下，我们也没有像单个量子比特那样清晰的几何图像帮助我们理解。我们在下一节也会看到，一些特殊的纠缠态正是量子算法可以加速完成计算任务的关键。值得注意的是，多量子比特的纠缠态是非常复杂的。**当多个量子比特纠缠在一起时，单个量子比特的量子态信息会在某种意义上变得不确定**，所以我们有时需要消除不需要的纠缠。


* > 例子：对纠缠态的测量。如果我们对$|\Phi_+\rangle$第一个量子比特进行$Z$的测量。那么我们有$P_0 = |0_0\rangle\langle0_0|, P_1 = |1_0\rangle\langle 1_0|$, 和$p_0 = 1/2$, $p_1=1/2$。在测量结束后，我们以概率$1/2$得到$0$和$1$,两比特系统相应的最终态为$|00\rangle$ 和 $|11\rangle$，量子纠缠不再存在。这里我们注意到，当测量第一个量子比特得到结果0时，第二个量子比特也必须同时塌缩到$|0\rangle$态。 
"""

"""
### 2.3 密度矩阵

#### 2.3.1 纯态与混合态

一个纯态的量子系统，其量子态可以用态向量表示 $|\psi \rangle$。而几种纯态依照概率组成的量子态称为混合态。例如，假设一个量子系统处于纯态 $|\psi _{1}\rangle |\psi _{2}\rangle$ 的概率都为50%，则这量子系统处于混合态。密度矩阵专门用来表示混合态。任何量子态，不管是纯态，还是混合态，都可以用密度矩阵表示。

混合态与叠加态的概念不同，几种纯态通过量子叠加所组成的叠加态仍旧是纯态。例如，$(|\psi _{1}\rangle +|\psi _{2}\rangle )/\sqrt  {2}$ 是个纯态。
"""

"""
#### 2.3.2 密度矩阵 (density matrix)

假设一个量子系统处于纯态 $|\psi _{1}\rangle、|\psi _{2}\rangle、|\psi _{3}\rangle$、……的概率分别为 $w_{1}、w_{2}、w_{3}$、……，则这混合态量子系统的密度算符 $\rho$ 为

$${\rho }=\sum _{i}w_{i}|\psi _{i}\rangle \langle \psi _{i}|$$

注意到所有概率的总和为1：

$$\sum _{i}w_{i}=1$$
"""



================================================
FILE: docs/source/textbook/chap3.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 第三章 量子线路 (Quantum Circuit)
"""

"""
## 1 逻辑门与电路

&emsp;&emsp;量子计算通过量子电路来实现。量子电路的本质是幺正变换和测量的组合。在物理上，我们无法直接实现过分复杂的幺正变换，所以期望通过一些容易实现的幺正变化来产生更复杂的幺正变换，这些较容易实现的变换则称为量子门。这个过程就类似于通过最基本的逻辑操作（与非门）来搭建大规模的数字电路一样。在本小节，我们将学习最基本的量子门，并简单了解一下如何通过她们来搭建复杂的量子电路。

### 1.1 经典逻辑门与电路

&emsp;&emsp;在了解量子电路之前我们可以先看一下经典计算是如何进行的。在本节中，我们介绍一种直观的经典计算模型，即电路模型。电路模型不只是一种数学模型，它也是真实计算机的实际物理实现方式。一个电路由导线和门组成，它们分别携带信息并执行简单的计算任务。例如，下图显示了一些简单的电路，他们只包含单个逻辑门进行逻辑上的非（NOT），与(AND)，或(OR) ，异或（XOR），NAND（与非）和或非（NOR）操作，称为量子门。输入和输出都只能是0（False）或者1(True)，称为经典比特（简称比特，和量子比特进行区分）。电路可以代表比特在空间中的运动，或者经过一段时间的状态变换。
<div align = center>
<img src="./img/classical_gates.png" width="350" title="bloch" />
<figcaption align = "center"> 经典逻辑门 </figcaption>
</div>

</bra>

> 例: 给出一个稍微复杂点的电路，由两个逻辑门组成，称为半加器（half-adder）。我们可以验证以下输入输出关系：

$$\begin{aligned}
&x = 0, y = 0 \rightarrow c = 0, x\oplus y = 0\\
&x = 0, y = 1 \rightarrow c = 0, x\oplus y = 1\\
\end{aligned}$$

而这正是两个比特相加的结果（包括进位比特）。这个电路称为半加器的原因是，其输入并没有包含进位比特。

<div align = center>
<img src="./img/half_adder.png" width="250" title="bloch" />
<figcaption align = "center"> 半加器电路，其中c是进位符 </figcaption>
</div>

</bra>
&emsp;&emsp;我们可以发现，这里的“电路”是指代数字逻辑电路，与高中所学到的电阻、电源组成的电路有一定的区别。真实的计算机（包括电脑，手机等）通过使用高级语言的编程来实现算法功能。在计算机芯片层面，这些高级语言（Java, C, Python等）写的程序最终被转换（编译）成二进制数据，驱动CPU上一个个的逻辑门，并最终完成算法运算（本质是逻辑运算）。

&emsp;&emsp;更一般地，一个电路可以有很多输入和输出比特。任意一般的逻辑函数可以定义为

$$f:\{0,1\}^n \rightarrow \{0,1\}^l$$

例如，半加器就是一个$\{0,1\}^2$到$\{0,1\}^2$的逻辑函数。

&emsp;&emsp;这里有一个重要的结论: 

&emsp;&emsp;<b>(经典线路)普适性定理：任意逻辑函数 $f$ ，都可以由仅仅包含NAND门的电路来实现。我们称NAND门对经典逻辑计算是普适（universal）的。</b>

&emsp;&emsp; 由于篇幅限制，我们这里不给出相关的证明，有兴趣的读者可以参考任意一本数字逻辑电路的教科书。这个定理的意义在于，我们只需要实现一种类型的逻辑门，就能实现所有的逻辑电路，这将极大地简化理论分析和工程实现。
"""

"""
### 1.2 量子逻辑门与电路

&emsp;&emsp;与经典计算类似，在本节中，我们提供量子电路这种语言来描述量子计算过程的离散单元（量子门）的集合。对量子信息的处理本质上是一种幺正变换（配合额外的测量）。在我们考虑量子电路时，不失一般性，总是将测量放到整个计算过程的最后。这样，我们也可以仅将整个幺正变换看成是量子电路。这样，通用的量子计算过程可以如下图所示：

<div align = center>
<img src="./img/quantum_circuit.png" width="400" />
<figcaption align = "center"> 量子计算过程 </figcaption>
</div>

接下来，我们将具体介绍不同类型的量子门，并演示他们如何组合成一些量子电路。
"""

"""
## 2 量子比特门

### 2.1 单量子比特门

&emsp;&emsp;与经典逻辑门不同，量子电路对单个比特的操作要丰富得多。直觉地理解，经典比特对应 $|0\rangle$ 和 $|1\rangle$ 两个量子态，在Bloch球上分别是北极和南极，唯一能涉及的操作也就是在这两个点之间翻转（NOT门）。与之不同，单量子比特态却对应整个球面，我们可以通过变换连接球面上任意两个点。一些最重要的量子门包括泡利（Pauli）矩阵：

$$X = \left(
  \begin{array}{cc}
    0 & 1  \\
    1 & 0 \\
  \end{array}
\right), \quad 
Y = \left(
  \begin{array}{cc}
    0 & -i  \\
    i & 0 \\
  \end{array}
\right), \quad
Z =  \left(
  \begin{array}{cc}
    1 & 0  \\
    0 & -1 \\
  \end{array}
\right)。\quad$$

要注意$X$，$Y$和$Z$这三个矩阵既是幺正的，又是厄米的。所以我们可以将他们看做量子门，也可以对他们进行测量。

另外三个在量子计算中极为重要的量子门包括:

$$H = \frac{1}{\sqrt{2}}\left(
  \begin{array}{cc}
    1 & 1  \\
    1 & -1 \\
  \end{array}
\right), \quad 
S = \left(
  \begin{array}{cc}
    1 & 0  \\
    0 & i \\
  \end{array}
\right), \quad
T = \left(
  \begin{array}{cc}
    1 & 0  \\
    0 & e^{i\pi/4} \\
  \end{array}
\right)。\quad$$

为了更深入地理解单量子比特门，我们可以用泡利矩阵定义如下三个幺正矩阵：

$$\begin{aligned}
R_x(\theta) &= \cos(\theta/2)I - i\sin(\theta/2) X = \left(
  \begin{array}{cc}
    \cos(\theta/2) & -i\sin(\theta/2)  \\
    -i\sin(\theta/2) & \cos(\theta/2) \\
  \end{array}
\right)，\\
R_y(\theta) &= \cos(\theta/2)I - i\sin(\theta/2) Y =  \left(
  \begin{array}{cc}
    \cos(\theta/2) & -\sin(\theta/2)  \\
    \sin(\theta/2) & \cos(\theta/2) \\
  \end{array}
\right)，\\
R_z(\theta) &= \cos(\theta/2)I - i\sin(\theta/2) Z =  \left(
  \begin{array}{cc}
    e^{-i\theta/2} & 0  \\
    0 & e^{i\theta/2} \\
  \end{array}
\right)。
\end{aligned}$$

这三个矩阵分别称为绕Bloch球上的$x,y,z$轴顺时针旋转$\theta$角。我们以$R_z$为例：

$$R_z(\alpha)(\cos(\theta/2)|0\rangle + \sin(\theta/2)e^{i\varphi}|1\rangle) \cong \cos(\theta/2)|0\rangle + \sin(\theta/2)e^{i(\varphi+\theta)}|1\rangle$$

而这正是量子态在Bloch球上绕着$Z$轴顺时针旋转了$\theta$。$R_x，R_y$的验证要复杂些，我们这边不做展开。除了$x,y,z$轴的旋转外，我们也可以找到绕任意单位向量$\hat{n}=(n_x,n_y,n_z)$旋转的单量子比特门：

$$R_{\hat{n}}(\theta)=\cos(\theta/2)I + i \sin(\theta/2)(n_xX + n_y Y +n_z Z)。$$

事实上，任意单量子比特门都可以看成是绕某根特定轴$\hat{n}$的旋转，满足：

$$U_1 \cong R_{\hat{n}}(\theta)，$$

其中$\theta$，$\hat{n}$完全由$U_1$决定。

如果我们只能进行绕固定轴的旋转，我们仍然可以实现任意单量子比特门的幺正变化。这里我们有以下非常有用的定理：

&emsp;&emsp;定理：任意作用在单量子比特上的幺正操作 $U_1$ ,都可以分解为绕任意2根固定的互相垂直的轴的旋转。以$y$轴和$z$轴为例，我们有：

$$U_1\cong R_z(\beta)R_y(\gamma)R_z(\delta),$$

其中$\beta,\gamma,\delta$由$U_1$决定。


"""

import tensorcircuit as tc
import tensorflow as tf
import math
import numpy as np

tc.set_backend("tensorflow")

X = tc.gates._x_matrix  # same as tc.gates.xgate().tensor.numpy()
Y = tc.gates._y_matrix  # same as tc.gates.ygate().tensor.numpy()
Z = tc.gates._z_matrix  # same as tc.gates.zgate().tensor.numpy()
H = tc.gates._h_matrix  # same as tc.gates.hgate().tensor.numpy()
S = tc.gates._s_matrix
T = tc.gates._t_matrix

print(f"{X=}\n")
print(f"{Y=}\n")
print(f"{Z=}\n")
print(f"{H=}\n")
print(f"{S=}\n")
print(f"{T=}\n")

theta = math.pi / 2

rx = tc.gates.rx_gate(theta).tensor.numpy()
ry = tc.gates.ry_gate(theta).tensor.numpy()
rz = tc.gates.rz_gate(theta).tensor.numpy()

# print(f"{rx=}\n")
# print(f"{ry=}\n")
# print(f"{rz=}\n")

rx_square = rx**2
ry_square = ry**2
rz_square = rz**2

print(f"{rx_square=}\n")
print(f"{ry_square=}\n")
print(f"{rz_square=}\n")
# Output:
#   X=array([[0., 1.],

#          [1., 0.]])

#   

#   Y=array([[ 0.+0.j, -0.-1.j],

#          [ 0.+1.j,  0.+0.j]])

#   

#   Z=array([[ 1.,  0.],

#          [ 0., -1.]])

#   

#   H=array([[ 0.70710678,  0.70710678],

#          [ 0.70710678, -0.70710678]])

#   

#   S=array([[1.+0.j, 0.+0.j],

#          [0.+0.j, 0.+1.j]])

#   

#   T=array([[1.        +0.j        , 0.        +0.j        ],

#          [0.        +0.j        , 0.70710678+0.70710678j]])

#   

#   rx_square=array([[ 0.49999997+0.j, -0.49999997-0.j],

#          [-0.49999997-0.j,  0.49999997+0.j]], dtype=complex64)

#   

#   ry_square=array([[0.49999997+0.j, 0.49999997-0.j],

#          [0.49999997+0.j, 0.49999997+0.j]], dtype=complex64)

#   

#   rz_square=array([[0.-0.99999994j, 0.+0.j        ],

#          [0.+0.j        , 0.+0.99999994j]], dtype=complex64)

#   


"""
### 2.2 两量子比特门

&emsp;&emsp;如果我们只能使用单量子比特门，那么能做的事情是相当有限的，因为每个量子比特都相互独立。为了充分利用量子力学的特性，我们需要考虑作用在多个量子比特上的量子门。本节考虑作用在两个量子比特上的量子门。数学上，两量子比特门只是一个4维的幺正矩阵。这里我们着重介绍一个特别重要的两量子比特门——受控非门 (controlled NOT 或者 $\rm CNOT$)。 

* > 由于幺正变换是一个线性变换，我们只需要考察基向量在$\rm CNOT$的变换，即可完全确定这个幺正变换。这是因为

$$\begin{aligned}
\rm CNOT |\psi\rangle &= \rm CNOT(\alpha|00\rangle+\beta|01\rangle+\gamma|10\rangle+\delta|11\rangle)\\
&=\alpha \rm CNOT |00\rangle+\beta \rm CNOT |01\rangle+\gamma \rm CNOT |10\rangle+\delta \rm CNOT|11\rangle)
\end{aligned}$$

如果我们定义$\rm CNOT$对基向量做了以下变换：

$$\begin{aligned}
&|00\rangle \rightarrow |00\rangle,\\
&|01\rangle \rightarrow |01\rangle,\\
&|10\rangle \rightarrow |11\rangle,\\
&|11\rangle \rightarrow |10\rangle，
\end{aligned}$$

那么$\rm CNOT |\psi\rangle$的状态就完全确定了。

&emsp;&emsp;我们可以发现，当第一个量子比特处在 $|0\rangle$ 时，第二个量子比特的状态不变；如果第一个量子比特处于$|1\rangle$态时，那么第二个量子比特状态发生翻转。这就像是第一个量子比特控制了第二个量子比特，这就是“受控”一词的来源。因此，第一个量子比特也叫控制比特（Control 或者 C），第二个量子比特称为目标比特（Target或者T）。将$\rm CNOT$写成矩阵的形式，我们就有：

$${\rm CNOT} = \left(
  \begin{array}{cccc}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0 \\
  \end{array}
\right)，$$

其中第1-4列分别对应$|00\rangle， |01\rangle，|10\rangle， |11\rangle$。通常，我们用以下符号来表示$\rm CNOT$：

<div align = center>
<img src="./img/CNOT.png" width="150" />
<figcaption align = "center"> CNOT门 </figcaption>
</div>

其中，$\bullet$ 所在的横线代表控制量子比特，$\oplus$ 所在的横线代表目标量子比特。

&emsp;&emsp;$\rm CNOT$门一个重要的作用就是制备纠缠态。考虑三个量子比特初始制备于$|000\rangle$态（可以通过分别对三个量子比特进行$Z$的测量来完成），那么我们就能通过下图所示的电路来制备3比特的GHZ态：
<div align = center>
<img src="./img/3GHZ.png" width="200" />
<figcaption align = "center"> GHZ态的制备电路</figcaption>
</div>

* >  **GHZ态的制备：** 经过第一个H门后，量子态变为

$$|\psi\rangle_1=1/\sqrt{2}(|0\rangle+|1\rangle)|00\rangle=1/\sqrt{2}(|0\rangle|00\rangle+|1\rangle|00\rangle)，$$

经过第一个${\rm CNOT}$门后，量子态变成

$$|\psi\rangle_2=1/\sqrt{2}(|0\rangle|00\rangle+|1\rangle|10\rangle)=1/\sqrt{2}(|00\rangle+|11\rangle)|0\rangle，$$

经过第二个${\rm CNOT}$门后，量子态变成

$$|\psi\rangle_3=1/\sqrt{2}(|000\rangle+|111\rangle)。$$

"""

print(tc.gates._cnot_matrix)

# GHZ 态模拟

circuit = tc.Circuit(3)
circuit.H(0)
circuit.CNOT(0, 1)  # 第一个参数表示第一个要作用的线路，第二个参数表示第二个要作用的线路
circuit.CNOT(0, 2)

zeros, ones = 0, 0
# 模拟100次
for _ in range(200):
    res = circuit.measure(0, 1, 2, with_prob=False)
    if res[0].numpy().sum() == 0:  # |000>
        zeros += 1
    elif res[0].numpy().sum() == 3:  # |111>
        ones += 1

print(f"{zeros=}, {ones=}")
# Output:
#   [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]

#    [0. 0. 1. 0.]]

#   zeros=102, ones=98


"""
## 3 量子门的普适性（universality）

&emsp;&emsp;正如经典逻辑电路都可以用NAND门来构造一样，对量子电路，我们也希望可以找到一个固定的量子门的集合，并通过这个集合中的量子门来构造所有的量子电路。但是注意，这里的挑战在于，幺正变换是连续变换，而给定的一个量子门集合，有限个量子门所能实现的幺正变换数量是有限的。

&emsp;&emsp;但实际上，我们仍然可以用有限的量子门“足够好”地近似任意幺正变换。这里我们不加证明地给出以下重要定理：

&emsp;&emsp;<b>量子普适性定理：任意作用在$n$个量子比特上的幺正变换$U_n$，都可以拆分成包含 $m=O(4^n)$个单量子比特门和CNOT门电路。更进一步，我们可以仅通过$H$,$S$,$T$和 $\rm CNOT$这组有限个量子门所组成的电路$\tilde{U}_n$来近似这$m$个量子门组成的电路$U_n$，所需要$H$,$S$,$T$和 $\rm CNOT$量子门的总数为$O(m\log(m/\epsilon))$，其中$\epsilon$为$U_n$与$\tilde{U}_n$间的误差。{$H$,$S$,$T$和 $\rm CNOT$}称为普适门集（universal gate set）</b>

这样，我们可以通过增加少量的量子门，使量子电路产生的幺正变化和目标幺正变换之间的差距指数地减少！


* > 例子：${\rm Toffoli}$ 门。这种门可以看成是${\rm CNOT}$门的一种拓展——它有两个控制量子比特和一个目标量子比特。只有当两个控制量子比特处于$|11\rangle$态时，目标量子比特才会翻转。它的电路符号如下图所示，其矩阵表示是：

$${\rm Toffoli} = \left(
  \begin{array}{cccccccc}
    1 & 0 & 0 & 0  & 0 & 0& 0&0 \\
    0 & 1 & 0 & 0 & 0 & 0& 0&0\\
    0 & 0 & 1& 0 & 0 & 0& 0&0\\
    0 & 0 & 0 & 1  & 0 & 0& 0&0\\
     0 & 0 & 0 & 0  & 1 & 0& 0&0\\
      0 & 0 & 0 & 0  & 0 & 1& 0&0\\
       0 & 0 & 0 & 0  & 0 & 0& 0&1\\
        0 & 0 & 0 & 0  & 0 & 0& 1&0\\
  \end{array}
\right)$$

<div align = center>
<img src="./img/Toffoli_circuit.png" width="150" />
<figcaption align = "center"> Toffoli门 </figcaption>
</div>



* > ${\rm Toffoli}$ 门的分解：正如普适性定律所陈述的，我们可以通过$H$，$T$，$S$和${\rm CNOT}$门来构造${\rm Toffoli}$门，具体的电路图如下图所示，其中

$$T^\dagger = \left(
  \begin{array}{cc}
    1 & 0  \\
    0 & e^{-i\pi/4} \\
  \end{array}
\right) = S^3T。$$

称为$T$的共轭。注意，这里对${\rm Toffoli}$的构造是精确无误差的。
<div align = center>
<img src="./img/Toffoli.png" width="450" />
<figcaption align = "center"> Toffoli门的分解 </figcaption>
</div>
"""

print(tc.gates._toffoli_matrix)  # Toffli门对应的矩阵
# Output:
#   [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]]


# toffoli 门分解校验

c = tc.Circuit(3)
c.H(2)
c.CNOT(1, 2)
c.TD(2)
c.CNOT(0, 2)
c.T(2)
c.CNOT(1, 2)
c.TD(2)
c.CNOT(0, 2)
c.TD(1)
c.T(2)
c.CNOT(0, 1)
c.H(2)
c.TD(1)
c.CNOT(0, 1)
c.T(0)
c.S(1)

np.testing.assert_allclose(c.matrix(), tc.gates._toffoli_matrix, atol=1e-6)

"""
## 4 量子线路

### 4.1 经典可逆线路

一般来说，大部分的经典的逻辑门是不可逆。比如**与**门（AND），当输出是0时我们无法得知输入是00、01或者10；但最简单就如**非**门就是可逆的，可以通过输出值取反得到输入值。

但想要构造更复杂的电路，还是要通过结构上的构造，使用更多的门来构造出可逆的线路的，这时线路的输入输出有且仅有唯一确定的值。
"""

"""
### 4.2 可逆计算 （reversible computation）

&emsp;&emsp;如果一个逻辑门存在逆操作，即给定输出，我们可以确定输入状态，那我们称其为可逆的。我们可能已经发现，经典逻辑门和量子逻辑门的一个区别在于，经典逻辑门有时是不可逆的（例如AND门的输出0对应00，10，01三个输入状态，我们无法判断哪一个是真正的输入态），而量子逻辑门必须是可逆的（因为幺正变换是可逆的，且有 $U^{-1}=U^\dagger$ ）。但是如果稍微小心一点，在有辅助比特的帮助下，我们可以将经典逻辑门转换成可逆的。通过构造可逆电路，我们就有了从经典计算到量子计算的桥梁。


&emsp;&emsp;我们先看一个经典逻辑门，Fredkin门。这个逻辑门的真值表如下图所示。通过观察，我们可以发现，当 $c=1$ 时，Fredkin门对$a$和$b$进行了置换（SWAP）。从真值表可以看出，Fredkin门的输入和输出是一一对应的，所以它是可逆的（它的逆运算就是自己）。
<div align = center>
<img src="./img/Fredkin.png" width="350"  />
<figcaption align = "center"> Fredkin 门 </figcaption>
</div>

&emsp;&emsp;对Fredkin的一个重要的应用是构造可逆电路。我们以AND， NOT和SWAP门来说明，它们都可以用可逆的Fredkin门来构造。

* 当$a=0，b=y，c=x$时， 我们可以从真值表中提取：

$$\begin{aligned}
x = 0, y = 0 \rightarrow a'= 0, b' = 0\\
x = 0, y = 1 \rightarrow a'= 0, b' = 1 \\
x = 1, y = 0 \rightarrow a'= 0, b' = 0\\
x = 1, y = 1 \rightarrow a'= 1, b' = 0\\
\end{aligned}$$

我们就能发现$a' = xy$和$b'=\bar{x}y$($\bar{(\cdot)}$代表取反)。这样我们就通过一个辅助比特实现了AND门。利用类似的想法，我们也能构造NOT门和SWAP门，如下图所示。有了AND和NOT，我们就能构造NAND门。由经典普适性定理，所有的经典计算都可以是可逆的。

<div align = center>
<img src="./img/reversible_gates.png" width="400"  />
<figcaption align = "center"> Fredkin 门构造可逆的</figcaption>
</div>

&emsp;&emsp;如果我们构造Fredkin的幺正变换，那我们就能对任意量子态做可逆的经典计算。事实上，$\rm Toffoli$门也对任意经典计算构造可逆电路。值得注意的是，可逆计算通常需要很多辅助比特（通常正比于所考虑电路的门的数量）。这在经典计算中或许不是问题，但是对量子计算却是致命的。

* > 例：假设三个量子比特初始态为$\frac{1}{\sqrt{2}}|0\rangle|y\rangle\sum_{x\in \{0,1\}}|x\rangle$，那么经过量子Fredkin门之后，状态变为

$$\frac{1}{\sqrt{2}}\sum_{x\in \{0,1\}}|xy\rangle|\bar{x}y\rangle|x\rangle$$

如果只看量子比特1， 我们发现$y\cdot 0$和 $y\cdot 1$ 处于“叠加”的状态，似乎同时进行了两次AND运算。但是注意，第1个量子比特和第2，3个量子纠缠在一起的，会引起量子干涉，这时我们无法获取关于量子比特1的任何信息。

&emsp;&emsp;事实上我们可以完全消除这些辅助量子比特的影响，但是这需要额外的电路设计。在此不做展开，有兴趣的读者可以参考：https://qiskit.org/textbook/ch-gates/oracles.html


"""

"""
### 4.3 量子可逆线路

了解到经典计算线路都可以改造成可逆计算线路，逻辑门由矩阵表示后。结合量子线路所有的量子门都是幺正可逆的，我们就知道，量子线路能够实现的计算模型是经典计算的一个超集。


事实上，我们相信对于量子线路多项式时间可以解决的问题 BQP 集合是严格大于经典线路多项式时间可以解决的问题 P 集合的，也即量子计算具有本质上超越经典计算的优势。
"""

# 量子线路可逆验证

c = tc.Circuit(3)
c.H(0)
c.cnot(0, 1)
c.T(2)
c.rz(1, theta=0.6)
c.toffoli(1, 0, 2)
s = c.state()

c2 = tc.Circuit(3, inputs=s)
c2.toffoli(1, 0, 2)
c2.rz(1, theta=-0.6)
c2.TD(2)
c2.cnot(0, 1)
c2.H(0)
print(c2.state())
# Output:
#   tf.Tensor(

#   [1.+0.0000000e+00j 0.+0.0000000e+00j 0.+0.0000000e+00j 0.+0.0000000e+00j

#    0.-2.1073424e-08j 0.+0.0000000e+00j 0.+0.0000000e+00j 0.+0.0000000e+00j], shape=(8,), dtype=complex64)


"""
## 5 总结

&emsp;&emsp;本节我们学习了重要的电路概念。相对于离散的经典逻辑门，量子逻辑门是可以连续变化，且可逆的。我们发现，量子逻辑门包含经典逻辑门。在初态为直积态时，原则上量子计算可以完成所有经典计算的任务（由于可逆性的要求，效率会有所下降），毕竟世界的本质还是量子的。同时，与经典电路一样，对量子电路我们也有普适性定理，即任意幺正变换所对应量子电路都可以用普适门集合中的量子门组成的电路来很好地近似。所以在接下来的算法讨论中，我们也使用局限于普适门集合中的量子门。
"""



================================================
FILE: docs/source/tutorials/barren_plateaus.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Barren Plateaus
"""

"""
## Overview
"""

"""
Barren plateaus are the greatest difficulties in the gradient-based optimization for a large family of random parameterized quantum circuits (PQC). The gradients vanish almost everywhere. In this example, we will show barren plateaus in quantum neural networks (QNNs).
"""

"""
## Setup
"""

import numpy as np
import tensorflow as tf
import tensorcircuit as tc

tc.set_backend("tensorflow")
tc.set_dtype("complex64")

Rx = tc.gates.rx
Ry = tc.gates.ry
Rz = tc.gates.rz

"""
## Parameters
"""

n = 4  # The number of qubits
nlayers = 1  # The number of circuit layers
ncircuits = 3  # The number of circuits with different initial parameters
ntrials = 2  # The number of random circuits with different structures

"""
## Generating QNN
"""

def op_expectation(params, seed, n, nlayers):
    paramsc = tc.backend.cast(params, dtype="float32")  # parameters of gates
    seedc = tc.backend.cast(seed, dtype="float32")  # parameters of circuit structure

    c = tc.Circuit(n)
    for i in range(n):
        c.ry(i, theta=np.pi / 4)
    for l in range(nlayers):
        for i in range(n):
            # choose one gate from Rx, Ry, and Rz gates with equal prob=1/3; status is the seed.
            c.unitary_kraus(
                [Rx(paramsc[i, l]), Ry(paramsc[i, l]), Rz(paramsc[i, l])],
                i,
                prob=[1 / 3, 1 / 3, 1 / 3],
                status=seedc[i, l],
            )
        for i in range(n - 1):
            c.cz(i, i + 1)

    return tc.backend.real(
        c.expectation((tc.gates.z(), [0]), (tc.gates.z(), [1]))
    )  # expectations of <Z_0Z_1>

# use vmap and vvag to get the expectations of ZZ observable and gradients of different random circuit instances
op_expectation_vmap_vvag = tc.backend.jit(
    tc.backend.vmap(
        tc.backend.vvag(op_expectation, argnums=0, vectorized_argnums=0),
        vectorized_argnums=1,
    )
)

"""
## Batch Variance Computation
"""

seed = tc.array_to_tensor(
    np.random.uniform(low=0.0, high=1.0, size=[ntrials, n, nlayers]), dtype="float32"
)
params = tc.array_to_tensor(
    np.random.uniform(low=0.0, high=2 * np.pi, size=[ncircuits, n, nlayers]),
    dtype="float32",
)

e, grad = op_expectation_vmap_vvag(
    params, seed, n, nlayers
)  # the expectations of ZZ observable and gradients of different random circuits

grad_var = tf.math.reduce_std(tf.math.reduce_std(grad, axis=0), axis=0)[
    0, 0
]  # the gradient variance of the first parameter
print("The variance of the gradients is:", grad_var.numpy())
# Output:
#   The variance of the gradients is: 0.19805922


"""
## Results
"""

"""
The gradient variances in QNNs ($nlayers=50$, $ntrials=20$, $ncircuits=20$). The landscape become exponentially barren with increasing qubit number. 
"""

"""
![barren.jpg](attachment:028e2e26-1f06-452d-93f9-62f73c77e0f8.jpg)
"""



================================================
FILE: docs/source/tutorials/barren_plateaus_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 贫瘠高原
"""

"""
## 概述
"""

"""
贫瘠高原是一大类随机参数化量子电路（PQC）的基于梯度的优化中最大的困难。梯度消失几乎无处不在。 在此示例中，我们将展示量子神经网络 (QNN) 中的贫瘠高原。
"""

"""
## 设置
"""

import numpy as np
import tensorflow as tf
import tensorcircuit as tc

tc.set_backend("tensorflow")
tc.set_dtype("complex64")

Rx = tc.gates.rx
Ry = tc.gates.ry
Rz = tc.gates.rz

"""
## 参数
"""

n = 4  # 量子比特的数量
nlayers = 1  # 电路的层数
ncircuits = 3  # 有不同初始参数的电路数量
ntrials = 2  # 有不同结构的随机电路数量

"""
## 产生 QNN
"""

def op_expectation(params, seed, n, nlayers):
    paramsc = tc.backend.cast(params, dtype="float32")  # 门的参数
    seedc = tc.backend.cast(seed, dtype="float32")  # 电路结构的参数

    c = tc.Circuit(n)
    for i in range(n):
        c.ry(i, theta=np.pi / 4)
    for l in range(nlayers):
        for i in range(n):
            # 从 Rx、Ry 和 Rz 门中选择一个具有相等概率 = 1/3 的门； 状态是随机数种子。
            c.unitary_kraus(
                [Rx(paramsc[i, l]), Ry(paramsc[i, l]), Rz(paramsc[i, l])],
                i,
                prob=[1 / 3, 1 / 3, 1 / 3],
                status=seedc[i, l],
            )
        for i in range(n - 1):
            c.cz(i, i + 1)

    return tc.backend.real(
        c.expectation((tc.gates.z(), [0]), (tc.gates.z(), [1]))
    )  # <Z_0Z_1> 的期望

# 使用 vmap 和 vvag 获得 ZZ 可观察的期望值和不同随机电路实例的梯度
op_expectation_vmap_vvag = tc.backend.jit(
    tc.backend.vmap(
        tc.backend.vvag(op_expectation, argnums=0, vectorized_argnums=0),
        vectorized_argnums=1,
    )
)

"""
## 批量方差计算
"""

seed = tc.array_to_tensor(
    np.random.uniform(low=0.0, high=1.0, size=[ntrials, n, nlayers]), dtype="float32"
)
params = tc.array_to_tensor(
    np.random.uniform(low=0.0, high=2 * np.pi, size=[ncircuits, n, nlayers]),
    dtype="float32",
)

e, grad = op_expectation_vmap_vvag(
    params, seed, n, nlayers
)  # 不同随机电路的 ZZ 可观测量和梯度的期望

grad_var = tf.math.reduce_std(tf.math.reduce_std(grad, axis=0), axis=0)[
    0, 0
]  # 第一个参数的梯度方差
print("The variance of the gradients is:", grad_var.numpy())
# Output:
#   The variance of the gradients is: 0.19805922


"""
## 结果
"""

"""
QNN 中的梯度方差（$nlayers=50$，$ntrials=20$，$ncircuits=20$）。 随着量子比特数量的增加，能量曲面变得指数级贫瘠。
"""

"""
![barren.jpg](attachment:028e2e26-1f06-452d-93f9-62f73c77e0f8.jpg)
"""



================================================
FILE: docs/source/tutorials/circuit_basics.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Circuit Basics
"""

"""
## Overview

In this note, we will learn about basic operations of the core object in TensorCircuit - ``tc.Circuit`` which supports both noiseless simulation and noisy simulation with the Monte Carlo trajectory-based method. More importantly, near all the operations on the Circuit object is differentiable and jittable, which is the key for successful and efficient variational quantum algorithm simulations.

[WIP note]
"""

"""
## Setup
"""

from functools import partial
import inspect
import sys
import numpy as np
import tensorflow as tf

import tensorcircuit as tc

"""
## Hello world example
"""

def get_circuit(n):
    c = tc.Circuit(n)  # initialize a circuit object with n qubits
    for i in range(n):
        c.H(i)  # apply Hadamard gate on each qubit
    c.cnot(0, 1)  # apply cnot with control qubit on 0-th qubit
    c.CNOT(n - 1, n - 2)  # capitalized API also works
    return c

# print possible gates without parameters
print(tc.Circuit.sgates)
# Output:
#   ['i', 'x', 'y', 'z', 'h', 't', 's', 'td', 'sd', 'wroot', 'cnot', 'cz', 'swap', 'cy', 'iswap', 'ox', 'oy', 'oz', 'toffoli', 'fredkin']


# the corresponding matrix for these gates definition
for g in tc.Circuit.sgates:
    gf = getattr(tc.gates, g)
    print(g)
    print(tc.gates.matrix_for_gate(gf()))
# Output:
#   i

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j]]

#   x

#   [[0.+0.j 1.+0.j]

#    [1.+0.j 0.+0.j]]

#   y

#   [[0.+0.j 0.-1.j]

#    [0.+1.j 0.+0.j]]

#   z

#   [[ 1.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j]]

#   h

#   [[ 0.70710677+0.j  0.70710677+0.j]

#    [ 0.70710677+0.j -0.70710677+0.j]]

#   t

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710677+0.70710677j]]

#   s

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   td

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710677-0.70710677j]]

#   sd

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.-1.j]]

#   wroot

#   [[ 0.70710677+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710677+0.j ]]

#   cnot

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   cz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -1.+0.j]]

#   swap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   cy

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.-1.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]]

#   iswap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]

#    [0.+0.j 0.+1.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   ox

#   [[0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oy

#   [[0.+0.j 0.-1.j 0.+0.j 0.+0.j]

#    [0.+1.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]]

#   toffoli

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   fredkin

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]]


c = get_circuit(3)
ir = c.to_qir()  # intermediate representation of the circuit
ir
# Output:
#   [{'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#         edges: [

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-1'[0] )

#         ]),

#     'index': (0,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#         edges: [

#             Edge('cnot'[3] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-2'[0] )

#         ]),

#     'index': (1,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#         edges: [

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-3'[0] )

#         ]),

#     'index': (2,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]], dtype=complex64),

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge('cnot'[3] -> 'cnot'[1] ),

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('cnot'[3] -> 'h'[0] )

#         ]),

#     'index': (0, 1),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]], dtype=complex64),

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge(Dangling Edge)[1],

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('cnot'[3] -> 'cnot'[1] )

#         ]),

#     'index': (2, 1),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False}]

ir[0]["gatef"]().tensor, ir[-1]["gate"].tensor  # the actually unitary for each gate
# Output:
#   (array([[ 0.70710677+0.j,  0.70710677+0.j],

#           [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#    array([[[[1.+0.j, 0.+0.j],

#             [0.+0.j, 0.+0.j]],

#    

#            [[0.+0.j, 1.+0.j],

#             [0.+0.j, 0.+0.j]]],

#    

#    

#           [[[0.+0.j, 0.+0.j],

#             [0.+0.j, 1.+0.j]],

#    

#            [[0.+0.j, 0.+0.j],

#             [1.+0.j, 0.+0.j]]]], dtype=complex64))

# compute the final output quantum state from the circuit
c.state()
# Output:
#   array([0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j,

#          0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j],

#         dtype=complex64)

# compute some expectation values, say <X1>
x1 = c.expectation([tc.gates.x(), [1]])

# or <Z1Z2>
z1z2 = c.expectation([tc.gates.z(), [1]], [tc.gates.z(), [2]])

print(x1, z1z2)
# Output:
#   (0.9999998+0j) 0j


# make some random samples
for _ in range(10):
    print(c.perfect_sampling())
# Output:
#   (array([0., 0., 0.], dtype=float32), 0.12499997764825821)

#   (array([1., 1., 0.], dtype=float32), 0.1249999776482098)

#   (array([1., 1., 0.], dtype=float32), 0.1249999776482098)

#   (array([0., 1., 0.], dtype=float32), 0.12499997764825821)

#   (array([1., 0., 0.], dtype=float32), 0.12499997019766829)

#   (array([0., 0., 1.], dtype=float32), 0.12499997764825821)

#   (array([1., 1., 1.], dtype=float32), 0.1250001713634208)

#   (array([1., 0., 0.], dtype=float32), 0.12499997019766829)

#   (array([0., 1., 1.], dtype=float32), 0.12499997764825821)

#   (array([1., 0., 1.], dtype=float32), 0.12499997019766829)


# we can easily switch simulation backends away from NumPy!

with tc.runtime_backend("tensorflow") as K:
    c = get_circuit(3)
    print(c.state())

with tc.runtime_backend("jax") as K:
    c = get_circuit(3)
    print(c.state())

with tc.runtime_backend("pytorch") as K:
    # best performance and full functionality are not guaranteed on pytorch backend
    c = get_circuit(3)
    print(c.state())
# Output:
#   tf.Tensor(

#   [0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j

#    0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j], shape=(8,), dtype=complex64)

#   WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

#   [0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j

#    0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j]

#   tensor([0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,

#           0.3536+0.j])


"""
## Parameterized Quantum Circuit (PQC)
"""

# circuit gates that accept parameters

print(tc.Circuit.vgates)
# Output:
#   ['r', 'cr', 'rx', 'ry', 'rz', 'crx', 'cry', 'crz', 'orx', 'ory', 'orz', 'any', 'exp', 'exp1']


# see the keyword parameters (with type float) for each type of variable gate
for g in tc.Circuit.vgates:
    print(g, inspect.signature(getattr(tc.gates, g).f))
# Output:
#   r (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   cr (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   rx (theta: float = 0) -> tensorcircuit.gates.Gate

#   ry (theta: float = 0) -> tensorcircuit.gates.Gate

#   rz (theta: float = 0) -> tensorcircuit.gates.Gate

#   crx (*args: Any, **kws: Any) -> Any

#   cry (*args: Any, **kws: Any) -> Any

#   crz (*args: Any, **kws: Any) -> Any

#   orx (*args: Any, **kws: Any) -> Any

#   ory (*args: Any, **kws: Any) -> Any

#   orz (*args: Any, **kws: Any) -> Any

#   any (unitary: Any, name: str = 'any') -> tensorcircuit.gates.Gate

#   exp (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate

#   exp1 (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate


def get_circuit(n, params):
    c = tc.Circuit(n)  # initialize a circuit object with n qubits
    for i in range(n):
        c.rx(i, theta=params[i])  # apply rx gate
    c.cnot(0, 1)
    return c

K = tc.set_backend("tensorflow")

n = 3
params = K.ones([n])
c = get_circuit(n, params)
print(c.state())
# Output:
#   tf.Tensor(

#   [ 0.6758712 +0.j          0.        -0.36923012j  0.        -0.36923015j

#    -0.20171136-0.j         -0.20171136+0.j          0.        +0.11019541j

#     0.        -0.36923015j -0.20171136-0.j        ], shape=(8,), dtype=complex64)


ir = c.to_qir()
ir
# Output:
#   [{'gatef': rx,

#     'index': (0,),

#     'name': 'rx',

#     'split': None,

#     'mpo': False,

#     'parameters': {'theta': <tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>},

#     'gate': Gate(

#         name: '__unnamed_node__',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#             array([[0.87758255+0.j        , 0.        -0.47942555j],

#                    [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>,

#         edges: [

#             Edge('cnot'[2] -> '__unnamed_node__'[0] ),

#             Edge('__unnamed_node__'[1] -> 'qb-1'[0] )

#         ])},

#    {'gatef': rx,

#     'index': (1,),

#     'name': 'rx',

#     'split': None,

#     'mpo': False,

#     'parameters': {'theta': <tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>},

#     'gate': Gate(

#         name: '__unnamed_node__',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#             array([[0.87758255+0.j        , 0.        -0.47942555j],

#                    [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>,

#         edges: [

#             Edge('cnot'[3] -> '__unnamed_node__'[0] ),

#             Edge('__unnamed_node__'[1] -> 'qb-2'[0] )

#         ])},

#    {'gatef': rx,

#     'index': (2,),

#     'name': 'rx',

#     'split': None,

#     'mpo': False,

#     'parameters': {'theta': <tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>},

#     'gate': Gate(

#         name: '__unnamed_node__',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#             array([[0.87758255+0.j        , 0.        -0.47942555j],

#                    [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>,

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge('__unnamed_node__'[1] -> 'qb-3'[0] )

#         ])},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             <tf.Tensor: shape=(2, 2, 2, 2), dtype=complex64, numpy=

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]], dtype=complex64)>,

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge(Dangling Edge)[1],

#             Edge('cnot'[2] -> '__unnamed_node__'[0] ),

#             Edge('cnot'[3] -> '__unnamed_node__'[0] )

#         ]),

#     'index': (0, 1),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False}]

# see the gate unitary
ir[0]["gatef"](**ir[0]["parameters"]).tensor
# Output:
#   <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#   array([[0.87758255+0.j        , 0.        -0.47942555j],

#          [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>

# let us compose a differentiable quantum function


def energy(params):
    c = get_circuit(n, params)
    return K.real(c.expectation([tc.gates.z(), [1]]))


energy_vag = K.value_and_grad(energy)

print(energy_vag(params))

# once we have the gradient, we can run gradient-based descent for variational optimization
# Output:
#   (<tf.Tensor: shape=(), dtype=float32, numpy=0.2919265>, <tf.Tensor: shape=(3,), dtype=complex64, numpy=

#   array([-4.5464873e-01+0.j, -4.5464873e-01+0.j,  2.2351742e-08+0.j],

#         dtype=complex64)>)


# and jit it for acceleration!

energy_vag_jit = K.jit(K.value_and_grad(energy))

print(energy_vag_jit(params))
# the first time to run a jitted function will be slow, but the following evaluation will be super fast
# Output:
#   (<tf.Tensor: shape=(), dtype=float32, numpy=0.2919265>, <tf.Tensor: shape=(3,), dtype=complex64, numpy=

#   array([-4.5464873e-01+0.j, -4.5464873e-01+0.j,  2.2351742e-08+0.j],

#         dtype=complex64)>)


"""
## Advances for Circuit
"""

"""
### Input State

We can replace the input state from the default |0^n>
"""

input_state = K.ones([2**n])
input_state /= K.norm(input_state)

c = tc.Circuit(n, inputs=input_state)
c.H(0)
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([0.49999997+0.j, 0.49999997+0.j, 0.49999997+0.j, 0.49999997+0.j,

#          0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j],

#         dtype=complex64)>

"""
### Monte Carlo Noise Simulation

``tc.Circuit`` supports noisy simulation using the Monte Carlo method, and it is also jittable! Besides, ``tc.DMCircuit``  supports noisy simulation using the full density matrix method.
"""

c = tc.Circuit(n)
for i in range(n):
    c.H(i)
for i in range(n - 1):
    c.cnot(i, i + 1)
    c.depolarizing(i, px=0.1, py=0.1, pz=0.1)
    c.apply_general_kraus(tc.channels.phasedampingchannel(gamma=0.2), i + 1)
print(c.expectation([tc.gates.y(), [1]]))
# Output:
#   tf.Tensor(0j, shape=(), dtype=complex64)


"""
### Apply Arbitrary Gate

Just directly using ``any`` API by feeding the corresponding unitary
"""

c = tc.Circuit(n)
c.any(0, 1, unitary=K.ones([4, 4]) / K.norm(K.ones([4, 4])))
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([0.25+0.j, 0.  +0.j, 0.25+0.j, 0.  +0.j, 0.25+0.j, 0.  +0.j,

#          0.25+0.j, 0.  +0.j], dtype=complex64)>

"""
### Exponential Gate

If we want to simulate gate as $e^{i\theta G}$ where $G^2=1$ is a matrix, we have a fast and efficient implementation for such gates as
`exp1`
"""

c = tc.Circuit(n)
for i in range(n):
    c.H(i)
for i in range(n - 1):
    c.exp1(i, i + 1, theta=K.ones([]), unitary=tc.gates._zz_matrix)
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([-0.14713009-3.2148516e-01j,  0.35355335+1.4901161e-08j,

#          -0.14713009+3.2148516e-01j,  0.35355335-1.4901161e-08j,

#           0.35355335-1.4901161e-08j, -0.14713009+3.2148516e-01j,

#           0.35355335+1.4901161e-08j, -0.14713009-3.2148516e-01j],

#         dtype=complex64)>

"""
In the above example $G=Z\otimes Z$
"""

print(tc.gates._zz_matrix)
# Output:
#   [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
Common matrices in gates modules are listed below
"""

for name in dir(tc.gates):
    if name.endswith("_matrix"):
        print(name, ":\n", getattr(tc.gates, name))
# Output:
#   _cnot_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]

#    [0. 0. 1. 0.]]

#   _cy_matrix :

#    [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -0.-1.j]

#    [ 0.+0.j  0.+0.j  0.+1.j  0.+0.j]]

#   _cz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0.  1.  0.  0.]

#    [ 0.  0.  1.  0.]

#    [ 0.  0.  0. -1.]]

#   _fredkin_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]]

#   _h_matrix :

#    [[ 0.70710678  0.70710678]

#    [ 0.70710678 -0.70710678]]

#   _i_matrix :

#    [[1. 0.]

#    [0. 1.]]

#   _ii_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 0. 0. 1.]]

#   _s_matrix :

#    [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   _swap_matrix :

#    [[1. 0. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]]

#   _t_matrix :

#    [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710678+0.70710678j]]

#   _toffoli_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]]

#   _wroot_matrix :

#    [[ 0.70710678+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710678+0.j ]]

#   _x_matrix :

#    [[0. 1.]

#    [1. 0.]]

#   _xx_matrix :

#    [[0. 0. 0. 1.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [1. 0. 0. 0.]]

#   _y_matrix :

#    [[ 0.+0.j -0.-1.j]

#    [ 0.+1.j  0.+0.j]]

#   _yy_matrix :

#    [[ 0.+0.j  0.-0.j  0.-0.j -1.+0.j]

#    [ 0.+0.j  0.+0.j  1.-0.j  0.-0.j]

#    [ 0.+0.j  1.-0.j  0.+0.j  0.-0.j]

#    [-1.+0.j  0.+0.j  0.+0.j  0.+0.j]]

#   _z_matrix :

#    [[ 1.  0.]

#    [ 0. -1.]]

#   _zz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
### Non-unitary Gate

``tc.Circuit`` automatically support non-unitary circuit simulation due to its TensorNetwork engine nature
"""

c = tc.Circuit(n)
c.exp1(1, unitary=tc.gates._x_matrix, theta=K.ones([]) + 1.0j * K.ones([]))
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([0.83373   -0.9888977j, 0.        +0.j       ,

#          0.63496387-1.2984576j, 0.        +0.j       ,

#          0.        +0.j       , 0.        +0.j       ,

#          0.        +0.j       , 0.        +0.j       ], dtype=complex64)>

"""
Note in this case the final state is not normalized anymore
"""

try:
    np.testing.assert_allclose(K.norm(c.state()), 1.0)
except AssertionError as e:
    print(e)
# Output:
#   

#   Not equal to tolerance rtol=1e-07, atol=0

#   

#   Mismatched elements: 1 / 1 (100%)

#   Max absolute difference: 0.93963802

#   Max relative difference: 0.93963802

#    x: array(1.939638+0.j, dtype=complex64)

#    y: array(1.)




================================================
FILE: docs/source/tutorials/circuit_basics_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 电路基础
"""

"""
## 概述

在这篇笔记中，我们将了解 TensorCircuit 中核心对象的基本操作-``tc.Circuit``，它支持无噪声仿真和基于蒙特卡洛轨迹的噪声仿真。更重要的是，几乎所有对 Circuit 对象的操作都是可微分的和可即时编译的，这是成功高效地进行变分量子算法模拟的关键。
[WIP note]
"""

"""
## 设置
"""

from functools import partial
import inspect
import sys
import numpy as np
import tensorflow as tf

import tensorcircuit as tc

"""
## “Hello World" 样例
"""

def get_circuit(n):
    c = tc.Circuit(n)  # 用 n 个量子比特初始化一个电路对象
    for i in range(n):
        c.H(i)  # 在每个量子比特上使用 Hadamard 门
    c.cnot(0, 1)  # 在第 0 个量子比特上应用带有控制量子比特的 cnot 门
    c.CNOT(n - 1, n - 2)  # 大写的 API 也可以正常使用
    return c

# 打印不含参数的有可能的门
print(tc.Circuit.sgates)
# Output:
#   ['i', 'x', 'y', 'z', 'h', 't', 's', 'td', 'sd', 'wroot', 'cnot', 'cz', 'swap', 'cy', 'iswap', 'ox', 'oy', 'oz', 'toffoli', 'fredkin']


# 这些门定义的相应矩阵
for g in tc.Circuit.sgates:
    gf = getattr(tc.gates, g)
    print(g)
    print(tc.gates.matrix_for_gate(gf()))
# Output:
#   i

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j]]

#   x

#   [[0.+0.j 1.+0.j]

#    [1.+0.j 0.+0.j]]

#   y

#   [[0.+0.j 0.-1.j]

#    [0.+1.j 0.+0.j]]

#   z

#   [[ 1.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j]]

#   h

#   [[ 0.70710677+0.j  0.70710677+0.j]

#    [ 0.70710677+0.j -0.70710677+0.j]]

#   t

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710677+0.70710677j]]

#   s

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   td

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710677-0.70710677j]]

#   sd

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.-1.j]]

#   wroot

#   [[ 0.70710677+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710677+0.j ]]

#   cnot

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   cz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -1.+0.j]]

#   swap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   cy

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.-1.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]]

#   iswap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]

#    [0.+0.j 0.+1.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   ox

#   [[0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oy

#   [[0.+0.j 0.-1.j 0.+0.j 0.+0.j]

#    [0.+1.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]]

#   toffoli

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   fredkin

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]]


c = get_circuit(3)
ir = c.to_qir()  # 电路的中间表示
ir
# Output:
#   [{'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#         edges: [

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-1'[0] )

#         ]),

#     'index': (0,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#         edges: [

#             Edge('cnot'[3] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-2'[0] )

#         ]),

#     'index': (1,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#         edges: [

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-3'[0] )

#         ]),

#     'index': (2,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]], dtype=complex64),

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge('cnot'[3] -> 'cnot'[1] ),

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('cnot'[3] -> 'h'[0] )

#         ]),

#     'index': (0, 1),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]], dtype=complex64),

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge(Dangling Edge)[1],

#             Edge('cnot'[2] -> 'h'[0] ),

#             Edge('cnot'[3] -> 'cnot'[1] )

#         ]),

#     'index': (2, 1),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False}]

ir[0]["gatef"]().tensor, ir[-1]["gate"].tensor  # 每个门的实际幺正矩阵
# Output:
#   (array([[ 0.70710677+0.j,  0.70710677+0.j],

#           [ 0.70710677+0.j, -0.70710677+0.j]], dtype=complex64),

#    array([[[[1.+0.j, 0.+0.j],

#             [0.+0.j, 0.+0.j]],

#    

#            [[0.+0.j, 1.+0.j],

#             [0.+0.j, 0.+0.j]]],

#    

#    

#           [[[0.+0.j, 0.+0.j],

#             [0.+0.j, 1.+0.j]],

#    

#            [[0.+0.j, 0.+0.j],

#             [1.+0.j, 0.+0.j]]]], dtype=complex64))

# 计算电路的最终输出量子态
c.state()
# Output:
#   array([0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j,

#          0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j, 0.35355335+0.j],

#         dtype=complex64)

# 计算一些期望值，比如 <X1>
x1 = c.expectation([tc.gates.x(), [1]])

# 或 <Z1Z2>
z1z2 = c.expectation([tc.gates.z(), [1]], [tc.gates.z(), [2]])

print(x1, z1z2)
# Output:
#   (0.9999998+0j) 0j


# 做一些随机取样
for _ in range(10):
    print(c.perfect_sampling())
# Output:
#   (array([1., 1., 1.], dtype=float32), 0.1250001713634208)

#   (array([0., 0., 1.], dtype=float32), 0.12499997764825821)

#   (array([0., 1., 1.], dtype=float32), 0.12499997764825821)

#   (array([1., 1., 0.], dtype=float32), 0.1249999776482098)

#   (array([1., 1., 0.], dtype=float32), 0.1249999776482098)

#   (array([1., 0., 1.], dtype=float32), 0.12499997019766829)

#   (array([0., 1., 0.], dtype=float32), 0.12499997764825821)

#   (array([1., 1., 0.], dtype=float32), 0.1249999776482098)

#   (array([0., 0., 0.], dtype=float32), 0.12499997764825821)

#   (array([0., 1., 1.], dtype=float32), 0.12499997764825821)


# 我们可以轻松地从 NumPy 切换模拟后端！

with tc.runtime_backend("tensorflow") as K:
    c = get_circuit(3)
    print(c.state())

with tc.runtime_backend("jax") as K:
    c = get_circuit(3)
    print(c.state())

with tc.runtime_backend("pytorch") as K:
    # pytorch 后端无法保证最佳性能和完整功能
    c = get_circuit(3)
    print(c.state())
# Output:
#   tf.Tensor(

#   [0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j

#    0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j], shape=(8,), dtype=complex64)

#   WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

#   [0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j

#    0.35355335+0.j 0.35355335+0.j 0.35355335+0.j 0.35355335+0.j]

#   tensor([0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,

#           0.3536+0.j])


"""
## 参数化量子电路(PQC)
"""

# 接受参数的量子电路门

print(tc.Circuit.vgates)
# Output:
#   ['r', 'cr', 'rx', 'ry', 'rz', 'crx', 'cry', 'crz', 'orx', 'ory', 'orz', 'any', 'exp', 'exp1']


# 查看每种类型的变量门的关键字参数（类型为浮点数）
for g in tc.Circuit.vgates:
    print(g, inspect.signature(getattr(tc.gates, g).f))
# Output:
#   r (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   cr (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   rx (theta: float = 0) -> tensorcircuit.gates.Gate

#   ry (theta: float = 0) -> tensorcircuit.gates.Gate

#   rz (theta: float = 0) -> tensorcircuit.gates.Gate

#   crx (*args: Any, **kws: Any) -> Any

#   cry (*args: Any, **kws: Any) -> Any

#   crz (*args: Any, **kws: Any) -> Any

#   orx (*args: Any, **kws: Any) -> Any

#   ory (*args: Any, **kws: Any) -> Any

#   orz (*args: Any, **kws: Any) -> Any

#   any (unitary: Any, name: str = 'any') -> tensorcircuit.gates.Gate

#   exp (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate

#   exp1 (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate


def get_circuit(n, params):
    c = tc.Circuit(n)  # 用 n 个量子比特初始化一个电路对象
    for i in range(n):
        c.rx(i, theta=params[i])  # 应用 rx 门
    c.cnot(0, 1)
    return c

K = tc.set_backend("tensorflow")

n = 3
params = K.ones([n])
c = get_circuit(n, params)
print(c.state())
# Output:
#   tf.Tensor(

#   [ 0.6758712 +0.j          0.        -0.36923012j  0.        -0.36923015j

#    -0.20171136-0.j         -0.20171136+0.j          0.        +0.11019541j

#     0.        -0.36923015j -0.20171136-0.j        ], shape=(8,), dtype=complex64)


ir = c.to_qir()
ir
# Output:
#   [{'gatef': rx,

#     'index': (0,),

#     'name': 'rx',

#     'split': None,

#     'mpo': False,

#     'parameters': {'theta': <tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>},

#     'gate': Gate(

#         name: '__unnamed_node__',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#             array([[0.87758255+0.j        , 0.        -0.47942555j],

#                    [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>,

#         edges: [

#             Edge('cnot'[2] -> '__unnamed_node__'[0] ),

#             Edge('__unnamed_node__'[1] -> 'qb-1'[0] )

#         ])},

#    {'gatef': rx,

#     'index': (1,),

#     'name': 'rx',

#     'split': None,

#     'mpo': False,

#     'parameters': {'theta': <tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>},

#     'gate': Gate(

#         name: '__unnamed_node__',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#             array([[0.87758255+0.j        , 0.        -0.47942555j],

#                    [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>,

#         edges: [

#             Edge('cnot'[3] -> '__unnamed_node__'[0] ),

#             Edge('__unnamed_node__'[1] -> 'qb-2'[0] )

#         ])},

#    {'gatef': rx,

#     'index': (2,),

#     'name': 'rx',

#     'split': None,

#     'mpo': False,

#     'parameters': {'theta': <tf.Tensor: shape=(), dtype=complex64, numpy=(1+0j)>},

#     'gate': Gate(

#         name: '__unnamed_node__',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#             array([[0.87758255+0.j        , 0.        -0.47942555j],

#                    [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>,

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge('__unnamed_node__'[1] -> 'qb-3'[0] )

#         ])},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             <tf.Tensor: shape=(2, 2, 2, 2), dtype=complex64, numpy=

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]], dtype=complex64)>,

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge(Dangling Edge)[1],

#             Edge('cnot'[2] -> '__unnamed_node__'[0] ),

#             Edge('cnot'[3] -> '__unnamed_node__'[0] )

#         ]),

#     'index': (0, 1),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False}]

# 查看量子门对应的矩阵
ir[0]["gatef"](**ir[0]["parameters"]).tensor
# Output:
#   <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#   array([[0.87758255+0.j        , 0.        -0.47942555j],

#          [0.        -0.47942555j, 0.87758255+0.j        ]], dtype=complex64)>

# 让我们组成一个可微分的量子函数


def energy(params):
    c = get_circuit(n, params)
    return K.real(c.expectation([tc.gates.z(), [1]]))


energy_vag = K.value_and_grad(energy)

print(energy_vag(params))

# 一旦我们有了梯度，我们就可以运行基于梯度的下降的变分优化
# Output:
#   (<tf.Tensor: shape=(), dtype=float32, numpy=0.2919265>, <tf.Tensor: shape=(3,), dtype=complex64, numpy=

#   array([-4.5464873e-01+0.j, -4.5464873e-01+0.j,  2.2351742e-08+0.j],

#         dtype=complex64)>)


# 并且使其可即时编译以加速

energy_vag_jit = K.jit(K.value_and_grad(energy))

print(energy_vag_jit(params))
# 第一次运行可即时编译的函数会很慢，但是后面的测量会超级快
# Output:
#   (<tf.Tensor: shape=(), dtype=float32, numpy=0.2919265>, <tf.Tensor: shape=(3,), dtype=complex64, numpy=

#   array([-4.5464873e-01+0.j, -4.5464873e-01+0.j,  2.2351742e-08+0.j],

#         dtype=complex64)>)


"""
## 电路的高级用法
"""

"""
### 输入状态

我们可以从默认的 |0^n> 替换输入状态
"""

input_state = K.ones([2**n])
input_state /= K.norm(input_state)

c = tc.Circuit(n, inputs=input_state)
c.H(0)
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([0.49999997+0.j, 0.49999997+0.j, 0.49999997+0.j, 0.49999997+0.j,

#          0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j],

#         dtype=complex64)>

"""
### 蒙特卡罗方法噪声模拟

``tc.Circuit`` 支持使用蒙特卡罗方法进行噪声模拟，并且它也是可即时编译的！ 此外，``tc.DMCircuit`` 支持使用全密度矩阵方法进行噪声模拟。
"""

c = tc.Circuit(n)
for i in range(n):
    c.H(i)
for i in range(n - 1):
    c.cnot(i, i + 1)
    c.depolarizing(i, px=0.1, py=0.1, pz=0.1)
    c.apply_general_kraus(tc.channels.phasedampingchannel(gamma=0.2), i + 1)
print(c.expectation([tc.gates.y(), [1]]))
# Output:
#   tf.Tensor(0j, shape=(), dtype=complex64)


"""
### 应用任意幺正门

只需直接使用 ``any`` API 通过提供相应的幺正矩阵
"""

c = tc.Circuit(n)
c.any(0, 1, unitary=K.ones([4, 4]) / K.norm(K.ones([4, 4])))
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([0.25+0.j, 0.  +0.j, 0.25+0.j, 0.  +0.j, 0.25+0.j, 0.  +0.j,

#          0.25+0.j, 0.  +0.j], dtype=complex64)>

"""
### 指数门

如果我们想将门模拟为 $e^{i\theta G}$ ，其中 $G^2=1$ 是一个矩阵，我们有一个快速有效的门实现，例如
`exp1`
"""

c = tc.Circuit(n)
for i in range(n):
    c.H(i)
for i in range(n - 1):
    c.exp1(i, i + 1, theta=K.ones([]), unitary=tc.gates._zz_matrix)
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([-0.14713009-3.2148516e-01j,  0.35355335+1.4901161e-08j,

#          -0.14713009+3.2148516e-01j,  0.35355335-1.4901161e-08j,

#           0.35355335-1.4901161e-08j, -0.14713009+3.2148516e-01j,

#           0.35355335+1.4901161e-08j, -0.14713009-3.2148516e-01j],

#         dtype=complex64)>

"""
在上面的例子中 $G=Z\otimes Z$
"""

print(tc.gates._zz_matrix)
# Output:
#   [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
下面列出了门模块中的常用矩阵
"""

for name in dir(tc.gates):
    if name.endswith("_matrix"):
        print(name, ":\n", getattr(tc.gates, name))
# Output:
#   _cnot_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]

#    [0. 0. 1. 0.]]

#   _cy_matrix :

#    [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -0.-1.j]

#    [ 0.+0.j  0.+0.j  0.+1.j  0.+0.j]]

#   _cz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0.  1.  0.  0.]

#    [ 0.  0.  1.  0.]

#    [ 0.  0.  0. -1.]]

#   _fredkin_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]]

#   _h_matrix :

#    [[ 0.70710678  0.70710678]

#    [ 0.70710678 -0.70710678]]

#   _i_matrix :

#    [[1. 0.]

#    [0. 1.]]

#   _ii_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 0. 0. 1.]]

#   _s_matrix :

#    [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   _swap_matrix :

#    [[1. 0. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]]

#   _t_matrix :

#    [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710678+0.70710678j]]

#   _toffoli_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]]

#   _wroot_matrix :

#    [[ 0.70710678+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710678+0.j ]]

#   _x_matrix :

#    [[0. 1.]

#    [1. 0.]]

#   _xx_matrix :

#    [[0. 0. 0. 1.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [1. 0. 0. 0.]]

#   _y_matrix :

#    [[ 0.+0.j -0.-1.j]

#    [ 0.+1.j  0.+0.j]]

#   _yy_matrix :

#    [[ 0.+0.j  0.-0.j  0.-0.j -1.+0.j]

#    [ 0.+0.j  0.+0.j  1.-0.j  0.-0.j]

#    [ 0.+0.j  1.-0.j  0.+0.j  0.-0.j]

#    [-1.+0.j  0.+0.j  0.+0.j  0.+0.j]]

#   _z_matrix :

#    [[ 1.  0.]

#    [ 0. -1.]]

#   _zz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
### 非幺正门

``tc.Circuit``由于其张量网络引擎性质而自动支持非幺正电路仿真
"""

c = tc.Circuit(n)
c.exp1(1, unitary=tc.gates._x_matrix, theta=K.ones([]) + 1.0j * K.ones([]))
c.state()
# Output:
#   <tf.Tensor: shape=(8,), dtype=complex64, numpy=

#   array([0.83373   -0.9888977j, 0.        +0.j       ,

#          0.63496387-1.2984576j, 0.        +0.j       ,

#          0.        +0.j       , 0.        +0.j       ,

#          0.        +0.j       , 0.        +0.j       ], dtype=complex64)>

"""
请注意，在这种情况下，最终量子态不再归一化。
"""

try:
    np.testing.assert_allclose(K.norm(c.state()), 1.0)
except AssertionError as e:
    print(e)
# Output:
#   

#   Not equal to tolerance rtol=1e-07, atol=0

#   

#   Mismatched elements: 1 / 1 (100%)

#   Max absolute difference: 0.93963802

#   Max relative difference: 0.93963802

#    x: array(1.939638+0.j, dtype=complex64)

#    y: array(1.)




================================================
FILE: docs/source/tutorials/circuit_qudit_basics.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Qudit Circuit Basics

*A gentle intro to `tensorcircuit.quditcircuit.QuditCircuit`*
"""

"""

## Overview

This tutorial shows how to build and simulate **qudit** circuits (d‑level systems, where `d ≥ 3`) using `tensorcircuit`'s `QuditCircuit` API.
**Highlights**
- Create a `QuditCircuit(nqudits, dim)` with dimension `dim ∈ [3, 36]`.
- Single-qudit gates: `X`, `Z`, `H`, rotations `RX/RY/RZ` on selected levels `(j, k)`.
- Two‑qudit gates: `RXX`, `RZZ`, and the generalized controlled‑sum `CSUM` and controlled-phase `CPHASE`.
- Obtain wavefunctions, probabilities, samples, expectations, and sub‑system projections.
- Samples and bitstrings use base‑36 digits (`0–9A–Z`) where `A = 10, ..., Z = 35`.

"""

"""

## Setup
"""

import tensorcircuit as tc
from tensorcircuit.quditcircuit import QuditCircuit

tc.set_backend("numpy")  # or "jax", "tensorflow", "pytorch"
print("tensorcircuit version:", tc.__version__)
# Output:
#   tensorcircuit version: 1.3.0


"""

## Hello, Qutrit! (dim = 13)

We'll prepare a **single qudit** (`nqudits=1`, `dim=13`), apply a generalized Hadamard `H` to put it into an equal superposition, and inspect the resulting state and probabilities.

"""

c = QuditCircuit(nqudits=1, dim=13)
c.h(0)  # generalized Hadamard on the only qudit
psi = c.wavefunction()  # state vector of length 13^1 = 13
probs = c.probability()  # probability vector (length 3)
print(r"\psi:", psi)
print("P:", probs)
# Output:
#   \psi: [0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j

#    0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j

#    0.2773501+0.j 0.2773501+0.j 0.2773501+0.j]

#   P: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308

#    0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308

#    0.07692308]


"""

## Multi‑Qudit Basics

Let's move to **two qutrits** and create a maximally entangled state using `H` and the qudit controlled‑sum `CSUM`.

The operator `CSUM(control, target, cv=None)` adds the control's value to the target modulo `dim`. It's a natural generalization of CNOT. If you pass `cv`, the gate activates only when the control equals that value (default is `None`).

"""

cq = QuditCircuit(nqudits=2, dim=3)  # two qutrits
cq.h(0)  # superpose control
cq.csum(0, 1)  # qudit CNOT analog (control=0, target=1)
psi = cq.wavefunction()
probs = cq.probability()
print(r"|\psi|^2 (length 3^2=9):", probs)
# Output:
#   |\psi|^2 (length 3^2=9): [0.3333333 0.        0.        0.        0.3333333 0.        0.

#    0.        0.3333333]


"""

### Sampling and Base‑36 Readout

Sampling returns strings in base‑`dim` using **`0-9A-Z`**. For `dim=3`, the alphabet is `0,1,2`:

"""

samples = cq.sample(batch=512, format="count_dict_bin")  # e.g., '00', '11', '22'
samples
# Output:
#   {'00': 160, '11': 171, '22': 181}

"""

## Single‑Qudit Rotations on Selected Levels

For a qudit, rotations target a **two‑level subspace** inside the `d` levels.

- `rx(index, theta, j=0, k=1)` rotates between levels `j` and `k` about the X‑axis of that embedded SU(2).
- `ry(index, theta, j=0, k=1)` similarly for Y.
- `rz(index, theta, j=0)` applies a Z‑phase to a single level `j`.

> Tip: `(j, k)` must be distinct integers in `[0, dim-1]`.

"""

import numpy as np

c = QuditCircuit(nqudits=1, dim=5)  # a ququint
c.h(0)  # start in equal superposition
c.rx(0, theta=np.pi / 3, j=1, k=3)  # rotate levels 1 and 3
c.rz(0, theta=np.pi / 5, j=4)  # add a phase to level 4
psi = c.wavefunction()
probs = c.probability()
psi, probs
# Output:
#   (array([0.4472136 +0.j        , 0.38729832-0.2236068j ,

#           0.4472136 +0.j        , 0.38729832-0.2236068j ,

#           0.3618034 +0.26286554j], dtype=complex64),

#    array([0.19999999, 0.19999997, 0.19999999, 0.19999997, 0.20000002],

#          dtype=float32))

"""

## Two‑Qudit Interactions: `RXX`, `RZZ`

You can couple two qudits by acting on chosen **subspaces** of each:

- `rxx(q1, q2, theta, j1=0, k1=1, j2=0, k2=1)`
- `rzz(q1, q2, theta, j1=0, k1=1, j2=0, k2=1)`

Both gates are the natural generalizations of qubit XX/ZZ rotations but restricted to the `(j, k)` subspaces.

"""

c2 = QuditCircuit(nqudits=2, dim=4)  # two ququarts
c2.h(0)
c2.h(1)
c2.rxx(0, 1, theta=np.pi / 4, j1=0, k1=2, j2=1, k2=3)
c2.rzz(0, 1, theta=np.pi / 7, j1=0, k1=1, j2=0, k2=1)
c2.probability()
# Output:
#   array([0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,

#          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],

#         dtype=float32)

"""

## Expectation Values of Local Operators

`expectation(*ops)` computes the expectation for one or more local observables. Each observable is a pair `(op, [site_indices])` where `op` is a tensor (matrix) with appropriate dimension.

"""

# Example: build a diagonal operator on a single qutrit (dim=3)
import numpy as np

c = QuditCircuit(1, dim=3)
c.h(0)
op = np.diag([0.0, 0.5, 1.0])  # acts on subspace levels 0,1,2
expval = c.expectation((op, [0]))
expval
# Output:
#   array(0.49999997+0.j, dtype=complex64)

"""
### Apply Arbitrary Gate

Just directly using ``any`` API by feeding the corresponding unitary
"""

d = 36
c = tc.QuditCircuit(2, dim=d)
h_matrix = tc.quditgates.h_matrix_func(d)
c.any(0, unitary=h_matrix)
csum_matrix = tc.quditgates.csum_matrix_func(d)
c.any(0, 1, unitary=csum_matrix)
c.sample(1024, format="count_dict_bin")
# Output:
#   {'00': 29,

#    '11': 35,

#    '22': 29,

#    '33': 41,

#    '44': 25,

#    '55': 28,

#    '66': 28,

#    '77': 35,

#    '88': 32,

#    '99': 27,

#    'AA': 38,

#    'BB': 35,

#    'CC': 29,

#    'DD': 31,

#    'EE': 30,

#    'FF': 22,

#    'GG': 26,

#    'HH': 19,

#    'II': 26,

#    'JJ': 24,

#    'KK': 37,

#    'LL': 27,

#    'MM': 34,

#    'NN': 27,

#    'OO': 31,

#    'PP': 31,

#    'QQ': 28,

#    'RR': 26,

#    'SS': 23,

#    'TT': 27,

#    'UU': 32,

#    'VV': 27,

#    'WW': 19,

#    'XX': 27,

#    'YY': 22,

#    'ZZ': 17}

"""

## Notes & Tips

- **Dimensions**: `QuditCircuit` validates `dim` and keeps it consistent across the circuit.
- **Wavefunction & Probability**: `wavefunction()` returns the state; `probability()` returns a length‑`dim^n` vector.
- **Sampling**: `sample(batch, format="str")` returns base‑36 strings for readability; use `format=None` for raw integers.
- **Controlled Operations**: `csum(control, target, cv=None)` generalizes CNOT; `cv` picks the active control value.
- **Backend**: Switch via `tc.set_backend("numpy" | "jax" | "tensorflow" | "pytorch")` as needed.
- **Interoperability**: You can still obtain `matrix()` for the full unitary or `quoperator()` MPO‑like forms for advanced workflows.

All the functions are similar to the `tc.Circuit`

"""



================================================
FILE: docs/source/tutorials/circuit_qudit_basics_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Qudit（多能级量子比特） 电路基础

*`tensorcircuit.quditcircuit.QuditCircuit` 的轻量入门*
"""

"""

## 概述

这个教程展示了如何使用 `tensorcircuit` 的 `QuditCircuit` API 来构建和模拟 **qudit（多能级量子比特）** 电路（d 级系统，其中 `d ≥ 3`）。

**要点**
- 创建一个 `QuditCircuit(nqudits, dim)` with dimension `dim ∈ [3, 36]`；
- 单比特门: `X`, `Z`, `H`, rotations `RX/RY/RZ` on selected levels `(j, k)`；
- 两比特门: `RXX`, `RZZ`, and the 广义受控求和 `CSUM` and 受控相位 `CPHASE`；
- 可得到波函数、概率、采样、期望值以及子系统投影；
- 样本和比特串使用36进制数字（`0–9A–Z`），其中`A = 10, ..., Z = 35`。

"""

"""

## 配置
"""

import tensorcircuit as tc
from tensorcircuit.quditcircuit import QuditCircuit

tc.set_backend("numpy")  # or "jax", "tensorflow", "pytorch"
print("tensorcircuit version:", tc.__version__)
# Output:
#   tensorcircuit version: 1.3.0


"""
## 嗨，Qutrit！（维度为 13）

我们来准备一个**单独的 qudit* (`nqudits=1`, `dim=13`)， 对它应用一个广义的 Hadamard 门 `H`，让它进入一个等概率叠加态，然后观察最终的状态和概率。
"""

c = QuditCircuit(nqudits=1, dim=13)
c.h(0)  # generalized Hadamard on the only qudit
psi = c.wavefunction()  # state vector of length 13^1 = 13
probs = c.probability()  # probability vector (length 3)
print(r"\psi:", psi)
print("P:", probs)
# Output:
#   \psi: [0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j

#    0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j 0.2773501+0.j

#    0.2773501+0.j 0.2773501+0.j 0.2773501+0.j]

#   P: [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308

#    0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308

#    0.07692308]


"""
## 多量子比特基础

让我们来研究一下**两个三态量子比特**，使用 `H` 门和量子比特受控和门 `CSUM` 来创建一个最大纠缠态。

算符 `CSUM(control, target, cv=None)` 将控制位的数值加到目标位上，结果对 `dim` 取模。这是 CNOT 门的自然推广。如果传入 `cv`，则该门仅在控制位等于该值时激活（默认为 `None`）。
"""

cq = QuditCircuit(nqudits=2, dim=3)  # two qutrits
cq.h(0)  # superpose control
cq.csum(0, 1)  # qudit CNOT analog (control=0, target=1)
psi = cq.wavefunction()
probs = cq.probability()
print(r"|\psi|^2 (length 3^2=9):", probs)
# Output:
#   |\psi|^2 (length 3^2=9): [0.3333333 0.        0.        0.        0.3333333 0.        0.

#    0.        0.3333333]


"""
### 采样和 36 进制读数

采样会返回用 `0-9A-Z` 表示的、基于 `dim` 进制的字符串。例如，当 `dim=3` 时，字母表是 `0,1,2`：
"""

samples = cq.sample(batch=512, format="count_dict_bin")  # e.g., '00', '11', '22'
samples
# Output:
#   {'00': 175, '11': 165, '22': 172}

"""
## 在选定能级上的单量子比特旋转

对于一个 qudit（多态量子位），旋转的目标是 `d` 个能级中的一个**双能级子空间**。

- `rx(index, theta, j=0, k=1)` 在能级 `j` 和 `k` 之间绕该嵌入的 SU(2) 的 X 轴旋转。
- `ry(index, theta, j=0, k=1)` 类似地，绕 Y 轴旋转。
- `rz(index, theta, j=0)` 对单个能级 `j` 应用 Z 相位。

> 提示：`(j, k)` 必须是 `[0, dim-1]` 中的不同整数。
"""

import numpy as np

c = QuditCircuit(nqudits=1, dim=5)  # a ququint
c.h(0)  # start in equal superposition
c.rx(0, theta=np.pi / 3, j=1, k=3)  # rotate levels 1 and 3
c.rz(0, theta=np.pi / 5, j=4)  # add a phase to level 4
psi = c.wavefunction()
probs = c.probability()
psi, probs
# Output:
#   (array([0.4472136 +0.j        , 0.38729832-0.2236068j ,

#           0.4472136 +0.j        , 0.38729832-0.2236068j ,

#           0.3618034 +0.26286554j], dtype=complex64),

#    array([0.19999999, 0.19999997, 0.19999999, 0.19999997, 0.20000002],

#          dtype=float32))

"""
## 两个多能级量子比特的相互作用：`RXX`，`RZZ`

你可以对每个多能级量子比特的选定**子空间**进行操作来进行耦合：

- `rxx(q1, q2, theta, j1=0, k1=1, j2=0, k2=1)`
- `rzz(q1, q2, theta, j1=0, k1=1, j2=0, k2=1)`

这两个门都是量子比特 XX/ZZ 旋转的自然推广，但仅限于 `(j, k)` 子空间。
"""

c2 = QuditCircuit(nqudits=2, dim=4)  # two ququarts
c2.h(0)
c2.h(1)
c2.rxx(0, 1, theta=np.pi / 4, j1=0, k1=2, j2=1, k2=3)
c2.rzz(0, 1, theta=np.pi / 7, j1=0, k1=1, j2=0, k2=1)
c2.probability()
# Output:
#   array([0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,

#          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],

#         dtype=float32)

"""
## 局部算符的期望值

`expectation(*ops)` 计算一个或多个局部可观测量（算符）的期望值。每个可观测量是一个对 `(op, [site_indices])`，其中 `op` 是一个具有适当维度的张量（矩阵）。
"""

# Example: build a diagonal operator on a single qutrit（三能级量子比特）（三能级量子比特） (dim=3)
import numpy as np

c = QuditCircuit(1, dim=3)
c.h(0)
op = np.diag([0.0, 0.5, 1.0])  # acts on subspace levels 0,1,2
expval = c.expectation((op, [0]))
expval
# Output:
#   array(0.49999997+0.j, dtype=complex64)

"""
### 应用任意门

直接使用 ``any`` API，输入对应的幺正矩阵即可。
"""

d = 36
c = tc.QuditCircuit(2, dim=d)
h_matrix = tc.quditgates.h_matrix_func(d)
c.any(0, unitary=h_matrix)
csum_matrix = tc.quditgates.csum_matrix_func(d)
c.any(0, 1, unitary=csum_matrix)
c.sample(1024, format="count_dict_bin")
# Output:
#   {'00': 35,

#    '11': 28,

#    '22': 28,

#    '33': 25,

#    '44': 33,

#    '55': 30,

#    '66': 34,

#    '77': 25,

#    '88': 22,

#    '99': 22,

#    'AA': 40,

#    'BB': 38,

#    'CC': 35,

#    'DD': 17,

#    'EE': 21,

#    'FF': 27,

#    'GG': 24,

#    'HH': 26,

#    'II': 27,

#    'JJ': 24,

#    'KK': 18,

#    'LL': 27,

#    'MM': 23,

#    'NN': 38,

#    'OO': 26,

#    'PP': 31,

#    'QQ': 27,

#    'RR': 37,

#    'SS': 28,

#    'TT': 20,

#    'UU': 34,

#    'VV': 34,

#    'WW': 32,

#    'XX': 25,

#    'YY': 29,

#    'ZZ': 34}

"""
## 注意事项和提示

- **维度**: `QuditCircuit` 会验证 `dim` 并且保持它在整个电路中的一致性。
- **波函数和概率**: `wavefunction()` 返回状态；`probability()` 返回一个长度为 `dim^n` 的向量。
- **采样**: `sample(batch, format="str")` 返回易于阅读的 36 进制字符串；使用 `format=None` 获取原始整数。
- **受控操作**: `csum(control, target, cv=None)` 泛化了 CNOT 门；`cv` 选择激活的控制值。
- **后端**: 根据需要通过 `tc.set_backend("numpy" | "jax" | "tensorflow" | "pytorch")` 切换后端。
- **互操作性**: 你仍然可以获取完整幺正矩阵的 `matrix()` 或类似 MPO 的 `quoperator()` 形式，用于高级工作流程。

所有这些函数都类似于 `tc.Circuit`。
"""



================================================
FILE: docs/source/tutorials/contractors.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# The usage of contractor

## Overview

In this tutorial, we will demonstrate how to utilize different types of TensorNetwork contractors for the circuit simulation to achieve a better space-time tradeoff. The customization of the contractor is the main highlight for the TensorCircuit package since a better contractor can make better use of the power of the TensorNetwok simulation engine. [WIP]
"""

"""
## Setup
"""

import tensorcircuit as tc
import numpy as np
import cotengra as ctg
import opt_einsum as oem

K = tc.set_backend("tensorflow")

"""
## Testbed System

We provide tensor networks for two circuits, and test the contraction efficiency for these two systems, the first system is small while the second one is large.
"""

# get state for small system
def small_tn():
    n = 10
    d = 4
    param = K.ones([2 * d, n])
    c = tc.Circuit(n)
    c = tc.templates.blocks.example_block(c, param, nlayers=d)
    return c.state()

# get expectation for extra large system
def large_tn():
    n = 60
    d = 8
    param = K.ones([2 * d, n])
    c = tc.Circuit(n)
    c = tc.templates.blocks.example_block(c, param, nlayers=d, is_split=True)
    # the two qubit gate is split and truncated with SVD decomposition
    return c.expectation([tc.gates.z(), [n // 2]], reuse=False)

"""
## Opt-einsum

There are several contractor optimizers provided by opt-einsum and shipped with the TensorNetwork package. Since TensorCircuit is built on top of TensorNetwork, we can use these simple contractor optimizers. Though for any moderate system, only greedy optimizer works, other optimizers come with exponential scaling and fail in circuit simulation scenarios.

We always set ``contraction_info=True`` (default ``False``) for the contractor system, which will print contraction information summary including contraction size, flops, and writes. For the definition of these metrics, also refer to cotengra docs.
"""

# if we set nothing, the default optimizer is greedy, i.e.
tc.set_contractor("greedy", debug_level=2, contraction_info=True)
# We set debug_level=2 to not really run the contraction computation
# i.e. by set debug_level>0, only contraction information and the return shape is correct, the result is wrong
# Output:
#   functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd588e281f0>, memory_limit=None, debug_level=2)

small_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  5.132  log2[SIZE]:  11  log2[WRITE]:  13.083

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  17.766  log2[SIZE]:  44  log2[WRITE]:  49.636

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

# we can use more fancy contractor in opt-einsum but not in tensornetwork
# custom_stateful is used for contraction path finder which has a life cycle of one-time path solver
tc.set_contractor(
    "custom_stateful",
    optimizer=oem.RandomGreedy,
    max_time=60,
    max_repeats=128,
    minimize="size",
    debug_level=2,
    contraction_info=True,
)
# Output:
#   functools.partial(<function custom_stateful at 0x7fd5a0a3d4c0>, optimizer=<class 'opt_einsum.path_random.RandomGreedy'>, opt_conf=None, contraction_info=True, debug_level=2, max_time=60, max_repeats=128, minimize='size')

small_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  4.925  log2[SIZE]:  10  log2[WRITE]:  12.531

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  11.199  log2[SIZE]:  26  log2[WRITE]:  28.183

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
## Cotengra
"""

"""
for more advanced contractors, we ask help for the sota contractor optimizer on the market: cotengra
"""

opt = ctg.ReusableHyperOptimizer(
    methods=["greedy", "kahypar"],
    parallel=True,
    minimize="write",
    max_time=120,
    max_repeats=1024,
    progbar=True,
)
tc.set_contractor(
    "custom", optimizer=opt, preprocessing=True, contraction_info=True, debug_level=2
)
## for more setup on cotengra optimizers, see the reference in
## https://cotengra.readthedocs.io/en/latest/advanced.html
## preprocessing=True will merge all single-qubit gates into neighboring two-qubit gates
# Output:
#   functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd588e28ee0>, memory_limit=None, debug_level=2, preprocessing=True)

small_tn()
# Output:
#   log2[SIZE]: 10.00 log10[FLOPs]: 4.90: 100%|█████████████████████████████████████████| 1024/1024 [00:28<00:00, 35.45it/s]
#   ------ contraction cost summary ------

#   log10[FLOPs]:  4.900  log2[SIZE]:  10  log2[WRITE]:  12.255

#   

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   log2[SIZE]: 20.00 log10[FLOPs]: 9.50:   4%|█▊                                         | 43/1024 [02:09<49:22,  3.02s/it]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  9.501  log2[SIZE]:  20  log2[WRITE]:  24.090

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
We can also apply subtree reconf after cotengra find the path, which in general further (and usually greatly) improve flops and write for the contraction. Indeed, the subtree reconf postprocessing is in general more important than increasing the search time or repeats for the optimizer.
"""

opt = ctg.ReusableHyperOptimizer(
    minimize="combo",
    max_repeats=1024,
    max_time=240,
    progbar=True,
)


def opt_reconf(inputs, output, size, **kws):
    tree = opt.search(inputs, output, size)
    tree_r = tree.subtree_reconfigure_forest(
        progbar=True, num_trees=10, num_restarts=20, subtree_weight_what=("size",)
    )
    return tree_r.get_path()


tc.set_contractor(
    "custom",
    optimizer=opt_reconf,
    contraction_info=True,
    preprocessing=True,
    debug_level=2,
)
# Output:
#   functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd58c832700>, memory_limit=None, debug_level=2, preprocessing=True)

small_tn()
# Output:
#   log2[SIZE]: 10.00 log10[FLOPs]: 4.87: 100%|█████████████████████████████████████████| 1024/1024 [02:29<00:00,  6.86it/s]

#   log2[SIZE]: 10.00 log10[FLOPs]: 4.86: 100%|█████████████████████████████████████████████| 20/20 [00:31<00:00,  1.57s/it]
#   ------ contraction cost summary ------

#   log10[FLOPs]:  4.859  log2[SIZE]:  10  log2[WRITE]:  12.583

#   

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   log2[SIZE]: 21.00 log10[FLOPs]: 9.62:   9%|███▉                                       | 93/1024 [04:04<40:49,  2.63s/it]

#   log2[SIZE]: 17.00 log10[FLOPs]: 8.66: 100%|█████████████████████████████████████████████| 20/20 [03:08<00:00,  9.44s/it]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  8.657  log2[SIZE]:  17  log2[WRITE]:  25.035

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
We can also extract the tensornetwork directly for the circuit or observable calculations and we can do the contraction by ourselves using any method we like. Moreover, all these contractors or our customized external contractions can still be compatible with jit, automatic differentiation. Specifically, the contraction path solver, though taking some time overhead, is only evaluated once due to the jit infrastructure (note for demonstration usage, we don't decorate our contraction function with ``K.jit`` here).
"""



================================================
FILE: docs/source/tutorials/contractors_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# contractor 使用

## 概述

在本教程中，我们将演示如何利用不同类型的张量网络 contractor 进行电路仿真，以实现更好的时空消耗平衡。contractor 的定制是 TensorCircuit 库的主要亮点之一，因为更好的 contractor 可以更好地利用 TensorNetwok 仿真引擎的强大功能。
"""

"""
## 设置
"""

import tensorcircuit as tc
import numpy as np
import cotengra as ctg
import opt_einsum as oem

K = tc.set_backend("tensorflow")

"""
## 试验体系

我们为两个电路提供张量网络，并测试这两个系统的收缩效率，第一个系统小，第二个系统大。
"""

# 获取小系统的量子态
def small_tn():
    n = 10
    d = 4
    param = K.ones([2 * d, n])
    c = tc.Circuit(n)
    c = tc.templates.blocks.example_block(c, param, nlayers=d)
    return c.state()

# 获得对超大型系统的期望
def large_tn():
    n = 60
    d = 8
    param = K.ones([2 * d, n])
    c = tc.Circuit(n)
    c = tc.templates.blocks.example_block(c, param, nlayers=d, is_split=True)
    #
    return c.expectation([tc.gates.z(), [n // 2]], reuse=False)

"""
## opt-einsum

opt-einsum 提供了几个 contractor 优化器，并与 TensorNetwork 包一起提供。 由于 TensorCircuit 建立在 TensorNetwork 之上，我们可以使用这些简单的 contractor 优化器。 尽管对于任何中等系统，只有贪婪优化器有效，但其他优化器具有指数缩放并且在电路仿真场景中失效。我们总是为 contractor 系统设置``contraction_info=True``（默认为``False``），它将打印包括 contraction size、flops 和 writes 在内的收缩信息摘要。 有关这些指标的定义，另请参阅 cotengra 文档。
"""

# 如果我们什么都不设置，默认优化器是贪婪的，即:
tc.set_contractor("greedy", debug_level=2, contraction_info=True)
# 我们设置 debug_level=2 以不真正运行收缩计算
# 即通过设置debug_level>0，只有收缩信息和返回形状正确，结果错误
# Output:
#   functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd588e281f0>, memory_limit=None, debug_level=2)

small_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  5.132  log2[SIZE]:  11  log2[WRITE]:  13.083

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  17.766  log2[SIZE]:  44  log2[WRITE]:  49.636

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

# 我们可以在 opt-einsum 中使用更多花哨的 contractor，他们不在 TensorNetwork 中定义
# custom_stateful 用于具有一次性生命周期的路径求解器的收缩路径查找器
tc.set_contractor(
    "custom_stateful",
    optimizer=oem.RandomGreedy,
    max_time=60,
    max_repeats=128,
    minimize="size",
    debug_level=2,
    contraction_info=True,
)
# Output:
#   functools.partial(<function custom_stateful at 0x7fd5a0a3d4c0>, optimizer=<class 'opt_einsum.path_random.RandomGreedy'>, opt_conf=None, contraction_info=True, debug_level=2, max_time=60, max_repeats=128, minimize='size')

small_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  4.925  log2[SIZE]:  10  log2[WRITE]:  12.531

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  11.199  log2[SIZE]:  26  log2[WRITE]:  28.183

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
## cotengra
"""

"""
对于更高级的 contractor，我们向市场上的 sota contractor 优化器寻求帮助：cotengra
"""

opt = ctg.ReusableHyperOptimizer(
    methods=["greedy", "kahypar"],
    parallel=True,
    minimize="write",
    max_time=120,
    max_repeats=1024,
    progbar=True,
)
tc.set_contractor(
    "custom", optimizer=opt, preprocessing=True, contraction_info=True, debug_level=2
)
## 有关 cotengra 优化器的更多设置，请参阅参考资料
## https://cotengra.readthedocs.io/en/latest/advanced.html
## preprocessing=True 将所有单量子比特门合并到相邻的双量子比特门
# Output:
#   functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd588e28ee0>, memory_limit=None, debug_level=2, preprocessing=True)

small_tn()
# Output:
#   log2[SIZE]: 10.00 log10[FLOPs]: 4.90: 100%|█████████████████████████████████████████| 1024/1024 [00:28<00:00, 35.45it/s]
#   ------ contraction cost summary ------

#   log10[FLOPs]:  4.900  log2[SIZE]:  10  log2[WRITE]:  12.255

#   

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   log2[SIZE]: 20.00 log10[FLOPs]: 9.50:   4%|█▊                                         | 43/1024 [02:09<49:22,  3.02s/it]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  9.501  log2[SIZE]:  20  log2[WRITE]:  24.090

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
我们也可以在 cotengra 找到路径之后应用 subtree reconf，这通常会进一步（并且通常会大大）为 contraction 改善 flops 和 writes。实际上, subtree reconf 后期处理通常比增加优化器的搜索时间或重复次数更重要。
"""

opt = ctg.ReusableHyperOptimizer(
    minimize="combo",
    max_repeats=1024,
    max_time=240,
    progbar=True,
)


def opt_reconf(inputs, output, size, **kws):
    tree = opt.search(inputs, output, size)
    tree_r = tree.subtree_reconfigure_forest(
        progbar=True, num_trees=10, num_restarts=20, subtree_weight_what=("size",)
    )
    return tree_r.get_path()


tc.set_contractor(
    "custom",
    optimizer=opt_reconf,
    contraction_info=True,
    preprocessing=True,
    debug_level=2,
)
# Output:
#   functools.partial(<function custom at 0x7fd5a0a3d430>, optimizer=<function contraction_info_decorator.<locals>.new_algorithm at 0x7fd58c832700>, memory_limit=None, debug_level=2, preprocessing=True)

small_tn()
# Output:
#   log2[SIZE]: 10.00 log10[FLOPs]: 4.87: 100%|█████████████████████████████████████████| 1024/1024 [02:29<00:00,  6.86it/s]

#   log2[SIZE]: 10.00 log10[FLOPs]: 4.86: 100%|█████████████████████████████████████████████| 20/20 [00:31<00:00,  1.57s/it]
#   ------ contraction cost summary ------

#   log10[FLOPs]:  4.859  log2[SIZE]:  10  log2[WRITE]:  12.583

#   

#   <tf.Tensor: shape=(1024,), dtype=complex64, numpy=

#   array([0.+0.j, 0.+0.j, 0.+0.j, ..., 0.+0.j, 0.+0.j, 0.+0.j],

#         dtype=complex64)>

large_tn()
# Output:
#   log2[SIZE]: 21.00 log10[FLOPs]: 9.62:   9%|███▉                                       | 93/1024 [04:04<40:49,  2.63s/it]

#   log2[SIZE]: 17.00 log10[FLOPs]: 8.66: 100%|█████████████████████████████████████████████| 20/20 [03:08<00:00,  9.44s/it]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  8.657  log2[SIZE]:  17  log2[WRITE]:  25.035

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
我们也可以直接提取张量网络用于电路或可观测计算，我们可以使用我们喜欢的任何方法进行收缩。此外，所有这些 contractor 或我们定制的外部收缩仍然可以与 jit，自动微分等兼容。具体来说，收缩路径求解器虽然需要一些时间成本，但由于 jit 基础设施只测量一次（注意，为了演示使用，我们在这里不使用``K.jit``装饰我们的收缩函数）。
"""



================================================
FILE: docs/source/tutorials/distributed_simulation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Distributed Circuit Simulation and TensorNetwork Contraction

## Overview

Simulating large quantum circuits or computing expectation values for complex Hamiltonians often involves contracting a massive tensor network. The computational cost (both time and memory) of this contraction can be a significant bottleneck, especially for systems with many qubits.

TensorCircuit provides an experimental feature, `DistributedContractor`, designed to tackle this challenge. It leverages multiple devices (e.g., GPUs) to parallelize the tensor network contraction. The core idea is:

1.  **Pathfinding with `cotengra`**: It first uses the powerful `cotengra` library to find an optimal or near-optimal contraction path for the given tensor network. This path often involves "slicing" the network, which breaks the single large contraction into many smaller, independent contractions.
2.  **Distributed Computation**: It then distributes these smaller contraction tasks across all available devices. Each device computes a subset of the slices in parallel.
3.  **Aggregation**: Finally, the results from all devices are aggregated to produce the final value (e.g., an expectation value or a state amplitude).

This approach allows us to tackle much larger problems than would be possible on a single device, significantly reducing the wall-clock time for expensive computations.

In this tutorial, we will demonstrate how to use `DistributedContractor` for two common tasks:
-   Calculating the amplitude of a specific bitstring for a large quantum state.
-   Running a Variational Quantum Eigensolver (VQE) optimization for a transverse-field Ising model.
"""

"""
## Setup

First, let's configure JAX to use multiple (virtual) devices and import the necessary libraries.

"""

import os

# Set this for multiple virtual devices
NUM_DEVICES = 4
os.environ["XLA_FLAGS"] = f"--xla_force_host_platform_device_count={NUM_DEVICES}"

import time
import jax
from jax import numpy as jnp
import numpy as np
import optax
import tensorcircuit as tc
from tensorcircuit.experimental import DistributedContractor

K = tc.set_backend("jax")
tc.set_dtype("complex64")

# Verify that JAX sees the configured number of devices
print(f"JAX is using {jax.local_device_count()} devices.")
# Output:
#   JAX is using 4 devices.


"""
## `DistributedContractor`: Mechanism

Before diving into examples, let's understand the core components and the inner workings of the `DistributedContractor`.

### The `nodes_fn` Template

The central requirement for using `DistributedContractor` is to provide a function, which we conventionally call `nodes_fn`. This function serves as a template that defines the structure of the tensor network.

-   **Input**: `nodes_fn` must accept a single argument, typically a dictionary or a JAX PyTree of parameters (`params`). These parameters are what you would typically vary in your computation (e.g., the variational parameters of a circuit).
-   **Output**: It must return the list of tensors nodes (`tc.Gate` or `tn.Node` objects, which contain tensors) that constitute the tensor network *before the final contraction*. `tensorcircuit-ng` provides convenient methods like `.expectation_before()` and `.amplitude_before()` for this purpose.

The `DistributedContractor` calls this `nodes_fn` once during its initialization (`__init__`) with a set of template parameters. It does this to understand the network's connectivity and size, which is necessary for the `cotengra` pathfinder. The *actual values* in the tensors from this initial call are discarded; only the *structure* is used.

### The Internal Mechanism

Here's a step-by-step breakdown of what happens inside `DistributedContractor`:

1.  **Initialization (`__init__`)**:
    - You provide the `nodes_fn` and a set of `params`.
    - `DistributedContractor` calls `nodes_fn(params)` to get the tensor network structure.
    - It passes this structure to `cotengra`'s `ReusableHyperOptimizer`.
    - **Pathfinding**: `cotengra` then performs an exhaustive search for an efficient contraction path. A key part of this is **slicing**. If the largest intermediate tensor in the best path exceeds a memory limit (which you can control via `cotengra_options`), `cotengra` will "slice" one or more of the largest tensor edges. Slicing means the contraction is repeated for each possible value of the sliced indices, and the results are summed up. This trades a massive increase in computation for a drastic reduction in memory.
    - The final output of this step is a `ContractionTree`, a plan that details the sequence of pairwise contractions and the slicing strategy.
    - **Task Distribution**: The contractor then divides the list of slices evenly among the available JAX devices.

2.  **Execution (`.value()` or `.value_and_grad()`)**:
    - You call the method with a *new* set of `params`.
    - The contractor uses JAX's `pmap` to send the contraction plan and the new `params` to all devices.
    - **Parallel Execution**: Each device, in parallel:
        - Calls your `nodes_fn` with the new `params` to generate the tensors with their *current numerical values*.
        - Iterates through its assigned subset of slices.
        - For each slice, it performs the small, memory-efficient contraction as prescribed by the `ContractionTree`.
        - It sums up the results of all its assigned slices.
    - **Aggregation**: The final results from each device are postprocessed via `op` function which can provided in `value()` and `value_and_grad()` methods and summed up on the host to produce the total value. If `.value_and_grad()` was called, the gradients are also aggregated in the same way.

This design is powerful because the most expensive step—pathfinding—is done only once. All subsequent calls with different parameters reuse the same optimized path, leading to very fast execution, especially in iterative algorithms like VQE.
"""

"""
## Example 1: Calculating State Amplitudes

One fundamental task is to compute the amplitude of a specific basis state, $\langle s | \psi \rangle$, where $|s\rangle$ is a bitstring like $|0110\dots\rangle$ and $|\psi\rangle$ is the state produced by a quantum circuit.

### Defining the `nodes_fn`

"""

N_QUBITS_AMP = 14
DEPTH_AMP = 7


def circuit_ansatz(n, d, params):
    """A standard hardware-efficient ansatz."""
    c = tc.Circuit(n)
    c.h(range(n))
    for i in range(d):
        for j in range(0, n - 1):
            c.rzz(j, j + 1, theta=params[j, i, 0])
        for j in range(n):
            c.rx(j, theta=params[j, i, 1])
        for j in range(n):
            c.ry(j, theta=params[j, i, 2])
    return c


def get_nodes_fn_amp(n, d):
    """
    This function returns another function that will be the input for DistributedContractor.
    The inner function takes a dictionary of parameters and returns the tensor for a single amplitude.
    """

    def nodes_fn(params):
        psi = circuit_ansatz(n, d, params["circuit"])
        # `amplitude_before` gives us the tensor network before final contraction
        return psi.amplitude_before(params["amplitude"])

    return nodes_fn


def get_binary_representation(i: int, N: int) -> jax.Array:
    """Helper function to convert an integer to its binary representation."""
    shifts = jnp.arange(N - 1, -1, -1)
    return ((i >> shifts) & 1).astype(jnp.int32)

"""
### Initializing the `DistributedContractor`
"""

nodes_fn_amp = get_nodes_fn_amp(N_QUBITS_AMP, DEPTH_AMP)

# We need some initial parameters to define the network structure
key = jax.random.PRNGKey(42)
params_circuit_amp = (
    jax.random.normal(key, shape=[N_QUBITS_AMP, DEPTH_AMP, 3], dtype=tc.rdtypestr) * 0.1
)
initial_params_amp = {
    "circuit": params_circuit_amp,
    "amplitude": get_binary_representation(0, N_QUBITS_AMP),
}

print("Initializing DistributedContractor for amplitude calculation...")
# cotengra_options allow fine-tuning of the pathfinding process.
# `target_size` in `slicing_reconf_opts` controls the memory size of each slice.
DC_amp = DistributedContractor(
    nodes_fn=nodes_fn_amp,
    params=initial_params_amp,
    cotengra_options={
        "slicing_reconf_opts": {"target_size": 2**14},
        "max_repeats": 64,
        "progbar": True,
        "minimize": "write",  # Optimizes for memory write operations
        "parallel": 4,
    },
)
# Output:
#   Initializing DistributedContractor for amplitude calculation...

#   F=4.88 C=6.02 S=9.00 P=11.21: 100%|██████████| 64/64 [00:08<00:00,  7.70it/s] 
#   

#   --- Contraction Path Info ---

#   Path found with 1 slices.

#   Arithmetic Intensity (higher is better): 4.94

#   flops (TFlops): 1.7143975128419697e-08

#   write (GB): 0.00011376291513442993

#   size (GB): 3.814697265625e-06

#   -----------------------------

#   

#   Distributing across 4 devices. Each device will sequentially process up to 1 slices.

#   


"""
### Calculating Multiple Amplitudes

"""

n_amp = 10
print("Starting amplitude loop...")
for i in range(n_amp):
    bs_vector = get_binary_representation(i, N_QUBITS_AMP)
    params = {"circuit": params_circuit_amp, "amplitude": bs_vector}

    t0 = time.time()
    amp = DC_amp.value(params)
    t1 = time.time()

    print(
        f"Bitstring: {bs_vector.tolist()} | "
        f"Amp (DC): {amp:.8f} | "
        f"Time: {t1 - t0:.4f} s"
    )
# Output:
#   Starting amplitude loop...

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] | Amp (DC): 0.00353913+0.00129915j | Time: 0.0011 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] | Amp (DC): 0.00360568+0.00364827j | Time: 0.0007 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] | Amp (DC): 0.00032678+0.00287017j | Time: 0.0007 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1] | Amp (DC): 0.00228170+0.00377162j | Time: 0.0006 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] | Amp (DC): 0.00194864+0.00337264j | Time: 0.0006 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1] | Amp (DC): 0.00013618+0.00505736j | Time: 0.0005 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0] | Amp (DC): 0.00230806+0.00277415j | Time: 0.0006 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1] | Amp (DC): 0.00485482+0.00290096j | Time: 0.0005 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] | Amp (DC): 0.00410894+0.00097546j | Time: 0.0005 s

#   Bitstring: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1] | Amp (DC): 0.00449419+0.00358756j | Time: 0.0006 s


"""
## Example 2: Variational Quantum Eigensolver (VQE)

`DistributedContractor` is especially powerful for variational algorithms, where we need to repeatedly compute expectation values and their gradients.

### Defining the `nodes_fn` for Expectation Value
"""

N_QUBITS_VQE = 10
DEPTH_VQE = 4


def get_tfi_mpo(n):
    """Gets the MPO for the 1D Transverse-Field Ising model Hamiltonian."""
    import tensornetwork as tn

    Jx = np.ones(n - 1)
    Bz = -1.0 * np.ones(n)
    tn_mpo = tn.matrixproductstates.mpo.FiniteTFI(Jx, Bz, dtype=np.complex64)
    return tc.quantum.tn2qop(tn_mpo)


def get_nodes_fn_vqe(n, d, mpo):
    """
    The nodes_fn for VQE expectation value.
    It returns the list of tensors for <psi|H|psi>.
    """

    def nodes_fn(params):
        psi = circuit_ansatz(n, d, params).get_quvector()
        expression = psi.adjoint() @ mpo @ psi
        return expression.nodes

    return nodes_fn

"""
### VQE Optimization Loop

"""

tfi_mpo = get_tfi_mpo(N_QUBITS_VQE)
nodes_fn_vqe = get_nodes_fn_vqe(N_QUBITS_VQE, DEPTH_VQE, tfi_mpo)

# Initial parameters for VQE
key = jax.random.PRNGKey(42)
params_vqe = (
    jax.random.normal(key, shape=[N_QUBITS_VQE, DEPTH_VQE, 3], dtype=tc.rdtypestr) * 0.1
)

print("\nInitializing DistributedContractor for VQE...")
DC_vqe = DistributedContractor(
    nodes_fn=nodes_fn_vqe,
    params=params_vqe,
    cotengra_options={
        "slicing_reconf_opts": {
            "target_size": 2**8
        },  # Smaller target size for VQE network
        "max_repeats": 16,
        "progbar": True,
        "minimize": "write",
        "parallel": 4,
    },
)

# Setup Optax optimizer
optimizer = optax.adam(2e-2)
opt_state = optimizer.init(params_vqe)


@K.jit
def opt_update(params, opt_state, grads):
    updates, new_opt_state = optimizer.update(grads, opt_state, params)
    new_params = optax.apply_updates(params, updates)
    return new_params, new_opt_state


n_steps_vqe = 100
print("\nStarting VQE optimization loop...")
for i in range(n_steps_vqe):
    t0 = time.time()
    loss, grads = DC_vqe.value_and_grad(params_vqe)

    params_vqe, opt_state = opt_update(params_vqe, opt_state, grads)
    t1 = time.time()

    if (i + 1) % 10 == 0:
        print(f"Step {i+1:03d} | " f"Loss: {loss:.8f} | " f"Time: {t1 - t0:.4f} s")

print("\nOptimization finished.")
# Output:
#   

#   Initializing DistributedContractor for VQE...

#   F=5.61 C=6.81 S=7.58 P=11.10 $=16.00: 100%|██████████| 16/16 [00:08<00:00,  1.98it/s]

#   

#   --- Contraction Path Info ---

#   Path found with 16 slices.

#   Arithmetic Intensity (higher is better): 4.30

#   flops (TFlops): 9.178620530292392e-08

#   write (GB): 4.3742358684539795e-05

#   size (GB): 1.430511474609375e-06

#   -----------------------------

#   

#   Distributing across 4 devices. Each device will sequentially process up to 4 slices.

#   

#   Starting VQE optimization loop...

#   Step 010 | Loss: -3.29106593 | Time: 0.0022 s

#   Step 020 | Loss: -8.78426552 | Time: 0.0021 s

#   Step 030 | Loss: -10.85906601 | Time: 0.0022 s

#   Step 040 | Loss: -11.47075844 | Time: 0.0020 s

#   Step 050 | Loss: -11.72393227 | Time: 0.0022 s

#   Step 060 | Loss: -11.91652107 | Time: 0.0021 s

#   Step 070 | Loss: -12.04074574 | Time: 0.0020 s

#   Step 080 | Loss: -12.11665630 | Time: 0.0024 s

#   Step 090 | Loss: -12.15851784 | Time: 0.0021 s

#   Step 100 | Loss: -12.17830086 | Time: 0.0022 s

#   

#   Optimization finished.


"""
## Conclusion

The `DistributedContractor` provides a powerful and streamlined interface for scaling up tensor network contractions to multiple devices. By abstracting away the complexities of pathfinding, slicing, and parallel execution, it allows researchers to focus on the physics of their problem while leveraging the full computational power of their hardware. This is particularly advantageous for simulating large quantum circuits and accelerating the convergence of variational quantum algorithms.
"""



================================================
FILE: docs/source/tutorials/dqas.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Differentiable Quantum Architecture Search
"""

"""
## Overview

This tutorial demonstrates how to utilize the advanced computational features provided by TensorCircuit such as ``jit`` and ``vmap`` to super efficiently simulate the differentiable quantum architecture search (DQAS) algorithm, where an ensemble of quantum circuits with different structures can be compiled to simulate at the same time.

[WIP note]
"""

"""
## Setup
"""

import numpy as np
import tensorcircuit as tc
import tensorflow as tf

K = tc.set_backend("tensorflow")
ctype, rtype = tc.set_dtype("complex128")

"""
## Problem Description

The task is to find the state preparation circuit for GHZ state $\vert \text{GHZ}_N\rangle = \frac{1}{\sqrt{2}}\left(\vert 0^N\rangle +\vert 1^N\rangle \right)$. We prepare a gate pool with rx0, rx1, ry0, ry1, rz0, rz1, cnot01, cnot10 for the $N=2$ demo. Amongst the eight gates, six are parameterized.
"""

def rx0(theta):
    return K.kron(
        K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._x_matrix, K.eye(2)
    )


def rx1(theta):
    return K.kron(
        K.eye(2), K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._x_matrix
    )


def ry0(theta):
    return K.kron(
        K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._y_matrix, K.eye(2)
    )


def ry1(theta):
    return K.kron(
        K.eye(2), K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._y_matrix
    )


def rz0(theta):
    return K.kron(
        K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._z_matrix, K.eye(2)
    )


def rz1(theta):
    return K.kron(
        K.eye(2), K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._z_matrix
    )


def cnot01():
    return K.cast(K.convert_to_tensor(tc.gates._cnot_matrix), ctype)


def cnot10():
    return K.cast(
        K.convert_to_tensor(
            np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])
        ),
        ctype,
    )


ops_repr = ["rx0", "rx1", "ry0", "ry1", "rz0", "rz1", "cnot01", "cnot10"]

n, p, ch = 2, 3, 8
# number of qubits, number of layers, size of operation pool

target = tc.array_to_tensor(np.array([1, 0, 0, 1.0]) / np.sqrt(2.0))
# target wavefunction, we here use GHZ2 state as the objective target


def ansatz(params, structures):
    c = tc.Circuit(n)
    params = K.cast(params, ctype)
    structures = K.cast(structures, ctype)
    for i in range(p):
        c.any(
            0,
            1,
            unitary=structures[i, 0] * rx0(params[i, 0])
            + structures[i, 1] * rx1(params[i, 1])
            + structures[i, 2] * ry0(params[i, 2])
            + structures[i, 3] * ry1(params[i, 3])
            + structures[i, 4] * rz0(params[i, 4])
            + structures[i, 5] * rz1(params[i, 5])
            + structures[i, 6] * cnot01()
            + structures[i, 7] * cnot10(),
        )
    s = c.state()
    loss = K.sum(K.abs(target - s))
    return loss


vag1 = K.jit(K.vvag(ansatz, argnums=0, vectorized_argnums=1))

"""
## Probability Ensemble Approach

This approach is more practical and experimental relevant and is the same algorithm described in Ref.1, though we here use advanced vmap to accelerate the simulation of circuits with different structures.
"""

def sampling_from_structure(structures, batch=1):
    prob = K.softmax(K.real(structures), axis=-1)
    return np.array([np.random.choice(ch, p=K.numpy(prob[i])) for i in range(p)])


@K.jit
def best_from_structure(structures):
    return K.argmax(structures, axis=-1)


@K.jit
def nmf_gradient(structures, oh):
    """
    compute the Monte Carlo gradient with respect of naive mean-field probabilistic model
    """
    choice = K.argmax(oh, axis=-1)
    prob = K.softmax(K.real(structures), axis=-1)
    indices = K.transpose(K.stack([K.cast(tf.range(p), "int64"), choice]))
    prob = tf.gather_nd(prob, indices)
    prob = K.reshape(prob, [-1, 1])
    prob = K.tile(prob, [1, ch])

    return tf.tensor_scatter_nd_add(
        tf.cast(-prob, dtype=ctype),
        indices,
        tf.ones([p], dtype=ctype),
    )


nmf_gradient_vmap = K.vmap(nmf_gradient, vectorized_argnums=1)

verbose = False
epochs = 400
batch = 256
lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)
structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.12))
network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))
nnp = K.implicit_randn(stddev=0.02, shape=[p, 6], dtype=rtype)
stp = K.implicit_randn(stddev=0.02, shape=[p, 8], dtype=rtype)
avcost1 = 0
for epoch in range(epochs):  # iteration to update strcuture param
    avcost2 = avcost1
    costl = []
    batched_stuctures = K.onehot(
        np.stack([sampling_from_structure(stp) for _ in range(batch)]), num=8
    )
    infd, gnnp = vag1(nnp, batched_stuctures)
    gs = nmf_gradient_vmap(stp, batched_stuctures)  # \nabla lnp
    gstp = [K.cast((infd[i] - avcost2), ctype) * gs[i] for i in range(infd.shape[0])]
    gstp = K.real(K.sum(gstp, axis=0) / infd.shape[0])
    avcost1 = K.sum(infd) / infd.shape[0]
    nnp = network_opt.update(gnnp, nnp)
    stp = structure_opt.update(gstp, stp)

    if epoch % 40 == 0 or epoch == epochs - 1:
        print("----------epoch %s-----------" % epoch)
        print(
            "batched average loss: ",
            np.mean(avcost1),
        )

        if verbose:
            print(
                "strcuture parameter: \n",
                stp.numpy(),
                "\n network parameter: \n",
                nnp.numpy(),
            )

        cand_preset = best_from_structure(stp)
        print("best candidates so far:", [ops_repr[i] for i in cand_preset])
        print(
            "corresponding weights for each gate:",
            [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],
        )
# Output:
#   WARNING:tensorflow:Using a while_loop for converting GatherNd

#   WARNING:tensorflow:Using a while_loop for converting TensorScatterAdd

#   ----------epoch 0-----------

#   batched average loss:  1.438692604002888

#   best candidates so far: ['cnot01', 'rx0', 'rx1']

#   corresponding weights for each gate: [0.0, -0.049711242696246834, 0.0456804722145847]

#   ----------epoch 40-----------

#   batched average loss:  1.0024311791127296

#   best candidates so far: ['cnot01', 'ry0', 'cnot01']

#   corresponding weights for each gate: [0.0, -0.1351106165832465, 0.0]

#   ----------epoch 80-----------

#   batched average loss:  0.09550699673720528

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.06370593607560585, -0.7355997299177472, 0.0]

#   ----------epoch 120-----------

#   batched average loss:  0.0672150785213724

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.062430880135008346, -0.7343246757666638, 0.0]

#   ----------epoch 160-----------

#   batched average loss:  0.07052086338808516

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.060554804305087445, -0.7324486014485383, 0.0]

#   ----------epoch 200-----------

#   batched average loss:  0.06819711768556835

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.05860750144346523, -0.7305012995010937, 0.0]

#   ----------epoch 240-----------

#   batched average loss:  0.05454652406620351

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.05680703664615186, -0.728700835323507, 0.0]

#   ----------epoch 280-----------

#   batched average loss:  0.047745385543626825

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.05680097807715014, -0.7286947772784904, 0.0]

#   ----------epoch 320-----------

#   batched average loss:  0.039626618064439574

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.05679499116702013, -0.7286887907723886, 0.0]

#   ----------epoch 360-----------

#   batched average loss:  0.036450806118657045

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.056789547021157315, -0.7286833469676062, 0.0]

#   ----------epoch 399-----------

#   batched average loss:  0.012538933640035648

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [-0.06360204206353537, -0.7354958422632526, 0.0]


"""
## Directly Optimize the Structure Parameters

Since we are using numerical simulation anyway, we can directly optimize the structure parameter and omit whether the super circuit is unitary or not, this approach can be faster and more reliable for some scenarios.
"""

def ansatz2(params, structures):
    c = tc.Circuit(n)
    params = K.cast(params, ctype)
    structures = K.softmax(structures, axis=-1)
    structures = K.cast(structures, ctype)
    for i in range(p):
        c.any(
            0,
            1,
            unitary=structures[i, 0] * rx0(params[i, 0])
            + structures[i, 1] * rx1(params[i, 1])
            + structures[i, 2] * ry0(params[i, 2])
            + structures[i, 3] * ry1(params[i, 3])
            + structures[i, 4] * rz0(params[i, 4])
            + structures[i, 5] * rz1(params[i, 5])
            + structures[i, 6] * cnot01()
            + structures[i, 7] * cnot10(),
        )
    s = c.state()
    s /= K.norm(s)
    loss = K.sum(K.abs(target - s))
    return loss


vag2 = K.jit(K.value_and_grad(ansatz2, argnums=(0, 1)))

verbose = True
epochs = 700
lr = tf.keras.optimizers.schedules.ExponentialDecay(0.05, 200, 0.5)
structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.04))
network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))
nnp = K.implicit_randn(stddev=0.02, shape=[p, 6], dtype=rtype)
stp = K.implicit_randn(stddev=0.02, shape=[p, 8], dtype=rtype)
for epoch in range(epochs):
    infd, (gnnp, gstp) = vag2(nnp, stp)

    nnp = network_opt.update(gnnp, nnp)
    stp = structure_opt.update(gstp, stp)
    if epoch % 70 == 0 or epoch == epochs - 1:
        print("----------epoch %s-----------" % epoch)
        print(
            "batched average loss: ",
            np.mean(infd),
        )
        if verbose:
            print(
                "strcuture parameter: \n",
                stp.numpy(),
                "\n network parameter: \n",
                nnp.numpy(),
            )

        cand_preset = best_from_structure(stp)
        print("best candidates so far:", [ops_repr[i] for i in cand_preset])
        print(
            "corresponding weights for each gate:",
            [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],
        )
# Output:
#   ----------epoch 0-----------

#   batched average loss:  1.3046788213442395

#   strcuture parameter: 

#    [[ 0.07621179  0.04934165  0.04669995  0.04737751  0.02036102  0.01170415

#      0.03786593 -0.05644197]

#    [ 0.01168381  0.0561013   0.02979136  0.03134415  0.03763557  0.03739249

#      0.03408754 -0.05335854]

#    [ 0.03540374  0.03219197  0.01680129  0.02014464  0.06939972  0.02393527

#      0.04619596 -0.01844729]] 

#    network parameter: 

#    [[-0.0584098   0.04281717  0.0642035   0.06008445  0.0357175   0.05512457]

#    [-0.07067937  0.04410743  0.03608519  0.03465959  0.02446072  0.06917318]

#    [-0.01337738  0.04776898  0.04278249  0.04917169  0.0495427   0.01059102]]

#   best candidates so far: ['rx0', 'rx1', 'rz0']

#   corresponding weights for each gate: [-0.058409803714939854, 0.04410743113093344, 0.04954270315507654]

#   ----------epoch 70-----------

#   batched average loss:  1.0081966098666586

#   strcuture parameter: 

#    [[-0.91750096  0.35057522  0.32585577  0.37681816  1.77239369  1.7734987

#      1.80143958 -0.38591221]

#    [ 0.30087524  0.28764993  0.36971695  0.36078872  1.79887933  1.47542633

#      1.79490296 -0.38283427]

#    [ 0.29950339  0.32101711 -0.07372448  0.34959339  1.83486426  1.78887106

#      1.81320642 -0.34792317]] 

#    network parameter: 

#    [[ 0.01163284 -0.02749067 -0.00602475  0.46422017 -0.03365732 -0.01443091]

#    [-0.00057541 -0.02624807 -0.03408587  0.43879875 -0.04520759 -0.00055711]

#    [ 0.05673025  0.03099979 -0.02736317  0.45331194 -0.02026327 -0.00559595]]

#   best candidates so far: ['cnot01', 'rz0', 'rz0']

#   corresponding weights for each gate: [0.0, -0.045207589104267296, -0.02026326781055693]

#   ----------epoch 140-----------

#   batched average loss:  0.8049806880722175

#   strcuture parameter: 

#    [[-3.20900567 -2.18126972  1.96173331  0.3704988   0.75310085  2.01979348

#      2.47701794 -0.37965676]

#    [-0.78487034 -1.05072503  0.83960507  0.35409074  1.49913186  0.4284363

#      4.58858068 -0.37664102]

#    [ 0.72348068  0.29661214  0.82121041  0.34328667  4.57946006  3.79373413

#      2.24252671 -0.3416766 ]] 

#    network parameter: 

#    [[-5.93268249e-04 -4.03543595e-02 -1.13260135e+00  4.62883177e-01

#     -3.47753230e-02 -1.57096245e-02]

#    [-1.38210543e-03 -5.03624409e-02  1.02006945e+00  4.37465879e-01

#     -4.64645263e-02 -1.16956649e-03]

#    [-7.80346264e-02  1.90816551e-02  1.09724554e+00  4.51972627e-01

#     -2.15345680e-02 -6.84665987e-03]]

#   best candidates so far: ['cnot01', 'cnot01', 'rz0']

#   corresponding weights for each gate: [0.0, 0.0, -0.02153456797370576]

#   ----------epoch 210-----------

#   batched average loss:  0.041816948869616476

#   strcuture parameter: 

#    [[-3.86458991 -2.84058112  2.62327171  0.26992388  0.09167012  1.35827717

#      1.81549048 -0.56415243]

#    [-1.16314411 -1.7698344   1.49466411 -0.30614419  0.75064439 -0.31409853

#      5.25000534 -0.65623059]

#    [ 1.4704075   0.89799938  2.01474589  2.50046978  4.8946084   4.44647834

#      1.49549043 -0.52420796]] 

#    network parameter: 

#    [[ 0.00716229  0.0950563  -1.62490102  0.60459966 -0.033863   -0.01472524]

#    [ 0.41329341  0.02296645  1.58326833  0.57927215 -0.04604745 -0.05234586]

#    [-0.07409766  0.08796055 -0.2881097  -0.52346262 -0.02053635 -0.00585734]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.624901021386579, 0.0, -0.020536353581361112]

#   ----------epoch 280-----------

#   batched average loss:  0.04771541732805661

#   strcuture parameter: 

#    [[-3.86686803 -2.8436989   2.62698311  0.26657346  0.0879849   1.35457084

#      1.81178177 -0.67868932]

#    [-1.33926046 -1.75120967  1.4909566  -0.18080598  0.75530859 -0.30731494

#      5.25370236 -0.67797002]

#    [ 1.47961761  0.93984054  2.12875762  2.49693907  4.78860574  4.56031727

#      1.50105437 -3.90111934]] 

#    network parameter: 

#    [[ 0.01376387  0.10062571 -1.62306954  0.60290458 -0.0321182  -0.01292187]

#    [ 0.41014329  0.10543278  1.58481867  0.66429872 -0.04488787 -0.0548457 ]

#    [-0.07315826  0.09047629 -0.35272068 -0.52529957 -0.01871782 -0.00404166]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.6230695444527814, 0.0, -0.018717818357922293]

#   ----------epoch 350-----------

#   batched average loss:  0.0484244468649333

#   strcuture parameter: 

#    [[-3.86889078 -2.84635759  2.63008504  0.26367828  0.08490232  1.35147264

#      1.80868181 -0.68180282]

#    [-1.61872015 -1.73606594  1.48784963 -0.16422792  0.75890549 -0.30250011

#      5.25680704 -0.70078063]

#    [ 1.48953381  0.96377125  2.13183873  2.49380904  4.79193079  4.5635497

#      1.5054478  -5.6630325 ]] 

#    network parameter: 

#    [[ 0.02014668  0.10330406 -1.62102814  0.60086296 -0.03016668 -0.01090957]

#    [ 0.40649692  0.12322429  1.58685415  0.67753112 -0.04356077 -0.05752607]

#    [-0.0730366   0.09201651 -0.35067966 -0.52733454 -0.01668937 -0.00201604]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.6210281393366774, 0.0, -0.01668936553578817]

#   ----------epoch 420-----------

#   batched average loss:  0.0490371292665724

#   strcuture parameter: 

#    [[-3.8707214  -2.84868677  2.63274431  0.26116667  0.08225763  1.34881617

#      1.80602401 -0.68447817]

#    [-2.17422677 -1.72638998  1.48517837 -0.14718937  0.76130497 -0.30044208

#      5.25948165 -0.72889795]

#    [ 1.50031397  0.986817    2.13447811  2.49111713  4.79485557  4.56643982

#      1.50815833 -6.42399688]] 

#    network parameter: 

#    [[ 2.66201104e-02  1.04232454e-01 -1.61904956e+00  5.98884773e-01

#     -2.82809595e-02 -8.96136472e-03]

#    [ 4.01114366e-01  1.42266730e-01  1.58916732e+00  6.88026030e-01

#     -4.23682835e-02 -6.01395817e-02]

#    [-7.30273413e-02  9.34775776e-02 -3.48701372e-01 -5.29307121e-01

#     -1.47240949e-02 -5.37945560e-05]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.6190495562985034, 0.0, -0.014724094945736954]

#   ----------epoch 490-----------

#   batched average loss:  0.04976228840362948

#   strcuture parameter: 

#    [[-3.87241212 -2.85077089  2.63506255  0.25896325  0.07995022  1.3465

#      1.80370686 -0.68681685]

#    [-2.94364254 -1.72476469  1.48284202 -0.13112958  0.76233067 -0.30191621

#      5.26182547 -0.76243791]

#    [ 1.5130618   1.0086917   2.13677828  2.4887622   4.79747567  4.56906223

#      1.50813331 -6.85905145]] 

#    network parameter: 

#    [[ 0.03313986  0.1034762  -1.6172524   0.59708835 -0.02657844 -0.00719578]

#    [ 0.39208856  0.1630666   1.59170906  0.69656645 -0.04144895 -0.06259256]

#    [-0.07305937  0.09493765 -0.34690445 -0.53109883 -0.01294077  0.00172629]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.617252401393024, 0.0, -0.012940768995802935]

#   ----------epoch 560-----------

#   batched average loss:  0.05065420046477945

#   strcuture parameter: 

#    [[-3.8739842  -2.85266121  2.6371075   0.25701232  0.0779132   1.34445655

#      1.80166269 -0.68888562]

#    [-3.87431144 -1.73479083  1.48077359 -0.1180455   0.76187864 -0.30734926

#      5.26390441 -0.7996279 ]

#    [ 1.52854329  1.02822466  2.13880714  2.48667702  4.79985267  4.57146409

#      1.50375206 -7.1518221 ]] 

#    network parameter: 

#    [[ 0.03952625  0.09952938 -1.6156793   0.59551608 -0.02510117 -0.0056556 ]

#    [ 0.37625454  0.18679017  1.59450355  0.70332975 -0.04084047 -0.0651834 ]

#    [-0.07341076  0.09584067 -0.3453315  -0.53266713 -0.01138222  0.00328135]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.6156792973551577, 0.0, -0.01138222012800648]

#   ----------epoch 630-----------

#   batched average loss:  0.05169300354431061

#   strcuture parameter: 

#    [[-3.87544608 -2.85438213  2.63892729  0.25527243  0.0760992   1.34263784

#      1.79984346 -0.69073143]

#    [-4.81271904 -1.75890412  1.47892571 -0.1106167   0.75992863 -0.31691451

#      5.26576503 -0.83861308]

#    [ 1.54468553  1.04577833  2.14061233  2.48481415  4.80202942  4.57367936

#      1.49248445 -7.36638329]] 

#    network parameter: 

#    [[ 0.04556425  0.08358514 -1.61433443  0.59417203 -0.02385289 -0.00434529]

#    [ 0.35393466  0.2231968   1.59768193  0.70856295 -0.04056008 -0.06806482]

#    [-0.07428592  0.0956737  -0.3439867  -0.53400786 -0.01005294  0.00460687]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.614334431628148, 0.0, -0.010052938889800701]

#   ----------epoch 699-----------

#   batched average loss:  0.10256012079470897

#   strcuture parameter: 

#    [[-3.787228   -2.7662122   2.55075055  0.34350642  0.16427612  1.43081451

#      1.88802021 -0.60259055]

#    [-5.6073258  -2.03961941  1.56706312 -0.37366319  0.66712936 -0.41905248

#      5.17764218 -0.96685202]

#    [ 1.46844149  0.80497766  2.05243854  2.57293907  4.71424946  4.48595762

#      1.38208153 -7.52953678]] 

#    network parameter: 

#    [[ 4.11010856e-02  9.05029846e-03 -1.60328084e+00  5.83119412e-01

#     -1.28978327e-02  6.67130794e-03]

#    [ 3.37676048e-01  5.40067470e-01  1.61148200e+00  7.22459586e-01

#     -3.43741776e-02 -6.15320200e-02]

#    [-6.44290250e-02  8.86161369e-02 -3.32933747e-01 -5.45057551e-01

#      9.83673124e-04  1.56395047e-02]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.6032808418929712, 0.0, 0.0009836731240883082]


"""
## Final Fine-tune

For the obtained circuit layout we can further adjust the circuit weights to make the objective more close to zero.
"""

chosen_structure = K.onehot(np.array([2, 4, 6]), num=8)
chosen_structure = K.reshape(chosen_structure, [1, p, ch])
chosen_structure
# Output:
#   <tf.Tensor: shape=(1, 3, 8), dtype=float32, numpy=

#   array([[[0., 0., 1., 0., 0., 0., 0., 0.],

#           [0., 0., 0., 0., 1., 0., 0., 0.],

#           [0., 0., 0., 0., 0., 0., 1., 0.]]], dtype=float32)>

network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(1e-3))
nnp = K.implicit_randn(stddev=0.02, shape=[p, 6], dtype=rtype)
verbose = True
epochs = 600
for epoch in range(epochs):
    infd, gnnp = vag1(nnp, chosen_structure)
    nnp = network_opt.update(gnnp, nnp)
    if epoch % 60 == 0 or epoch == epochs - 1:
        print(epoch, "loss: ", K.numpy(infd[0]))
# Output:
#   0 loss:  0.9827084438054802

#   60 loss:  0.9449745688150044

#   120 loss:  0.8850948396917335

#   180 loss:  0.8048454837720991

#   240 loss:  0.706158632509899

#   300 loss:  0.5901794549931197

#   360 loss:  0.45808014651166296

#   420 loss:  0.3113751664914397

#   480 loss:  0.1520672098147883

#   540 loss:  0.0014714944860031312

#   599 loss:  0.0032763286672428237


"""
## References

1. https://arxiv.org/pdf/2010.08561.pdf
"""



================================================
FILE: docs/source/tutorials/dqas_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 可微量子架构搜索
"""

"""
## 概述

本教程演示了如何利用 TensorCircuit 提供的高级计算功能，例如 ``jit`` 和 ``vmap`` 来超级有效地模拟可微量子架构搜索（DQAS）算法，其中具有不同结构的量子电路的集合可以同时编译模拟。
[WIP note]
"""

"""
## 设置
"""

import numpy as np
import tensorcircuit as tc
import tensorflow as tf

K = tc.set_backend("tensorflow")
ctype, rtype = tc.set_dtype("complex128")

"""
## 问题描述

任务是找到 GHZ 状态的状态准备电路 $\vert \text{GHZ}_N\rangle = \frac{1}{\sqrt{2}}\left(\vert 0^N\rangle +\vert 1^N\rangle \right)$。我们为 $N=2$ 演示准备了一个包含 rx0、rx1、ry0、ry1、rz0、rz1、cnot01、cnot10 的门池。 在八个门中，有六个是参数化的。
"""

def rx0(theta):
    return K.kron(
        K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._x_matrix, K.eye(2)
    )


def rx1(theta):
    return K.kron(
        K.eye(2), K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._x_matrix
    )


def ry0(theta):
    return K.kron(
        K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._y_matrix, K.eye(2)
    )


def ry1(theta):
    return K.kron(
        K.eye(2), K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._y_matrix
    )


def rz0(theta):
    return K.kron(
        K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._z_matrix, K.eye(2)
    )


def rz1(theta):
    return K.kron(
        K.eye(2), K.cos(theta) * K.eye(2) + 1.0j * K.sin(theta) * tc.gates._z_matrix
    )


def cnot01():
    return K.cast(K.convert_to_tensor(tc.gates._cnot_matrix), ctype)


def cnot10():
    return K.cast(
        K.convert_to_tensor(
            np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])
        ),
        ctype,
    )


ops_repr = ["rx0", "rx1", "ry0", "ry1", "rz0", "rz1", "cnot01", "cnot10"]

n, p, ch = 2, 3, 8
# 量子比特数、层数、操作池大小

target = tc.array_to_tensor(np.array([1, 0, 0, 1.0]) / np.sqrt(2.0))
# 目标波函数，我们这里使用 GHZ2 状态作为目标函数


def ansatz(params, structures):
    c = tc.Circuit(n)
    params = K.cast(params, ctype)
    structures = K.cast(structures, ctype)
    for i in range(p):
        c.any(
            0,
            1,
            unitary=structures[i, 0] * rx0(params[i, 0])
            + structures[i, 1] * rx1(params[i, 1])
            + structures[i, 2] * ry0(params[i, 2])
            + structures[i, 3] * ry1(params[i, 3])
            + structures[i, 4] * rz0(params[i, 4])
            + structures[i, 5] * rz1(params[i, 5])
            + structures[i, 6] * cnot01()
            + structures[i, 7] * cnot10(),
        )
    s = c.state()
    loss = K.sum(K.abs(target - s))
    return loss


vag1 = K.jit(K.vvag(ansatz, argnums=0, vectorized_argnums=1))

"""
## 概率系综方法

这种方法更加实用和实验相关，并且与参考文献 1 中描述的算法相同，尽管我们在这里使用高级 vmap 来加速具有不同结构的电路的仿真。
"""

def sampling_from_structure(structures, batch=1):
    prob = K.softmax(K.real(structures), axis=-1)
    return np.array([np.random.choice(ch, p=K.numpy(prob[i])) for i in range(p)])


@K.jit
def best_from_structure(structures):
    return K.argmax(structures, axis=-1)


@K.jit
def nmf_gradient(structures, oh):
    """
    根据朴素平均场概率模型计算蒙特卡洛梯度
    """
    choice = K.argmax(oh, axis=-1)
    prob = K.softmax(K.real(structures), axis=-1)
    indices = K.transpose(K.stack([K.cast(tf.range(p), "int64"), choice]))
    prob = tf.gather_nd(prob, indices)
    prob = K.reshape(prob, [-1, 1])
    prob = K.tile(prob, [1, ch])

    return tf.tensor_scatter_nd_add(
        tf.cast(-prob, dtype=ctype),
        indices,
        tf.ones([p], dtype=ctype),
    )


nmf_gradient_vmap = K.vmap(nmf_gradient, vectorized_argnums=1)

verbose = False
epochs = 400
batch = 256
lr = tf.keras.optimizers.schedules.ExponentialDecay(0.06, 100, 0.5)
structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.12))
network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))
nnp = K.implicit_randn(stddev=0.02, shape=[p, 6], dtype=rtype)
stp = K.implicit_randn(stddev=0.02, shape=[p, 8], dtype=rtype)
avcost1 = 0
for epoch in range(epochs):  # 更新结构参数的迭代
    avcost2 = avcost1
    costl = []
    batched_stuctures = K.onehot(
        np.stack([sampling_from_structure(stp) for _ in range(batch)]), num=8
    )
    infd, gnnp = vag1(nnp, batched_stuctures)
    gs = nmf_gradient_vmap(stp, batched_stuctures)  # \nabla lnp
    gstp = [K.cast((infd[i] - avcost2), ctype) * gs[i] for i in range(infd.shape[0])]
    gstp = K.real(K.sum(gstp, axis=0) / infd.shape[0])
    avcost1 = K.sum(infd) / infd.shape[0]
    nnp = network_opt.update(gnnp, nnp)
    stp = structure_opt.update(gstp, stp)

    if epoch % 40 == 0 or epoch == epochs - 1:
        print("----------epoch %s-----------" % epoch)
        print(
            "batched average loss: ",
            np.mean(avcost1),
        )

        if verbose:
            print(
                "strcuture parameter: \n",
                stp.numpy(),
                "\n network parameter: \n",
                nnp.numpy(),
            )

        cand_preset = best_from_structure(stp)
        print("best candidates so far:", [ops_repr[i] for i in cand_preset])
        print(
            "corresponding weights for each gate:",
            [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],
        )
# Output:
#   WARNING:tensorflow:Using a while_loop for converting GatherNd

#   WARNING:tensorflow:Using a while_loop for converting TensorScatterAdd

#   ----------epoch 0-----------

#   batched average loss:  1.486862041224946

#   best candidates so far: ['rz1', 'cnot01', 'rx0']

#   corresponding weights for each gate: [0.04850114379068718, 0.0, 0.05130625869908137]

#   ----------epoch 40-----------

#   batched average loss:  1.0129558433713033

#   best candidates so far: ['rx0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [0.027262624897770482, 0.027810247234826772, 0.0]

#   ----------epoch 80-----------

#   batched average loss:  0.05192747113248927

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7929784361217006, 0.028373579732963325, 0.0]

#   ----------epoch 120-----------

#   batched average loss:  0.031656973667466226

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7917033798904415, 0.02709852348238091, 0.0]

#   ----------epoch 160-----------

#   batched average loss:  0.028017594123095527

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7898273038100041, 0.02406457071696495, 0.0]

#   ----------epoch 200-----------

#   batched average loss:  0.029086134952175734

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7878800008021832, 0.020169898669812416, 0.0]

#   ----------epoch 240-----------

#   batched average loss:  0.02272153644755242

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7860795359107594, 0.016568960455492835, 0.0]

#   ----------epoch 280-----------

#   batched average loss:  0.019205161854778285

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7854269350973763, 0.013422528595912621, 0.0]

#   ----------epoch 320-----------

#   batched average loss:  0.015424930560900666

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7854255410759101, 0.010762703562019423, 0.0]

#   ----------epoch 360-----------

#   batched average loss:  0.012287067999120332

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7854212965432693, 0.008565353750462018, 0.0]

#   ----------epoch 399-----------

#   batched average loss:  0.009789006724779316

#   best candidates so far: ['ry0', 'rx1', 'cnot01']

#   corresponding weights for each gate: [-0.7922338874583758, -4.046275452326831e-05, 0.0]


"""
## 直接优化结构参数

无论如何，由于我们是用数值模拟，所以可以直接优化结构参数，省略超级电路是否是幺正的，这种方法在某些场景下可以更快更可靠。
"""

def ansatz2(params, structures):
    c = tc.Circuit(n)
    params = K.cast(params, ctype)
    structures = K.softmax(structures, axis=-1)
    structures = K.cast(structures, ctype)
    for i in range(p):
        c.any(
            0,
            1,
            unitary=structures[i, 0] * rx0(params[i, 0])
            + structures[i, 1] * rx1(params[i, 1])
            + structures[i, 2] * ry0(params[i, 2])
            + structures[i, 3] * ry1(params[i, 3])
            + structures[i, 4] * rz0(params[i, 4])
            + structures[i, 5] * rz1(params[i, 5])
            + structures[i, 6] * cnot01()
            + structures[i, 7] * cnot10(),
        )
    s = c.state()
    s /= K.norm(s)
    loss = K.sum(K.abs(target - s))
    return loss


vag2 = K.jit(K.value_and_grad(ansatz2, argnums=(0, 1)))

verbose = True
epochs = 700
lr = tf.keras.optimizers.schedules.ExponentialDecay(0.05, 200, 0.5)
structure_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(0.04))
network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))
nnp = K.implicit_randn(stddev=0.02, shape=[p, 6], dtype=rtype)
stp = K.implicit_randn(stddev=0.02, shape=[p, 8], dtype=rtype)
for epoch in range(epochs):
    infd, (gnnp, gstp) = vag2(nnp, stp)

    nnp = network_opt.update(gnnp, nnp)
    stp = structure_opt.update(gstp, stp)
    if epoch % 70 == 0 or epoch == epochs - 1:
        print("----------epoch %s-----------" % epoch)
        print(
            "batched average loss: ",
            np.mean(infd),
        )
        if verbose:
            print(
                "strcuture parameter: \n",
                stp.numpy(),
                "\n network parameter: \n",
                nnp.numpy(),
            )

        cand_preset = best_from_structure(stp)
        print("best candidates so far:", [ops_repr[i] for i in cand_preset])
        print(
            "corresponding weights for each gate:",
            [K.numpy(nnp[j, i]) if i < 6 else 0.0 for j, i in enumerate(cand_preset)],
        )
# Output:
#   ----------epoch 0-----------

#   batched average loss:  1.3024341605187928

#   strcuture parameter: 

#    [[ 0.00265054  0.04495954  0.05265605  0.04751008  0.03309468  0.02743368

#      0.03382795 -0.06647121]

#    [ 0.03544281  0.03207712  0.03629811  0.0266235   0.03264895  0.03198189

#      0.03505167 -0.03449981]

#    [ 0.0304648   0.07042194  0.03075206  0.02515865  0.02984363  0.00955019

#      0.07527341 -0.05831911]] 

#    network parameter: 

#    [[-0.0380125   0.0688923   0.04393423  0.04205065  0.06243917  0.03672062]

#    [-0.05277717  0.04834309  0.05176114  0.07030034  0.02983666  0.04821408]

#    [-0.04095011  0.0393773   0.03383929  0.06559557  0.03458135  0.02436751]]

#   best candidates so far: ['ry0', 'ry0', 'cnot01']

#   corresponding weights for each gate: [0.043934227235354874, 0.05176113831452516, 0.0]

#   ----------epoch 70-----------

#   batched average loss:  1.0078726220234666

#   strcuture parameter: 

#    [[ 0.34119556  0.37154559  0.2781548   0.37689646  1.79624262  1.78939153

#      1.79791154 -0.3958881 ]

#    [-1.04375011 -0.1280161  -0.98656309  0.35601588  1.79624062  1.7944131

#      1.80069633 -0.36391841]

#    [ 0.05278476  0.40486479  0.33074328  0.35455104  1.79289574  1.77260117

#      1.83987217 -0.38773815]] 

#    network parameter: 

#    [[ 0.03213363 -0.00113532 -0.02629044  0.44615806 -0.00711866 -0.03271277]

#    [ 0.01737624 -0.02169861 -0.01841347  0.47440825  0.01052075 -0.02137655]

#    [ 0.02919352 -0.03064528 -0.03628025  0.46970421 -0.03434029 -0.04519493]]

#   best candidates so far: ['cnot01', 'cnot01', 'cnot01']

#   corresponding weights for each gate: [0.0, 0.0, 0.0]

#   ----------epoch 140-----------

#   batched average loss:  0.8974790925982725

#   strcuture parameter: 

#    [[-0.60734424 -0.71178177  1.75016478  0.37059946  2.29304101  1.40087053

#      2.75041722 -0.4812161 ]

#    [-3.20848853 -2.62803529 -1.0799243   0.34964202  1.75222537  0.96340588

#      4.59441388 -0.44922864]

#    [-1.14052853 -0.10976557  0.9998582   0.34848638  3.81458301  3.38821681

#      2.51193413 -0.38172792]] 

#    network parameter: 

#    [[-0.03052873 -0.00288839 -1.08840853  0.51536679 -0.00833568 -0.03375284]

#    [ 0.0216315  -0.02917968  0.8216509   0.54353084  0.00946658 -0.02120069]

#    [ 0.02581133 -0.03281417  0.94900358  0.46842014 -0.0355813  -0.04637678]]

#   best candidates so far: ['cnot01', 'cnot01', 'rz0']

#   corresponding weights for each gate: [0.0, 0.0, -0.03558129647764995]

#   ----------epoch 210-----------

#   batched average loss:  0.06833171337421318

#   strcuture parameter: 

#    [[-1.46101866 -1.56562139  2.60460926  0.36697705  1.43861808  0.54649088

#      1.8959831  -0.6664916 ]

#    [-2.8599476  -3.71447893 -0.1444609  -0.31801788  0.70398108  0.12590137

#      5.44896172 -0.64032593]

#    [-0.29063004  0.46966802  1.85427644  0.44130728  4.56540152  4.13933904

#      1.26524942 -0.47396024]] 

#    network parameter: 

#    [[-0.80254223 -0.07223631 -1.72113169  0.65719836 -0.00717114 -0.03248183]

#    [ 0.86880067 -0.98161163  1.73279897  0.68543186  0.57173565  0.81901641]

#    [ 0.41384514  0.03460214  1.4409846   0.26020133 -0.03450978 -0.04528429]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.7211316854515077, 0.0, -0.03450978363305436]

#   ----------epoch 280-----------

#   batched average loss:  0.07287093721912785

#   strcuture parameter: 

#    [[-1.46462962 -1.5694426   2.60832482  0.36325644  1.43489835  0.54276147

#      1.89226573 -0.66280103]

#    [-1.57089047 -3.66510778 -0.14837051 -0.32242832  0.70013882  0.1239224

#      5.45278654 -0.63670874]

#    [-0.28665073  0.50527464  1.85793113  0.43721505  4.56217177  4.13630294

#      1.26107282 -0.47016776]] 

#    network parameter: 

#    [[-0.80172748 -0.0712512  -1.71930255  0.6591615  -0.00531003 -0.030584  ]

#    [ 0.8833277  -1.26937984  1.73185803  0.68715766  0.57394473  0.8184381 ]

#    [ 0.41194181  0.03270603  1.4387577   0.26174242 -0.03266689 -0.04343292]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.7193025540909423, 0.0, -0.032666892917796356]

#   ----------epoch 350-----------

#   batched average loss:  0.0763633455796077

#   strcuture parameter: 

#    [[-1.46759968 -1.57262214  2.61142979  0.36014718  1.43179046  0.53964649

#      1.88915951 -0.65972411]

#    [-1.34936013 -3.5935837  -0.15090151 -0.32639326  0.69677033  0.10483803

#      5.45606073 -0.63438681]

#    [-0.28282805  0.54032205  1.86109945  0.43217854  4.55947529  4.13345958

#      1.25730198 -0.46703484]] 

#    network parameter: 

#    [[-0.80018367 -0.06928467 -1.71726239  0.66128651 -0.00323225 -0.0284642 ]

#    [ 0.88958794 -1.36344637  1.73074888  0.68928184  0.57647947  0.81753049]

#    [ 0.40992401  0.03064141  1.43664452  0.26328958 -0.03061012 -0.04136483]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.7172623928298167, 0.0, -0.030610123400026196]

#   ----------epoch 420-----------

#   batched average loss:  0.07933778661926846

#   strcuture parameter: 

#    [[-1.47009725 -1.57533451  2.61409128  0.35748195  1.42912701  0.53697792

#      1.88649718 -0.65709335]

#    [-1.19469005 -3.49291914 -0.15236124 -0.32995762  0.69380539  0.07513886

#      5.45890732 -0.63296266]

#    [-0.27952556  0.57900165  1.86385667  0.42698093  4.55720421  4.13093395

#      1.25401354 -0.46439162]] 

#    network parameter: 

#    [[-7.98493487e-01 -6.71330183e-02 -1.71528444e+00  6.63308869e-01

#     -1.20663132e-03 -2.63889444e-02]

#    [ 8.92122476e-01 -1.41065660e+00  1.72949508e+00  6.91390756e-01

#      5.79085213e-01  8.16347571e-01]

#    [ 4.07963360e-01  2.86428770e-02  1.43467895e+00  2.64831277e-01

#     -2.86101270e-02 -3.93481586e-02]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.7152844386878365, 0.0, -0.028610127013962768]

#   ----------epoch 490-----------

#   batched average loss:  0.08254545024351018

#   strcuture parameter: 

#    [[-1.47222071e+00 -1.57769163e+00  2.61641114e+00  3.55158679e-01

#      1.42680584e+00  5.34652980e-01  1.88417675e+00 -6.54806876e-01]

#    [-9.73250463e-01 -3.16903830e+00 -3.18559749e-03 -3.33158739e-01

#      8.55776706e-01  5.24787309e-02  5.46137377e+00 -4.79944986e-01]

#    [-4.31555592e-01  6.29503046e-01  1.86558679e+00  4.22659216e-01

#      4.55614569e+00  4.13032405e+00  1.25003608e+00 -4.62133108e-01]] 

#    network parameter: 

#    [[-7.96868537e-01 -6.41205534e-02 -1.71348758e+00  6.65124861e-01

#      7.01296452e-04 -2.44123401e-02]

#    [ 8.95067083e-01 -1.46634629e+00  1.72823707e+00  6.93124762e-01

#      6.19045003e-01  8.16911575e-01]

#    [ 3.68577179e-01 -1.06859075e-02  1.43203849e+00  2.65575517e-01

#     -2.67782067e-02 -3.74965029e-02]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.7134875797200289, 0.0, -0.026778206681234634]

#   ----------epoch 560-----------

#   batched average loss:  0.08507962845391319

#   strcuture parameter: 

#    [[-1.47404605e+00 -1.57976616e+00  2.61845726e+00  3.53109374e-01

#      1.42475889e+00  5.32603322e-01  1.88213024e+00 -6.52796788e-01]

#    [-9.74391719e-01 -2.60249357e+00 -5.45311032e-03 -3.36090881e-01

#      8.87457853e-01  5.09795396e-02  5.46352386e+00 -4.77818480e-01]

#    [-4.31981604e-01  7.01277774e-01  1.86614168e+00  4.17460986e-01

#      4.55685554e+00  4.13219105e+00  1.24506389e+00 -4.60189080e-01]] 

#    network parameter: 

#    [[-0.7953961  -0.05746862 -1.7119146   0.66669705  0.00291374 -0.0224405 ]

#    [ 0.90083188 -1.50990732  1.72710663  0.69459133  0.61788545  0.82003669]

#    [ 0.37017675 -0.00904591  1.42870116  0.26514657 -0.02513608 -0.03583994]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.711914597657612, 0.0, -0.025136080292754256]

#   ----------epoch 630-----------

#   batched average loss:  0.0874301658835076

#   strcuture parameter: 

#    [[-1.47562848 -1.58159874  2.62027778  0.35128611  1.42293816  0.53078116

#      1.88030961 -0.65101441]

#    [-0.97538969 -2.02211517 -0.00750621 -0.33876198  0.90002101  0.04975063

#      5.46545544 -0.47588949]

#    [-0.43177273  0.78495553  1.86618092  0.41182654  4.55815983  4.13468705

#      1.24131582 -0.45851101]] 

#    network parameter: 

#    [[-0.79411737 -0.03262083 -1.71056973  0.66801607  0.00931137 -0.02061058]

#    [ 0.90559687 -1.52299082  1.72605402  0.69584073  0.61673658  0.82270295]

#    [ 0.37157164 -0.00763778  1.42576784  0.26501218 -0.02367246 -0.03437195]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.7105697331825656, 0.0, -0.02367245788118412]

#   ----------epoch 699-----------

#   batched average loss:  0.07954282768319362

#   strcuture parameter: 

#    [[-1.38724436 -1.4934299   2.53210121  0.43945868  1.51111454  0.61895662

#      1.96848611 -0.7392194 ]

#    [-0.88692788 -1.19022046  0.08057521 -0.25156948  0.81347111 -0.04509805

#      5.3775582  -0.56406862]

#    [-0.34354263  0.94817808  1.77696399  0.48982178  4.64871759  4.22638916

#      1.32761078 -0.54681732]] 

#    network parameter: 

#    [[-0.78310707  0.01573153 -1.69951606  0.67901289  0.0447718  -0.00935101]

#    [ 0.91920164 -1.51646333  1.7152693   0.70682227  0.60494688  0.8352184 ]

#    [ 0.38342071  0.00423554  1.41389299  0.27478806 -0.01255172 -0.02324825]]

#   best candidates so far: ['ry0', 'cnot01', 'rz0']

#   corresponding weights for each gate: [-1.6995160647885235, 0.0, -0.01255171733214709]


"""
## 最后的微调

对于获得的电路布局，我们可以进一步调整电路权重，使目标函数更接近于零。
"""

chosen_structure = K.onehot(np.array([2, 4, 6]), num=8)
chosen_structure = K.reshape(chosen_structure, [1, p, ch])
chosen_structure
# Output:
#   <tf.Tensor: shape=(1, 3, 8), dtype=float32, numpy=

#   array([[[0., 0., 1., 0., 0., 0., 0., 0.],

#           [0., 0., 0., 0., 1., 0., 0., 0.],

#           [0., 0., 0., 0., 0., 0., 1., 0.]]], dtype=float32)>

network_opt = tc.backend.optimizer(tf.keras.optimizers.Adam(1e-3))
nnp = K.implicit_randn(stddev=0.02, shape=[p, 6], dtype=rtype)
verbose = True
epochs = 600
for epoch in range(epochs):
    infd, gnnp = vag1(nnp, chosen_structure)
    nnp = network_opt.update(gnnp, nnp)
    if epoch % 60 == 0 or epoch == epochs - 1:
        print(epoch, "loss: ", K.numpy(infd[0]))
# Output:
#   0 loss:  1.004872758871745

#   60 loss:  0.9679200431227549

#   120 loss:  0.9091748060127385

#   180 loss:  0.8302632245154631

#   240 loss:  0.73297561645977

#   300 loss:  0.6183436622858383

#   360 loss:  0.4874390992051161

#   420 loss:  0.34168453047914643

#   480 loss:  0.18299822849737177

#   540 loss:  0.013923344772669728

#   599 loss:  0.0016133833518836463


"""
## 参考资料

1. https://arxiv.org/pdf/2010.08561.pdf
"""



================================================
FILE: docs/source/tutorials/fermion_gaussian_states.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Fermion Gaussian State (FGS) Simulator

This tutorial demonstrates how to use the Fermion Gaussian State (FGS) simulator implemented in `tensorcircuit-ng`. The FGS simulator allows for efficient simulation of non-interacting fermionic systems, which is particularly useful for studying free fermions on lattices.

## Introduction

The FGS simulator efficiently handles systems governed by quadratic Hamiltonians of the form:
$$H = \sum_{ij} H_{ij} c_i^\dagger c_j + \frac{1}{2} \sum_{ij} (H_{ij}^{(2)} c_i^\dagger c_j^\dagger + h.c.)$$

Instead of working with the full $2^N$-dimensional Hilbert space, the FGS simulator uses the correlation matrix formalism which scales polynomially with system size.

## Setup
"""

import numpy as np
import tensorcircuit as tc

# Set the backend (using numpy for this tutorial)
tc.set_backend("numpy")
tc.set_dtype("complex128")
# Output:
#   ('complex128', 'float64')

"""
## Creating an FGS Simulator Instance

We can initialize an FGS simulator in several ways:

1. By specifying occupied sites in a product state
2. From the groudn state of a given Hamiltonian
3. Directly with the alpha matrix
"""

# Method 1: Initialize with occupied sites
# Create a 4-site system with sites 0 and 2 occupied
sim1 = tc.FGSSimulator(L=4, filled=[0, 2])
print("Initialized FGS with filled sites [0, 2]")

# Method 2: Initialize from a Hamiltonian (ground state)
# Create a simple hopping Hamiltonian
L = 4
hc = np.zeros([2 * L, 2 * L])
# Add hopping terms between neighboring sites
for i in range(L - 1):
    # chi * (c_i^\dagger c_j + h.c.)
    hc[i, i + 1] = 1.0
    hc[i + L + 1, i + L] = -1.0

sim2 = tc.FGSSimulator(L=4, hc=hc)
print("Initialized FGS from Hamiltonian ground states")

# Check the alpha matrix of the first simulator
alpha = sim1.get_alpha()
print(f"Alpha matrix shape: {alpha.shape}")
# Output:
#   Initialized FGS with filled sites [0, 2]

#   Initialized FGS from Hamiltonian ground states

#   Alpha matrix shape: (8, 4)


"""
## Single-particle Green's Functions

We can compute correlation functions, which are related to single-particle Green's functions:
$$C_{ij} = \langle c_i^\dagger c_j \rangle$$
"""

# Get the correlation matrix
cmatrix = sim1.get_cmatrix()
print(f"Correlation matrix shape: {cmatrix.shape}")

# Check occupation numbers (diagonal elements)
print("Occupation numbers:")
for i in range(sim1.L):
    print(f"Site {i}: {1-cmatrix[i, i].real:.3f}")

# Compute off-diagonal correlations
print("\nSelected off-diagonal correlations:")
print(f"<c_0 c_0^†> = {sim1.expectation_2body(0, 0):.3f}")
print(f"<c_0 c_1^†> = {sim1.expectation_2body(0, 1):.3f}")
print(f"<c_2 c_3^†> = {sim1.expectation_2body(2, 3):.3f}")
# Output:
#   Correlation matrix shape: (8, 8)

#   Occupation numbers:

#   Site 0: 1.000

#   Site 1: 0.000

#   Site 2: 1.000

#   Site 3: 0.000

#   

#   Selected off-diagonal correlations:

#   <c_0 c_0^†> = 0.000+0.000j

#   <c_0 c_1^†> = 0.000+0.000j

#   <c_2 c_3^†> = 0.000+0.000j


"""
## Time Evolution

The FGS simulator supports evolution under quadratic Hamiltonians:
1. Hopping terms: $\chi c_i^\dagger c_j + h.c.$
2. Chemical potential terms: $\chi c_i^\dagger c_i$
3. Superconducting pairing terms: $\chi c_i^\dagger c_j^\dagger + h.c.$

Let's demonstrate hopping:
"""

# Create a new simulator
sim = tc.FGSSimulator(L=4, filled=[0])

print("Initial state:")
cmatrix_init = sim.get_cmatrix()
for i in range(sim.L):
    print(f"Site {i} occupation: {1-cmatrix_init[i, i].real:.3f}")

# Apply hopping between sites 0 and 1 with strength 1.0 for time π/2
# This should transfer the fermion from site 0 to site 1
sim.evol_hp(0, 1, np.pi)

print("\nAfter hopping evolution:")
cmatrix_final = sim.get_cmatrix()
for i in range(sim.L):
    print(f"Site {i} occupation: {1-cmatrix_final[i, i].real:.3f}")
# Output:
#   Initial state:

#   Site 0 occupation: 1.000

#   Site 1 occupation: 0.000

#   Site 2 occupation: 0.000

#   Site 3 occupation: 0.000

#   

#   After hopping evolution:

#   Site 0 occupation: 0.000

#   Site 1 occupation: 1.000

#   Site 2 occupation: 0.000

#   Site 3 occupation: 0.000


"""
## Entanglement Measures

The FGS simulator can efficiently compute entanglement measures like von Neumann entropy and Renyi entropy:
"""

# Create a simple entangled state
sim_ent = tc.FGSSimulator(L=4, filled=[0, 2])
# Apply a hopping that creates entanglement
sim_ent.evol_hp(0, 1, np.pi / 4)

# Compute entanglement entropy for different subsystems
# Tracing out sites [2, 3] means we look at the entanglement of sites [0, 1]
entropy_01 = sim_ent.entropy([2, 3])
print(f"Entanglement entropy of sites [0,1]: {entropy_01.real:.6f}")

# Tracing out sites [1, 2, 3] means we look at the entanglement of site [0]
entropy_0 = sim_ent.entropy([1, 2, 3])
print(f"Entanglement entropy of site [0]: {entropy_0.real:.6f}")

# Compute Renyi entropy (n=2) for the same subsystems
renyi_01 = sim_ent.renyi_entropy(2, [2, 3])
print(f"Renyi-2 entropy of sites [0,1]: {renyi_01.real:.6f}")

renyi_0 = sim_ent.renyi_entropy(2, [1, 2, 3])
print(f"Renyi-2 entropy of site [0]: {renyi_0.real:.6f}")
# Output:
#   Entanglement entropy of sites [0,1]: -0.000000

#   Entanglement entropy of site [0]: 0.416496

#   Renyi-2 entropy of sites [0,1]: -0.000000

#   Renyi-2 entropy of site [0]: 0.287682


"""
## Measurements and Post-selection

The FGS simulator supports both projective measurements and post-selection:
"""

# Create a superposition state
sim_meas = tc.FGSSimulator(L=2, filled=[0])
# Put site 0 in an equal superposition of occupied and unoccupied
# This is a simplified example - in practice, creating such states requires specific evolutions
sim_meas.evol_hp(0, 1, np.pi / 4)

print("Before measurement:")
cmatrix = sim_meas.get_cmatrix()
for i in range(sim_meas.L):
    print(f"Site {i} occupation probability: {1-cmatrix[i, i].real:.3f}")

# Simulate a measurement on site 0 with a random outcome
# In practice, you would use a random number generator
# Here we manually specify the outcome for reproducibility
outcome = sim_meas.cond_measure(0, status=0.1)  # Should likely result in 0 (unoccupied)
print(f"\nMeasurement outcome for site 0: {outcome}")

print("After measurement:")
cmatrix_post = sim_meas.get_cmatrix()
for i in range(sim_meas.L):
    print(f"Site {i} occupation probability: {1-cmatrix_post[i, i].real:.3f}")

# Demonstrate post-selection (conditioning on a specific outcome)
sim_post = tc.FGSSimulator(L=2, filled=[0])
sim_post.evol_hp(0, 1, np.pi / 4)

print("\nBefore post-selection:")
print(f"Site 0 occupation: {1-sim_post.get_cmatrix()[0, 0].real:.3f}")

# Post-select on site 0 being occupied (keep=1)
sim_post.post_select(0, keep=1)
print("After post-selecting site 0 as occupied:")
print(f"Site 0 occupation: {1-sim_post.get_cmatrix()[0, 0].real:.3f}")
# Output:
#   Before measurement:

#   Site 0 occupation probability: 0.854

#   Site 1 occupation probability: 0.146

#   

#   Measurement outcome for site 0: 0.0

#   After measurement:

#   Site 0 occupation probability: 0.000

#   Site 1 occupation probability: 1.000

#   

#   Before post-selection:

#   Site 0 occupation: 0.854

#   After post-selecting site 0 as occupied:

#   Site 0 occupation: 1.000


"""
## Advanced Example: Kitaev Chain

Let's simulate a simple Kitaev chain, which includes both hopping and pairing terms:
"""

def kitaev_chain(L, mu, Delta, J):
    """
    Create the Hamiltonian matrix for a Kitaev chain

    H = -J Σ (c_i^† c_{i+1} + h.c.) - μ Σ c_i^† c_i + Δ Σ (c_i c_{i+1} + h.c.)
    """
    hc = np.zeros([2 * L, 2 * L], dtype=complex)

    # Chemical potential term
    for i in range(L):
        hc[i, i] = -mu
        hc[i + L, i + L] = mu

    # Hopping terms
    for i in range(L - 1):
        hc[i, i + 1] = -J
        hc[i + L + 1, i + L] = J

    # Pairing terms
    for i in range(L - 1):
        hc[i, i + 1 + L] = Delta
        hc[i + 1, i + L] = -Delta
        hc[i + L + 1, i] = Delta
        hc[i + L, i + 1] = -Delta

    return hc


# Parameters for the Kitaev chain
L = 6
mu = 1.0  # Chemical potential
Delta = 0.5  # Pairing amplitude
J = 1.0  # Hopping amplitude

# Create the Hamiltonian
kitaev_hc = kitaev_chain(L, mu, Delta, J)

# Initialize the ground state of the Kitaev chain
sim_kitaev = tc.FGSSimulator(L=L, hc=kitaev_hc)

print("Kitaev chain ground state:")
cmatrix = sim_kitaev.get_cmatrix()
print("Site occupation numbers:")
for i in range(L):
    print(f"  Site {i}: {1-cmatrix[i, i].real:.4f}")

# Calculate entanglement entropy for half the chain
half_chain_entropy = sim_kitaev.entropy(list(range(L // 2, L)))
print(f"\nEntanglement entropy of half the chain: {half_chain_entropy.real:.6f}")
# Output:
#   Kitaev chain ground state:

#   Site occupation numbers:

#     Site 0: 0.8166

#     Site 1: 0.8837

#     Site 2: 0.8746

#     Site 3: 0.8746

#     Site 4: 0.8837

#     Site 5: 0.8166

#   

#   Entanglement entropy of half the chain: 0.349326


"""
## Conclusion

This tutorial demonstrated the main features of the FGS simulator:

1. Initialization methods
2. Computation of correlation functions
3. Time evolution under quadratic Hamiltonians
4. Entanglement measures
5. Measurements and post-selection
6. Application to a physical model (Kitaev chain)

The FGS simulator provides an efficient way to study non-interacting fermionic systems, avoiding the exponential cost of full Hilbert space simulation while still capturing important physical properties like entanglement and correlation functions.
"""



================================================
FILE: docs/source/tutorials/gradient_benchmark.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Gradient Evaluation Efficiency Comparison
"""

"""
## Overview

In this tutorial, we compare the efficiency of gradient and gradient-like object (such as quantum Fisher information) evaluation via automatical differentiation framework provided by TensorCircuit and the traditional parameter shift framework provided by Qiskit
"""

"""
## Setup

We import necessary packages and modules from Qiskit and TensorCircuit
"""

import time
import numpy as np
from functools import reduce
from operator import xor

from qiskit.opflow import I, X, Z, StateFn, CircuitStateFn, SummedOp
from qiskit.circuit import QuantumCircuit, ParameterVector
from scipy.optimize import minimize
from qiskit.opflow.gradients import Gradient, QFI, Hessian

import tensorcircuit as tc
from tensorcircuit import experimental

"""
## Qiskit Gradient Framework Benchmark

Since Qiskit is **TOO** slow in terms of gradient evaluation, we use small systems to do the benchmark in Jupyter to save time,
for larger size and deep circuits, the efficiency difference will become more evident.

The three gradient like tasks are Gradient, Quantum Fisher Information(QFI), and Hessian evaluation.
"""

def benchmark(f, *args, trials=10):
    time0 = time.time()
    r = f(*args)
    time1 = time.time()
    for _ in range(trials):
        r = f(*args)
    time2 = time.time()
    if trials > 0:
        time21 = (time2 - time1) / trials
    else:
        time21 = 0
    ts = (time1 - time0, time21)
    print("staging time: %.6f s" % ts[0])
    if trials > 0:
        print("running time: %.6f s" % ts[1])
    return r, ts


def grad_qiskit(n, l, trials=2):
    hamiltonian = reduce(xor, [X for _ in range(n)])
    wavefunction = QuantumCircuit(n)
    params = ParameterVector("theta", length=3 * n * l)
    for j in range(l):
        for i in range(n - 1):
            wavefunction.cnot(i, i + 1)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i], i)
        for i in range(n):
            wavefunction.rz(params[3 * n * j + i + n], i)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i + 2 * n], i)

    # Define the expectation value corresponding to the energy
    op = ~StateFn(hamiltonian) @ StateFn(wavefunction)
    grad = Gradient().convert(operator=op, params=params)

    def get_grad_qiskit(values):
        value_dict = {params: values}
        grad_result = grad.assign_parameters(value_dict).eval()
        return grad_result

    return benchmark(get_grad_qiskit, np.ones([3 * n * l]), trials=trials)


def qfi_qiskit(n, l, trials=0):
    hamiltonian = reduce(xor, [X for _ in range(n)])
    wavefunction = QuantumCircuit(n)
    params = ParameterVector("theta", length=3 * n * l)
    for j in range(l):
        for i in range(n - 1):
            wavefunction.cnot(i, i + 1)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i], i)
        for i in range(n):
            wavefunction.rz(params[3 * n * j + i + n], i)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i + 2 * n], i)

    nat_grad = QFI().convert(operator=StateFn(wavefunction), params=params)

    def get_qfi_qiskit(values):
        value_dict = {params: values}
        grad_result = nat_grad.assign_parameters(value_dict).eval()
        return grad_result

    return benchmark(get_qfi_qiskit, np.ones([3 * n * l]), trials=trials)


def hessian_qiskit(n, l, trials=0):
    hamiltonian = reduce(xor, [X for _ in range(n)])
    wavefunction = QuantumCircuit(n)
    params = ParameterVector("theta", length=3 * n * l)
    for j in range(l):
        for i in range(n - 1):
            wavefunction.cnot(i, i + 1)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i], i)
        for i in range(n):
            wavefunction.rz(params[3 * n * j + i + n], i)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i + 2 * n], i)

    # Define the expectation value corresponding to the energy
    op = ~StateFn(hamiltonian) @ StateFn(wavefunction)
    grad = Hessian().convert(operator=op, params=params)

    def get_hs_qiskit(values):
        value_dict = {params: values}
        grad_result = grad.assign_parameters(value_dict).eval()
        return grad_result

    return benchmark(get_hs_qiskit, np.ones([3 * n * l]), trials=trials)

g0, _ = grad_qiskit(6, 3)  # gradient
# Output:
#   staging time: 1.665786 s

#   running time: 1.474930 s


qfi0, _ = qfi_qiskit(6, 3)  # QFI
# Output:
#   staging time: 47.131374 s


hs0, _ = hessian_qiskit(6, 3)  # Hessian
# Output:
#   staging time: 80.495983 s


"""
## TensorCircuit Automatic Differentiation Benchmark

We benchmark on the same problems defined in the Qiskit part above, and we can see the speed boost!
In fact, for a moderate 10-qubit 4-blocks system, QFI evaluation is accelerated more than $10^6$ times!
(Note how staging time for jit can be amortized and only running time relevant. In the Qiskit case, there is no jit and thus the running time is the same as the staging time.)
"""

def grad_tc(n, l, trials=10):
    def f(params):
        c = tc.Circuit(n)
        for j in range(l):
            for i in range(n - 1):
                c.cnot(i, i + 1)
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i])
            for i in range(n):
                c.rz(i, theta=params[3 * n * j + i + n])
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i + 2 * n])
        return tc.backend.real(c.expectation(*[[tc.gates.x(), [i]] for i in range(n)]))

    get_grad_tc = tc.backend.jit(tc.backend.grad(f))
    return benchmark(get_grad_tc, tc.backend.ones([3 * n * l], dtype="float32"))


def qfi_tc(n, l, trials=10):
    def s(params):
        c = tc.Circuit(n)
        for j in range(l):
            for i in range(n - 1):
                c.cnot(i, i + 1)
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i])
            for i in range(n):
                c.rz(i, theta=params[3 * n * j + i + n])
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i + 2 * n])
        return c.state()

    get_qfi_tc = tc.backend.jit(experimental.qng(s, mode="fwd"))
    return benchmark(get_qfi_tc, tc.backend.ones([3 * n * l], dtype="float32"))


def hessian_tc(n, l, trials=10):
    def f(params):
        c = tc.Circuit(n)
        for j in range(l):
            for i in range(n - 1):
                c.cnot(i, i + 1)
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i])
            for i in range(n):
                c.rz(i, theta=params[3 * n * j + i + n])
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i + 2 * n])
        return tc.backend.real(c.expectation(*[[tc.gates.x(), [i]] for i in range(n)]))

    get_hs_tc = tc.backend.jit(tc.backend.hessian(f))
    return benchmark(get_hs_tc, tc.backend.ones([3 * n * l], dtype="float32"))

for k in ["tensorflow", "jax"]:
    with tc.runtime_backend(k):
        print("---------------")
        print("%s backend" % k)
        print("gradient")
        g1, _ = grad_tc(6, 3)
        print("quantum Fisher information")
        qfi1, _ = qfi_tc(6, 3)
        print("Hessian")
        hs1, _ = hessian_tc(6, 3)
# Output:
#   ---------------

#   tensorflow backend

#   gradient

#   staging time: 15.889095 s

#   running time: 0.001126 s

#   quantum Fisher information

#   WARNING:tensorflow:The dtype of the watched primal must be floating (e.g. tf.float32), got tf.complex64

#   staging time: 53.973453 s

#   running time: 0.002332 s

#   Hessian

#   staging time: 96.066412 s

#   running time: 0.004685 s

#   WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

#   ---------------

#   jax backend

#   gradient

#   staging time: 4.696845 s

#   running time: 0.000105 s

#   quantum Fisher information

#   staging time: 4.618631 s

#   running time: 0.000386 s

#   Hessian

#   staging time: 23.591966 s

#   running time: 0.001681 s


"""
The results obtained from the two methods are consistent by the following checks
"""

"""
* Gradient
"""

np.testing.assert_allclose(g0, g1, atol=1e-4)

"""
* Quantum Fisher Information(QFI)
"""

np.testing.assert_allclose(qfi0, 4.0 * qfi1, atol=1e-3)

"""
* Hessian
"""

np.testing.assert_allclose(hs0, hs1, atol=1e-4)



================================================
FILE: docs/source/tutorials/gradient_benchmark_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 梯度计算效率比较
"""

"""
## 概述

在本教程中，我们比较了通过 TensorCircuit 提供的自动微分框架和 Qiskit 提供的传统参数平移框架对梯度和类梯度对象（例如量子费雪信息）进行估测的效率。
"""

"""
## 设置

我们从 Qiskit 和 TensorCircuit 导入必要的包和模块。
"""

import time
import numpy as np
from functools import reduce
from operator import xor

from qiskit.opflow import I, X, Z, StateFn, CircuitStateFn, SummedOp
from qiskit.circuit import QuantumCircuit, ParameterVector
from scipy.optimize import minimize
from qiskit.opflow.gradients import Gradient, QFI, Hessian

import tensorcircuit as tc
from tensorcircuit import experimental

"""
## Qiskit 梯度框架基准

由于 Qiskit 在梯度估测方面**太**慢了，我们使用小型系统在 Jupyter 中进行基准测试以节省时间，
对于更大尺寸和更深的电路，效率差异将变得更加明显。

三个类梯度的任务分别是梯度、量子费学信息和 Hessian 评估。
"""

def benchmark(f, *args, trials=10):
    time0 = time.time()
    r = f(*args)
    time1 = time.time()
    for _ in range(trials):
        r = f(*args)
    time2 = time.time()
    if trials > 0:
        time21 = (time2 - time1) / trials
    else:
        time21 = 0
    ts = (time1 - time0, time21)
    print("staging time: %.6f s" % ts[0])
    if trials > 0:
        print("running time: %.6f s" % ts[1])
    return r, ts


def grad_qiskit(n, l, trials=2):
    hamiltonian = reduce(xor, [X for _ in range(n)])
    wavefunction = QuantumCircuit(n)
    params = ParameterVector("theta", length=3 * n * l)
    for j in range(l):
        for i in range(n - 1):
            wavefunction.cnot(i, i + 1)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i], i)
        for i in range(n):
            wavefunction.rz(params[3 * n * j + i + n], i)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i + 2 * n], i)

    # 定义能量对应的期望值
    op = ~StateFn(hamiltonian) @ StateFn(wavefunction)
    grad = Gradient().convert(operator=op, params=params)

    def get_grad_qiskit(values):
        value_dict = {params: values}
        grad_result = grad.assign_parameters(value_dict).eval()
        return grad_result

    return benchmark(get_grad_qiskit, np.ones([3 * n * l]), trials=trials)


def qfi_qiskit(n, l, trials=0):
    hamiltonian = reduce(xor, [X for _ in range(n)])
    wavefunction = QuantumCircuit(n)
    params = ParameterVector("theta", length=3 * n * l)
    for j in range(l):
        for i in range(n - 1):
            wavefunction.cnot(i, i + 1)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i], i)
        for i in range(n):
            wavefunction.rz(params[3 * n * j + i + n], i)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i + 2 * n], i)

    nat_grad = QFI().convert(operator=StateFn(wavefunction), params=params)

    def get_qfi_qiskit(values):
        value_dict = {params: values}
        grad_result = nat_grad.assign_parameters(value_dict).eval()
        return grad_result

    return benchmark(get_qfi_qiskit, np.ones([3 * n * l]), trials=trials)


def hessian_qiskit(n, l, trials=0):
    hamiltonian = reduce(xor, [X for _ in range(n)])
    wavefunction = QuantumCircuit(n)
    params = ParameterVector("theta", length=3 * n * l)
    for j in range(l):
        for i in range(n - 1):
            wavefunction.cnot(i, i + 1)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i], i)
        for i in range(n):
            wavefunction.rz(params[3 * n * j + i + n], i)
        for i in range(n):
            wavefunction.rx(params[3 * n * j + i + 2 * n], i)

    # 定义能量对应的期望值
    op = ~StateFn(hamiltonian) @ StateFn(wavefunction)
    grad = Hessian().convert(operator=op, params=params)

    def get_hs_qiskit(values):
        value_dict = {params: values}
        grad_result = grad.assign_parameters(value_dict).eval()
        return grad_result

    return benchmark(get_hs_qiskit, np.ones([3 * n * l]), trials=trials)

g0, _ = grad_qiskit(6, 3)  # 梯度
# Output:
#   staging time: 1.665786 s

#   running time: 1.474930 s


qfi0, _ = qfi_qiskit(6, 3)  # QFI
# Output:
#   staging time: 47.131374 s


hs0, _ = hessian_qiskit(6, 3)  # Hessian
# Output:
#   staging time: 80.495983 s


"""
## TensorCircuit 自动微分基准测试

我们对上面 Qiskit 部分中定义的相同问题进行基准测试，我们可以看到速度提升！
事实上，对于一个中等的 10 量子比特 4 块系统，QFI 评估加速超过 $10^6$ 倍！
（注意即时编译的第一次运行的编译时间是可以被平摊的，只需要比较运行时间。在 Qiskit 的情况下，没有即时编译，因此运行时间与暂存时间相同。）
"""

def grad_tc(n, l, trials=10):
    def f(params):
        c = tc.Circuit(n)
        for j in range(l):
            for i in range(n - 1):
                c.cnot(i, i + 1)
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i])
            for i in range(n):
                c.rz(i, theta=params[3 * n * j + i + n])
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i + 2 * n])
        return tc.backend.real(c.expectation(*[[tc.gates.x(), [i]] for i in range(n)]))

    get_grad_tc = tc.backend.jit(tc.backend.grad(f))
    return benchmark(get_grad_tc, tc.backend.ones([3 * n * l], dtype="float32"))


def qfi_tc(n, l, trials=10):
    def s(params):
        c = tc.Circuit(n)
        for j in range(l):
            for i in range(n - 1):
                c.cnot(i, i + 1)
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i])
            for i in range(n):
                c.rz(i, theta=params[3 * n * j + i + n])
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i + 2 * n])
        return c.state()

    get_qfi_tc = tc.backend.jit(experimental.qng(s, mode="fwd"))
    return benchmark(get_qfi_tc, tc.backend.ones([3 * n * l], dtype="float32"))


def hessian_tc(n, l, trials=10):
    def f(params):
        c = tc.Circuit(n)
        for j in range(l):
            for i in range(n - 1):
                c.cnot(i, i + 1)
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i])
            for i in range(n):
                c.rz(i, theta=params[3 * n * j + i + n])
            for i in range(n):
                c.rx(i, theta=params[3 * n * j + i + 2 * n])
        return tc.backend.real(c.expectation(*[[tc.gates.x(), [i]] for i in range(n)]))

    get_hs_tc = tc.backend.jit(tc.backend.hessian(f))
    return benchmark(get_hs_tc, tc.backend.ones([3 * n * l], dtype="float32"))

for k in ["tensorflow", "jax"]:
    with tc.runtime_backend(k):
        print("---------------")
        print("%s backend" % k)
        print("gradient")
        g1, _ = grad_tc(6, 3)
        print("quantum Fisher information")
        qfi1, _ = qfi_tc(6, 3)
        print("Hessian")
        hs1, _ = hessian_tc(6, 3)
# Output:
#   ---------------

#   tensorflow backend

#   gradient

#   staging time: 15.889095 s

#   running time: 0.001126 s

#   quantum Fisher information

#   WARNING:tensorflow:The dtype of the watched primal must be floating (e.g. tf.float32), got tf.complex64

#   staging time: 53.973453 s

#   running time: 0.002332 s

#   Hessian

#   staging time: 96.066412 s

#   running time: 0.004685 s

#   WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

#   ---------------

#   jax backend

#   gradient

#   staging time: 4.696845 s

#   running time: 0.000105 s

#   quantum Fisher information

#   staging time: 4.618631 s

#   running time: 0.000386 s

#   Hessian

#   staging time: 23.591966 s

#   running time: 0.001681 s


"""
通过以下检查，两种方法得到的结果是一致的。
"""

"""
* 梯度
"""

np.testing.assert_allclose(g0, g1, atol=1e-4)

"""
* 量子费雪信息
"""

np.testing.assert_allclose(qfi0, 4.0 * qfi1, atol=1e-3)

"""
* Hessian
"""

np.testing.assert_allclose(hs0, hs1, atol=1e-4)



================================================
FILE: docs/source/tutorials/qaoa_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 量子近似优化算法 (QAOA)
"""

"""
## 概述
"""

"""
QAOA 是一种混合经典量子算法，它结合了量子电路与经典优化。
在本教程中，我们利用 QAOA 解决最大割 (MAX CUT) 组合优化问题：给定一个图 $G=(V, E)$，其中节点 $V$ 和边 $E$，找到一个子集 $S \in V$ 使得 $S$ 和 $S \backslash V$ 之间的边数最大化。
这个问题可以简化为寻找反铁磁伊辛模型的基态，其哈密顿量为：

$$H_C = \frac{1}{2} \sum_{i,j\in E} C_{ij} \sigma^{z}_{i} \sigma^{z}_{j},$$

其中 $\sigma^{z}_{i}$ 是第 $i$ 个量子比特上的 Pauli-z 矩阵，$C_{ij}$ 是节点 $i$ 和 $j$ 之间的边的权重。
为简单起见，我们设置 $C_{ij}=1$。如果 $\sigma^{z}_{i}=\sigma^{z}_{j}$, $i,j\in S$ 或 $i,j\in S \backslash V$；
如果 $\sigma^{z}_{i}= -\sigma^{z}_{j}$, $i\in S, j\in S \backslash V$ 或 $i\in S \backslash V，j\ 。
显然，$S 和 $S\backslash V$ 之间的边数在从基态解码的图结构中最大化。
"""

"""
## 设置
"""

import tensorcircuit as tc
import tensorflow as tf
import networkx as nx

K = tc.set_backend("tensorflow")

nlayers = 3  # 层数
ncircuits = 2  # 有不同初始参数的电路数量

"""
## 定义图
"""

def dict2graph(d):
    g = nx.to_networkx_graph(d)
    for e in g.edges:
        if not g[e[0]][e[1]].get("weight"):
            g[e[0]][e[1]]["weight"] = 1.0
    nx.draw(g, with_labels=True)
    return g


# 一个图实例
# 每个节点连接三个节点
# 例如，节点 0 连接到节点 1,7,3
example_graph_dict = {
    0: {1: {"weight": 1.0}, 7: {"weight": 1.0}, 3: {"weight": 1.0}},
    1: {0: {"weight": 1.0}, 2: {"weight": 1.0}, 3: {"weight": 1.0}},
    2: {1: {"weight": 1.0}, 3: {"weight": 1.0}, 5: {"weight": 1.0}},
    4: {7: {"weight": 1.0}, 6: {"weight": 1.0}, 5: {"weight": 1.0}},
    7: {4: {"weight": 1.0}, 6: {"weight": 1.0}, 0: {"weight": 1.0}},
    3: {1: {"weight": 1.0}, 2: {"weight": 1.0}, 0: {"weight": 1.0}},
    6: {7: {"weight": 1.0}, 4: {"weight": 1.0}, 5: {"weight": 1.0}},
    5: {6: {"weight": 1.0}, 4: {"weight": 1.0}, 2: {"weight": 1.0}},
}

example_graph = dict2graph(example_graph_dict)
# Output:
#   <Figure size 432x288 with 1 Axes>

"""
## 参数化量子电路 (PQC)
"""

"""
具有 $p$ 层的 PQC 可以写成：

$$U(\vec{\beta}, \vec{\gamma}) = V_{p}U_{p} \cdots V_{1}U_{1},$$

其中 $U_{j}= e^{-i\gamma_{j}H_{C}}$ 和 $V_{j}= e^{-i \beta_{j} \sum_{k} \sigma^{x }_{k}}$ 
"""

def QAOAansatz(params, g=example_graph):
    n = len(g.nodes)  # 节点数
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
    # PQC
    for j in range(nlayers):
        # U_j
        for e in g.edges:
            c.exp1(
                e[0],
                e[1],
                unitary=tc.gates._zz_matrix,
                theta=g[e[0]][e[1]].get("weight", 1.0) * params[2 * j],
            )
        # V_j
        for i in range(n):
            c.rx(i, theta=params[2 * j + 1])

    # 计算损失函数
    loss = 0.0
    for e in g.edges:
        loss += c.expectation_ps(z=[e[0], e[1]])

    return K.real(loss)

"""
## 主优化循环
"""

# 使用 vvag 获取不同随机电路实例的损失和梯度
QAOA_vvag = K.jit(tc.backend.vvag(QAOAansatz, argnums=0, vectorized_argnums=0))

params = K.implicit_randn(shape=[ncircuits, 2 * nlayers], stddev=0.1)  # 初始参数
opt = K.optimizer(tf.keras.optimizers.Adam(1e-2))

for i in range(50):
    loss, grads = QAOA_vvag(params, example_graph)
    print(K.numpy(loss))
    params = opt.update(grads, params)  # 梯度下降
# Output:
#   [-0.23837963 -1.1651934 ]

#   [-0.5175445 -1.4539642]

#   [-0.7306818 -1.6646069]

#   [-0.91530037 -1.8384367 ]

#   [-1.0832287 -1.9884492]

#   [-1.2398103 -2.120449 ]

#   [-1.3878661 -2.2374902]

#   [-1.5290209 -2.341291 ]

#   [-1.6642232 -2.4328852]

#   [-1.7940071 -2.5128942]

#   [-1.9186544 -2.5888019]

#   [-2.0382538 -2.6627793]

#   [-2.152771 -2.735217]

#   [-2.2620971 -2.8060198]

#   [-2.3657765 -2.8749723]

#   [-2.4635859 -2.942443 ]

#   [-2.5571456 -3.0074604]

#   [-2.6474872 -3.071116 ]

#   [-2.7343643 -3.1320357]

#   [-2.8174913 -3.1904984]

#   [-2.896546  -3.2464304]

#   [-2.971222 -3.298626]

#   [-3.0411685 -3.3485155]

#   [-3.1060221 -3.3945203]

#   [-3.1671162 -3.4365993]

#   [-3.2244647 -3.47741  ]

#   [-3.2800133 -3.51378  ]

#   [-3.328074  -3.5467668]

#   [-3.3779154 -3.5716858]

#   [-3.42378   -3.6026983]

#   [-3.4665916 -3.6264663]

#   [-3.5065007 -3.6452012]

#   [-3.5436964 -3.6676104]

#   [-3.5783873 -3.6827888]

#   [-3.6107998 -3.696251 ]

#   [-3.6411772 -3.710956 ]

#   [-3.6697989 -3.725151 ]

#   [-3.6969085 -3.739223 ]

#   [-3.7227716 -3.753837 ]

#   [-3.747642  -3.7637105]

#   [-3.7717733 -3.7778597]

#   [-3.7953677 -3.7897499]

#   [-3.8185773 -3.8026254]

#   [-3.8415692 -3.8186839]

#   [-3.864397  -3.8288355]

#   [-3.887118  -3.8470592]

#   [-3.9089546 -3.8578553]

#   [-3.9298224 -3.8789082]

#   [-3.9531326 -3.898365 ]

#   [-3.9759274 -3.9132624]




================================================
FILE: docs/source/tutorials/qcloud_sdk.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# tensorcircuit SDK for QCloud（230220 ver）
"""

"""
## import the package

``apis`` is temporarily as the entry point submodule for qcloud
"""

import tensorcircuit as tc
from tensorcircuit.cloud import apis
from tensorcircuit.cloud.wrapper import batch_expectation_ps
from tensorcircuit.compiler.qiskit_compiler import qiskit_compile
import numpy as np

"""
## setup the token

The users need an API token from tQuK to connect to the server and submit tasks, the token only need be set once and it is then written to the local computer
"""

apis.set_token("foobar")
# only required running once for a given laptop

"""
## list providers/devices/properties

Get basic info of devices and device information
"""

apis.list_providers()

apis.list_devices("tencent")

apis.list_devices("tencent", state="on")

apis.list_properties(device="9gmon")

d = apis.get_device("9gmon")

d.list_properties()["bits"][8]

d.topology()

d.native_gates()

d.topology_graph(visualize=True)

"""
## Task submit and the results
"""

"""
Basic task submission syntax below, here we use a simulator backend on tQuK `simulator:tc`
"""

c = tc.Circuit(1)
c.H(0)

t = apis.submit_task(device="simulator:tc", circuit=c, shots=1024)
print(t.details())
t.results(blocked=True)

"""
``blocked=True`` can wait until the task is finished or failed (rasing an error)
"""

t.status()

t.get_device()

"""
resubmit a job with the same source (device/shots) and command (circuit)

"""

t1 = t.resubmit()
t1.details(blocked=True, prettify=True)

"""
``t.details`` can also permit the ``blocked=True`` option, which waits until the task is finished or failed (no error raised).

Also note by using ``prettfiy=True`` option, we have python datatime object for the timestamp which is easy to read but hard for io (not directly json serializable anymore) 
"""

"""
## local provider enable quick debugging and testing

TC comes with a local provider which behaves as a simple cloud provider but run the circuit locally
"""

apis.set_provider("local")
# using tc simulator on local device: your own laptop is your server
apis.list_devices()

c = tc.Circuit(2)
c.h(0)
c.cx(0, 1)

# exactly the same API as tQuK
t = apis.submit_task(circuit=c, device="testing", shots=8192)
t.results(blocked=True)

tl = apis.list_tasks()
tl

id_ = tl[0].__str__()
print(id_)
t = apis.get_task(id_)
t.details()

id_ = tl[0].__str__()
print(id_.split("~~")[1])
t = apis.get_task(id_)
t.details()

"""
As shown above, the task can be indexed either with device information or not (as long as we use ``set_provider``)
"""

# back to tencent server for demonstration below
apis.set_provider("tencent")

"""
## GHZ state on real device and readout mitigation
"""

nqubit = 9
shots = 4096
c = tc.Circuit(nqubit)
c.H(8)
c.cnot(8, 4)
c.cnot(4, 0)
c.cnot(0, 2)
c.cnot(2, 6)

# above we dirct assign physical qubits

t = apis.submit_task(
    circuit=c, shots=shots, device="9gmon", enable_qos_qubit_mapping=False
)
raw_count = t.results(blocked=True)
# blocked = True will block the process until the result is returned
# the default behavior is blocked=False, where only one query is made and raise error when the task is incomplete

# note we explicitly turn off qubit mapping from qos, which gurantee our logical circuit are identical to the physical one.
# but one should ensure the topology link in the logical circuit is compatible with the target device

"""
In the below, we use tensorcircuit builtin powerful tool for readout mitigation: ``tc.results.rem.ReadoutMit``, it supports various method for calibriation and mitigation
"""

mit = tc.results.rem.ReadoutMit("9gmon?o=0")
# here o=0 is a short for disable qubit mapping and gate decomposition at the backend server
mit.cals_from_system(nqubit, shots, method="local")
# local calibriation
miti_count = mit.apply_correction(raw_count, nqubit, method="constrained_least_square")

"""
By attaching ``?o=0`` after the device string, we have the same effect of setting ``enable_qos_qubit_mapping=False`` (o=1)
and ``enable_qos_gate_decomposition=False`` (o=2), and both of them are by default True (o=3).

We can define the REM class by using more customizable function.
"""

def run(cs, shots):
    """batch mode"""
    ts = apis.submit_task(
        circuit=cs, shots=shots, device="9gmon", enable_qos_qubit_mapping=False
    )
    return [t.results(blocked=True) for t in ts]


mit = tc.results.rem.ReadoutMit(run)
mit.cals_from_system(nqubit, shots, method="local")

raw_count = tc.results.counts.marginal_count(raw_count, [8, 4, 0, 2, 6])
miti_count = tc.results.counts.marginal_count(miti_count, [8, 4, 0, 2, 6])
# only keep the result for qubit 8, 4, 0, 2, 6 and in that exact order

tc.results.counts.plot_histogram([raw_count, miti_count])

ideal_count = tc.results.counts.vec2count(c.probability(), prune=True)
# we can obtain analytic count results by ``c.probability()`` method, and ``vec2count`` with transform the vector as a dict

ideal_count = tc.results.counts.marginal_count(ideal_count, [8, 4, 0, 2, 6])
tc.results.counts.kl_divergence(
    ideal_count, raw_count
), tc.results.counts.kl_divergence(ideal_count, miti_count)

# we can directly check local readout matrix on each qubit
print("readout matrix")
for i, m in enumerate(mit.single_qubit_cals):
    print("qubit %s:" % i)
    print(m)

"""
Apart from calibriation from real experiments, we can access the readout error matrix from API (which is fast but may be not that up to date)
"""

mit = tc.results.rem.ReadoutMit("9gmon?o=0")
mit.cals_from_api(nqubit)
mit.single_qubit_cals[0]

"""
## Abstraction of three layers of qubits and the mappings

In the above example, the circuit is not compiled by the frontend: tc or backend: qos, in the follows, we will introduce circuit compiling and the new abstraction on different level of qubits.

New abstraction on qubits: positional qubits, logical qubits, physical qubits, we need two more mappings: ``positional_logical_mapping`` and ``logical_physical_mapping``.

The separation between positional and logical qubits is due to partial measurement, while the seperation between logical and physical qubits are from circuit compiling onto hardware, including swap inserting (where the last swap is omitted, current qos behavior), qubit routing (i.e. initial mapping).

Now we do the GHZ preparation on another chip, but use mapping and partial measurement abstraction this time
"""

# logical circuit for GHZ-5

c = tc.Circuit(5)
c.h(0)
for i in range(4):
    c.cx(i, i + 1)
for i in range(5):
    c.measure_instruction(i)

# We map the circuit on the physical qubits by hand

c1 = c.initial_mapping({0: 8, 1: 4, 2: 0, 3: 2, 4: 6}, n=9)
positional_logical_mapping = c1.get_positional_logical_mapping()
positional_logical_mapping

c1.draw()  # circuit after mapping

t = apis.submit_task(
    circuit=c1, shots=shots, device="9gmon", enable_qos_qubit_mapping=False
)
raw_count = t.results(blocked=True)

logical_physical_mapping = t.details()["optimization"]["pairs"]
logical_physical_mapping
# this mapping is identical since we disable qos qubit mapping above

mit = tc.results.rem.ReadoutMit("9gmon?o=0")
mit.cals_from_system(9, shots, method="local")
miti_count = mit.apply_correction(
    raw_count,
    [8, 4, 0, 2, 6],
    positional_logical_mapping=positional_logical_mapping,
    logical_physical_mapping=logical_physical_mapping,
    method="square",
)

plot_histogram([raw_count, miti_count])

"""
We can have another way to understand logical qubits: we could treat 0-4 in the original circuit as logical qubits, then we will have the following convention and the circuit after initial mapping as the physical one (abstraction reference shift)
"""

miti_count = mit.apply_correction(
    raw_count,
    [0, 1, 2, 3, 4],
    positional_logical_mapping=None,
    logical_physical_mapping={0: 8, 1: 4, 2: 0, 3: 2, 4: 6},
    method="square",
)
# note how the None by default implies an identity mapping

plot_histogram([raw_count, miti_count])
# the results should be exactly the same, since they are just the same thing using different reference system

"""
The above abstraction is rather low level where the compiling is done by hand and we recommend the following api for users (**the highly recommended way**).

The recommended approach heavily depends on the frontend compiling via qiskit (builtin support in tc).
"""

# 0. acquire readout mitigation class

mit = tc.results.rem.ReadoutMit("20xmon?o=0")
mit.cals_from_system(20)

# 1. define the logical circuit

n = 5
c = tc.Circuit(n)
c.h(0)
for i in range(n - 1):
    c.cx(i, i + 1)
for i in reversed(range(n)):
    c.measure_instruction(i)

# 2. compile the circuit

d = apis.get_device("20xmon")

c1, info = qiskit_compile(
    c,
    compiled_options={
        "basis_gates": d.native_gates(),
        "optimization_level": 3,
        "coupling_map": d.topology(),
    },
)


# 3. submit the job and get the raw result

t = apis.submit_task(
    circuit=c1,
    shots=8192,
    device=d,
    enable_qos_qubit_mapping=False,
    enable_qos_gate_decomposition=False,
)
raw_count = t.results(blocked=True)

# 4. obtain the mitigated result in terms of distribution or expectation

print("distribution", mit.apply_correction(raw_count, n, method="square", **info))
print("<Z0Z1>", mit.expectation(raw_count, [0, 1], **info))

info  # compiling info and the qubit mapping are recorded automatically

tc.results.counts.plot_histogram(
    [raw_count, mit.apply_correction(raw_count, n, method="square", **info)]
)

"""
And the **all-in-one API**: ``batch_expectation_ps`` with circuit generating, grouping, compiling, optimization and error mitigation support is as shown below, the API is also consistent with numerical simulations, basically the API capture all the workflow shown in above cell with extra enhancement
"""

c = tc.Circuit(2)
c.h(0)
c.cz(0, 1)
c.x(1)
print("numerical results: [<X_0>, <X_0Z_1>]", batch_expectation_ps(c, [[1, 0], [1, 3]]))
print(
    "hardware results: [<X_0>, <X_0Z_1>]",
    batch_expectation_ps(c, [[1, 0], [1, 3]], "20xmon"),
)
print(
    "numerical results: <X_0> + 0.5* <X_0Z_1>",
    batch_expectation_ps(c, [[1, 0], [1, 3]], ws=[1, 0.5]),
)
print(
    "hardware results: <X_0> + 0.5* <X_0Z_1>",
    batch_expectation_ps(c, [[1, 0], [1, 3]], "20xmon", ws=[1, 0.5]),
)

"""
batch submission is possible with multiple circuits in a list and the return is a list of task, respectively. The batch mechanism are supported both on real chips and simulators.
"""

# we can also do a batch submission for the real hardware chip, simply by provide a circuit list

c = tc.Circuit(2)
c.h(0)

c1 = tc.Circuit(2)
c1.h(1)

ts = apis.submit_task(device="20xmon", circuit=[c, c1], shots=1024)

for t in ts:
    print(t.results(blocked=True))

"""
## measure on partial of the qubits

Note the return order should ideally follow the measure order in the instructions
"""

# directly partial measure via qiskit

from qiskit.circuit import QuantumCircuit

qc = QuantumCircuit(9, 9)
qc.x(8)
qc.x(6)
qc.measure(8, 8)
qc.measure(2, 2)
qc.measure(6, 6)

t = apis.submit_task(circuit=qc, shots=shots, device="9gmon?o=0")
print(t.results(blocked=True))

"""
 The above case also indicates that tc ``submit_task`` API directly support Qiskit ``QuantumCircuit`` object
"""

# directly partial measure on tc

# recommended approach

nqubit = 9
shots = 4096
c = tc.Circuit(nqubit)
c.x(8)
c.x(6)
c.measure_instruction(8)
c.measure_instruction(2)
c.measure_instruction(6)

t = apis.submit_task(circuit=c, shots=shots, device="9gmon?o=0")
print(t.results(blocked=True))
print(c.get_positional_logical_mapping())

"""
partial measurment also supported via the simulator on the cloud
"""

nqubit = 9
shots = 4096
c = tc.Circuit(nqubit)
c.x(8)
c.x(6)
c.measure_instruction(8)
c.measure_instruction(2)
c.measure_instruction(6)

t = apis.submit_task(circuit=c, shots=shots, device="simulator:tc")
print(t.results(blocked=True))

nqubit = 9
shots = 4096
c = tc.Circuit(nqubit)
c.x(8)
c.x(6)
c.measure_instruction(8)
c.measure_instruction(2)
c.measure_instruction(6)

t = apis.submit_task(circuit=c, shots=shots, device="simulator:aer")
print(t.results(blocked=True))

"""
## two level compiling system

We provide compiling support at frond end (via tc-qiskit pipeline) and at back end (in qos).
The front end option is enabled by ``compiled=True`` (default to False) and also with an optional dict for ``qiskit.transpile`` arguments called ``compiled_options``. For advanced users, we recommand you to separately deal with the circuit compiling and submission as we discussed above as the recommended approach. The backend qos compiling is controlled by ``enable_qos_qubit_mapping`` and ``enable_qos_gate_decomposition`` (all default to True). The ``?o=int`` str after the device name can overide qos compiling options. We strongly recommend the users only use one part of the compiling in case confusing and conflicts. For front end compiling, though the built-in compiling via ``compiled`` switch in ``submit_task`` is handy, we recommend the advanced user to use standalone compiling module as shown above, i.e. explicitly call ``qiskit_compile``, the advantage for the latter is we can obtain qubit mapping information at the same time for further error mitigation pipelines.
"""

# directly use built-in mitigation with expectation evaluation + front-end (tc/qiskit) compiling system

nqubit = 3
shots = 8192
c = tc.Circuit(nqubit)
c.h(0)
c.rz(0, theta=0.4)
c.x(0)
c.y(0)
c.h(1)
c.rx(2, theta=0.7)
c.ry(1, theta=-1.2)
c.cnot(0, 1)
c.cnot(2, 0)
c.h(1)
c.x(2)

print("exact: ", [np.real(c.expectation_ps(z=[i])).tolist() for i in range(nqubit)])

t = apis.submit_task(
    circuit=c,
    shots=shots,
    device="9gmon",
    compiled=True,
    enable_qos_qubit_mapping=False,
    enable_qos_gate_decomposition=False,
)

ct = t.results(blocked=True)

mit = tc.results.readout_mitigation.ReadoutMit("9gmon?o=0")
mit.cals_from_system(3, shots=8192, method="local")

print(
    "experiments (mitigated directly via expectation): ",
    [mit.expectation(ct, [i]) for i in range(nqubit)],
)

# no need to provider mapping in mit as there is no mapping in this case,
# compiled=True itself doesn't enable front end qubit routing

print(
    "experiments (mitigated using lstm): ",
    [
        tc.results.counts.expectation(mit.apply_correction(ct, 3, method="square"), [i])
        for i in range(nqubit)
    ],
)

c.draw()  # target circuit: mimic a VQA case

# use backend compiling system enabled by qos and the very handy built-in auto mitigation
# (only works without qubit mapping at front end)

nqubit = 3
shots = 8192
c = tc.Circuit(nqubit)
c.h(0)
c.rz(0, theta=0.4)
c.x(0)
c.y(0)
c.h(1)
c.rx(2, theta=0.7)
c.ry(1, theta=-1.2)
c.cnot(0, 1)
c.cnot(2, 0)
c.h(1)
c.x(2)

print("exact: ", [np.real(c.expectation_ps(z=[i])) for i in range(nqubit)])

t = apis.submit_task(
    circuit=c,
    shots=shots,
    device="9gmon",
    compiled=False,
    enable_qos_qubit_mapping=True,
    enable_qos_gate_decomposition=True,
)

ct = t.results(blocked=True, mitigated=True)
# auto mitigation with backend qubit mapping considered


print(
    "experiments (mitigated): ",
    [tc.results.counts.expectation(ct, [i]) for i in range(nqubit)],
)

# inspect compiling results from the tc and qos for the task, we can directly get the circuit objects from prettified details

c_complied_before_qos = t.details(prettify=True)["frontend"]
c_complied_after_qos = t.details(prettify=True)["backend"]

c_complied_before_qos.draw()

c_complied_after_qos.draw(output="mpl")

"""
dry run mode to query compiled circuit only from qos (not really sending the circuit to chips), we can use ``qos_dry_run=True`` option

"""

nqubit = 3
shots = 8192
c = tc.Circuit(nqubit)
c.h(0)
c.h(1)
c.rx(2, theta=0.7)
c.ry(1, theta=-1.2)
c.cnot(0, 1)
c.cnot(2, 0)
c.h(1)


t = apis.submit_task(
    circuit=c,
    shots=shots,
    device="9gmon",
    compiled=True,
    enable_qos_qubit_mapping=True,
    enable_qos_gate_decomposition=True,
    qos_dry_run=True,
)

t.details(prettify=True, blocked=True)["backend"].draw()

"""
## scalable readout simulation and mitigation

Via TensorCircuit, we provide the capability to do scalable (20+ qubits) readout error simulation and mitigation
"""

# scalable readout error simulation on tQuK with tensorcircuit backend using tensor network approach

c = tc.Circuit(3)
t = apis.submit_task(circuit=c, device="simulator:tcn1", shots=8192)
t.results(blocked=True)

t.results(mitigated=True)

c = tc.Circuit(25)
t = apis.submit_task(circuit=c, device="simulator:tcn1", shots=8192)
t.results(blocked=True)

"""
Simulator device also support batch submission
"""

# batch submission to the simulator
cs = []
for i in range(15):
    c = tc.Circuit(15)
    c.x(i)
    cs.append(c)
ts = apis.submit_task(circuit=cs, device="simulator:tcn1", shots=8192)

# mitigated with m3 scalable on count dict
c = tc.Circuit(15)
c.x(0)
t = apis.submit_task(circuit=c, device="simulator:tcn1", shots=8192)

mit = tc.results.readout_mitigation.ReadoutMit("simulator:tcn1")
mit.cals_from_system(15)

raw_count = t.results(blocked=True)
mit.apply_correction(raw_count, 15, method="M3_auto")

# mitigated scalable directly on expectation: not a wrapper for count but a new algorithm!
# see eq 6 in https://arxiv.org/pdf/2006.14044.pdf

mit.expectation(raw_count, [0])

"""
## list task and get previous task

get history tasks and their details, so that your experimental data are always accessible with detailed meta data on the cloud
"""

apis.list_tasks()

apis.list_tasks(device="9gmon")

t = apis.get_task("d77bec2f-ab07-4dbc-a273-caa8b23a921c")

t.details()

t = apis.get_task("tencent::9gmon~~e32bb488-5ee9-4b07-8217-1e78ceb4bde3")

t.details(prettify=True)

t.results()



================================================
FILE: docs/source/tutorials/qml_scenarios.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Quantum Machine Learning for Classification Task

**Demonstrations on some common setups and techniques**
"""

"""
## Overview

We use the fashion-MNIST dataset to set up a binary classification task, we will try different encoding schemes for the inputs and apply possible classical post-processing on the quantum outputs to enhance the classification accuracy. In this tutorial, we stick to the TensorFlow backend and try to consistently use the **Keras interface** provided by tensorcircuit for quantum functions, where we can magically turn a quantum function into a Keras layer.
"""

from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import tensorflow as tf
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## Dataset and Pre-processing

We first load the fashion-mnist dataset and differentiation between T-shirt (0) and Trouser (1).
"""

(x_train, y_train), (x_test, y_test) = tc.templates.dataset.mnist_pair_data(
    0, 1, loader=tf.keras.datasets.fashion_mnist
)

x_train.shape, y_train.shape, x_test.shape, y_test.shape
# Output:
#   ((12000, 28, 28, 1), (12000,), (2000, 28, 28, 1), (2000,))

plt.imshow(x_train[0])
# Output:
#   <matplotlib.image.AxesImage at 0x7f9147029610>
#   <Figure size 432x288 with 1 Axes>

plt.imshow(x_train[1])
# Output:
#   <matplotlib.image.AxesImage at 0x7f91473b6c10>
#   <Figure size 432x288 with 1 Axes>

"""
## Amplitude Encoding
"""

x_train = tf.image.pad_to_bounding_box(x_train, 2, 2, 32, 32)
x_test = tf.image.pad_to_bounding_box(x_test, 2, 2, 32, 32)

batched_ae = K.vmap(tc.templates.dataset.amplitude_encoding, vectorized_argnums=0)

x_train_q = batched_ae(x_train, 10)
x_test_q = batched_ae(x_test, 10)

n = 10
blocks = 3


def qml(x, weights):
    c = tc.Circuit(n, inputs=x)
    for j in range(blocks):
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return K.sigmoid(K.sum(outputs))


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

model = tf.keras.Sequential([qml_layer])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_q, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 85s 559ms/step - loss: 0.6217 - binary_accuracy: 0.7667 - val_loss: 0.3990 - val_binary_accuracy: 0.9620

#   Epoch 2/3

#   75/75 [==============================] - 14s 185ms/step - loss: 0.3701 - binary_accuracy: 0.9571 - val_loss: 0.3421 - val_binary_accuracy: 0.9507

#   Epoch 3/3

#   75/75 [==============================] - 14s 185ms/step - loss: 0.3252 - binary_accuracy: 0.9542 - val_loss: 0.3030 - val_binary_accuracy: 0.9540

#   <keras.callbacks.History at 0x7f9147416e50>

"""
## Classical Post-processing

We attached one Linear layer after the quantum outputs to enhance the capacity of the machine learning model as a quantum-neural hybrid machine learning approach.
"""

def qml(x, weights):
    c = tc.Circuit(n, inputs=x)
    for j in range(blocks):
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return outputs


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

inputs = tf.keras.Input(shape=(2**n), dtype=tf.complex64)
measurements = qml_layer(inputs)
output = tf.keras.layers.Dense(1, activation="sigmoid")(measurements)
model = tf.keras.Model(inputs=inputs, outputs=output)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_q, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 71s 508ms/step - loss: 0.5140 - binary_accuracy: 0.8841 - val_loss: 0.3617 - val_binary_accuracy: 0.9521

#   Epoch 2/3

#   75/75 [==============================] - 14s 182ms/step - loss: 0.2803 - binary_accuracy: 0.9421 - val_loss: 0.2093 - val_binary_accuracy: 0.9506

#   Epoch 3/3

#   75/75 [==============================] - 15s 200ms/step - loss: 0.2057 - binary_accuracy: 0.9437 - val_loss: 0.1795 - val_binary_accuracy: 0.9483

#   <keras.callbacks.History at 0x7f913d4ee520>

"""
## PCA Embedding

Amplitude encoding is difficult to implement on real quantum hardware, we here instead consider another way for data input, where only a single qubit rotation is involved. To compress the input data such that it can fit into a small circuit, PCA dimension reduction is required.
"""

x_train_r = PCA(10).fit_transform(x_train.numpy().reshape([-1, 32 * 32]))

x_train_r.shape  # we now has 10-d vector compression for each figure
# Output:
#   (12000, 10)

def qml(x, weights):
    c = tc.Circuit(n)
    for i in range(10):
        c.rx(i, theta=x[i])  # loading the data
    for j in range(blocks):
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return K.sigmoid(K.sum(outputs))


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

model = tf.keras.Sequential([qml_layer])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_r, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 71s 447ms/step - loss: 0.7993 - binary_accuracy: 0.6996 - val_loss: 0.3026 - val_binary_accuracy: 0.8829

#   Epoch 2/3

#   75/75 [==============================] - 6s 80ms/step - loss: 0.2745 - binary_accuracy: 0.8983 - val_loss: 0.2559 - val_binary_accuracy: 0.9087

#   Epoch 3/3

#   75/75 [==============================] - 6s 83ms/step - loss: 0.2513 - binary_accuracy: 0.9167 - val_loss: 0.2385 - val_binary_accuracy: 0.9187

#   <keras.callbacks.History at 0x7f91204f1430>

"""
## Data Re-uploading

By loading the PCA embedding data multiple times in the VQA, we may further increase the accuracy of the model.
"""

def qml(x, weights):
    c = tc.Circuit(n)
    for j in range(blocks):
        for i in range(10):
            c.ry(i, theta=x[i])  # loading the data repetitively
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return K.sigmoid(K.sum(outputs))


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

model = tf.keras.Sequential([qml_layer])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_r, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 20s 146ms/step - loss: 0.2506 - binary_accuracy: 0.9070 - val_loss: 0.2271 - val_binary_accuracy: 0.9177

#   Epoch 2/3

#   75/75 [==============================] - 7s 95ms/step - loss: 0.2191 - binary_accuracy: 0.9246 - val_loss: 0.2071 - val_binary_accuracy: 0.9287

#   Epoch 3/3

#   75/75 [==============================] - 7s 95ms/step - loss: 0.2049 - binary_accuracy: 0.9287 - val_loss: 0.2016 - val_binary_accuracy: 0.9305

#   <keras.callbacks.History at 0x7f90a813b460>



================================================
FILE: docs/source/tutorials/qml_scenarios_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 分类问题的量子机器学习技巧

**一些常见设置和技巧的演示**
"""

"""
## 概述

我们使用 fashion-MNIST 数据集来建立二进制分类任务，我们将尝试不同的输入编码方案，并对量子输出应用可能的经典后处理以提高分类精度。 在本教程中，我们使用 TensorFlow 后端，并尝试始终如一地使用 TensorCircuit 为量子函数提供的 **Keras 接口**，在这里我们可以神奇地将量子函数转换为 Keras 层。
"""

from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
import tensorflow as tf
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## 数据集和预处理

我们首先加载 fashion-mnist 数据集并区分 T 恤 (0) 和裤子 (1)。
"""

(x_train, y_train), (x_test, y_test) = tc.templates.dataset.mnist_pair_data(
    0, 1, loader=tf.keras.datasets.fashion_mnist
)

x_train.shape, y_train.shape, x_test.shape, y_test.shape
# Output:
#   ((12000, 28, 28, 1), (12000,), (2000, 28, 28, 1), (2000,))

plt.imshow(x_train[0])
# Output:
#   <matplotlib.image.AxesImage at 0x7f9147029610>
#   <Figure size 432x288 with 1 Axes>

plt.imshow(x_train[1])
# Output:
#   <matplotlib.image.AxesImage at 0x7f91473b6c10>
#   <Figure size 432x288 with 1 Axes>

"""
## 幅度编码
"""

x_train = tf.image.pad_to_bounding_box(x_train, 2, 2, 32, 32)
x_test = tf.image.pad_to_bounding_box(x_test, 2, 2, 32, 32)

batched_ae = K.vmap(tc.templates.dataset.amplitude_encoding, vectorized_argnums=0)

x_train_q = batched_ae(x_train, 10)
x_test_q = batched_ae(x_test, 10)

n = 10
blocks = 3


def qml(x, weights):
    c = tc.Circuit(n, inputs=x)
    for j in range(blocks):
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return K.sigmoid(K.sum(outputs))


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

model = tf.keras.Sequential([qml_layer])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_q, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 85s 559ms/step - loss: 0.6217 - binary_accuracy: 0.7667 - val_loss: 0.3990 - val_binary_accuracy: 0.9620

#   Epoch 2/3

#   75/75 [==============================] - 14s 185ms/step - loss: 0.3701 - binary_accuracy: 0.9571 - val_loss: 0.3421 - val_binary_accuracy: 0.9507

#   Epoch 3/3

#   75/75 [==============================] - 14s 185ms/step - loss: 0.3252 - binary_accuracy: 0.9542 - val_loss: 0.3030 - val_binary_accuracy: 0.9540

#   <keras.callbacks.History at 0x7f9147416e50>

"""
## 经典后处理

我们在量子输出之后附加了一个线性层，以增强机器学习模型作为量子-神经混合机器学习方法的能力。
"""

def qml(x, weights):
    c = tc.Circuit(n, inputs=x)
    for j in range(blocks):
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return outputs


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

inputs = tf.keras.Input(shape=(2**n), dtype=tf.complex64)
measurements = qml_layer(inputs)
output = tf.keras.layers.Dense(1, activation="sigmoid")(measurements)
model = tf.keras.Model(inputs=inputs, outputs=output)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_q, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 71s 508ms/step - loss: 0.5140 - binary_accuracy: 0.8841 - val_loss: 0.3617 - val_binary_accuracy: 0.9521

#   Epoch 2/3

#   75/75 [==============================] - 14s 182ms/step - loss: 0.2803 - binary_accuracy: 0.9421 - val_loss: 0.2093 - val_binary_accuracy: 0.9506

#   Epoch 3/3

#   75/75 [==============================] - 15s 200ms/step - loss: 0.2057 - binary_accuracy: 0.9437 - val_loss: 0.1795 - val_binary_accuracy: 0.9483

#   <keras.callbacks.History at 0x7f913d4ee520>

"""
## PCA 嵌入

幅度编码很难在真正的量子硬件上实现，我们在这里考虑另一种数据输入方式，其中只涉及单个量子比特旋转。为了压缩输入数据以使其适合小型电路，需要 PCA 降维。
"""

x_train_r = PCA(10).fit_transform(x_train.numpy().reshape([-1, 32 * 32]))

x_train_r.shape  # 我们现在对每个图形进行 10 维向量压缩
# Output:
#   (12000, 10)

def qml(x, weights):
    c = tc.Circuit(n)
    for i in range(10):
        c.rx(i, theta=x[i])  # 加载数据
    for j in range(blocks):
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return K.sigmoid(K.sum(outputs))


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

model = tf.keras.Sequential([qml_layer])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_r, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 71s 447ms/step - loss: 0.7993 - binary_accuracy: 0.6996 - val_loss: 0.3026 - val_binary_accuracy: 0.8829

#   Epoch 2/3

#   75/75 [==============================] - 6s 80ms/step - loss: 0.2745 - binary_accuracy: 0.8983 - val_loss: 0.2559 - val_binary_accuracy: 0.9087

#   Epoch 3/3

#   75/75 [==============================] - 6s 83ms/step - loss: 0.2513 - binary_accuracy: 0.9167 - val_loss: 0.2385 - val_binary_accuracy: 0.9187

#   <keras.callbacks.History at 0x7f91204f1430>

"""
## 数据重新加载

通过在 VQA 中多次加载 PCA 嵌入数据，我们可以进一步提高模型的准确性。
"""

def qml(x, weights):
    c = tc.Circuit(n)
    for j in range(blocks):
        for i in range(10):
            c.ry(i, theta=x[i])  # 重复加载数据
        for i in range(n):
            c.rx(i, theta=weights[j, i, 0])
            c.rz(i, theta=weights[j, i, 1])
        for i in range(n - 1):
            c.exp1(i, i + 1, theta=weights[j, i, 2], unitary=tc.gates._zz_matrix)
    outputs = K.stack(
        [K.real(c.expectation([tc.gates.z(), [i]])) for i in range(n)]
        + [K.real(c.expectation([tc.gates.x(), [i]])) for i in range(n)]
    )
    outputs = K.reshape(outputs, [-1])
    return K.sigmoid(K.sum(outputs))


qml_layer = tc.keras.QuantumLayer(qml, weights_shape=[blocks, n, 3])

model = tf.keras.Sequential([qml_layer])
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.01),
    metrics=[tf.keras.metrics.BinaryAccuracy()],
)

model.fit(x_train_r, y_train, batch_size=32, epochs=3, validation_split=0.8)
# Output:
#   Epoch 1/3

#   75/75 [==============================] - 20s 146ms/step - loss: 0.2506 - binary_accuracy: 0.9070 - val_loss: 0.2271 - val_binary_accuracy: 0.9177

#   Epoch 2/3

#   75/75 [==============================] - 7s 95ms/step - loss: 0.2191 - binary_accuracy: 0.9246 - val_loss: 0.2071 - val_binary_accuracy: 0.9287

#   Epoch 3/3

#   75/75 [==============================] - 7s 95ms/step - loss: 0.2049 - binary_accuracy: 0.9287 - val_loss: 0.2016 - val_binary_accuracy: 0.9305

#   <keras.callbacks.History at 0x7f90a813b460>



================================================
FILE: docs/source/tutorials/sklearn_svc.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Support Vector Classification with SKLearn

Authored by [_Mark (Zixuan) Song_](https://marksong.tech)

We use the `SKLearn` library to implement `SVC` in the following tutorial.
"""

"""
## Overview

The aim of this tutorial is to implant a quantum machine learning (QML) transformer into SVC pipeline. And this is a general introduction to connect `tensorcircuit` with `scikit-learn`.
"""

"""
## Setup

Install `scikit-learn` and `requests`. The data that is going to be used is [German Credit Data by UCI](http://home.cse.ust.hk/~qyang/221/Assignments/German/GermanData.csv)

```bash
pip install scikit-learn requests
```
"""

import tensorcircuit as tc
import tensorflow as tf
from sklearn.svm import SVC
from sklearn import metrics
from time import time
import requests

K = tc.set_backend("tensorflow")

"""
## Data Preprocessing

The data has 20 variables and each is a integer value. In order for the model to use the data, we need to normalize the data to between 0 and 1.
"""

def load_GCN_data():
    link2gcn = "http://home.cse.ust.hk/~qyang/221/Assignments/German/GermanData.csv"
    data = requests.get(link2gcn)
    data = data.text
    data = data.split("\n")[:-1]
    x = None
    y = None

    def destring(string):
        string = string.split(",")
        return_array = []
        for i, v in enumerate(string):
            if v[0] == "A":
                return_array.append(int(v[1 + len(str(i)) :]))
            else:
                return_array.append(int(v))
        return K.cast([return_array[:-1]], dtype="float32"), K.cast(
            [return_array[-1] - 1], dtype="int32"
        )

    for i in data:
        if x is None:
            temp_x, temp_y = destring(i)
            x = K.cast(temp_x, dtype="float32")
            y = K.cast(temp_y, dtype="int32")
        else:
            temp_x, temp_y = destring(i)
            x = K.concat([x, temp_x], axis=0)
            y = K.concat([y, temp_y], axis=0)
    x = K.transpose(x)
    nx = None
    for i in x:
        max_i = K.cast(K.max(i), dtype="float32")
        temp_nx = [K.divide(i, max_i)]
        nx = K.concat([nx, temp_nx], axis=0) if nx is not None else temp_nx
    x = K.transpose(nx)
    return (x[:800], y[:800]), (x[800:], y[800:])


(x_train, y_train), (x_test, y_test) = load_GCN_data()

"""
## Quantum Model

This quantum model takes in 1x20 matrices as input and output the state of 5 qbits. The model is shown below:
"""

def quantumTran(inputs):
    c = tc.Circuit(5)
    for i in range(4):
        if i % 2 == 0:
            for j in range(5):
                c.rx(j, theta=(0 if i * 5 + j >= 20 else inputs[i * 5 + j]))
        else:
            for j in range(5):
                c.rz(j, theta=(0 if i * 5 + j >= 20 else inputs[i * 5 + j]))
            for j in range(4):
                c.cnot(j, j + 1)
    return c.state()


func_qt = tc.interfaces.tensorflow_interface(quantumTran, ydtype=tf.complex64, jit=True)

"""
## Wrapping Quantum Model into a SVC

Convert quantum model into svc that can be trained.
"""

def quantum_kernel(quantumTran, data_x, data_y):
    def kernel(x, y):
        x = K.convert_to_tensor(x)
        y = K.convert_to_tensor(y)
        x_qt = None
        for i, x1 in enumerate(x):
            if i == 0:
                x_qt = K.convert_to_tensor([quantumTran(x1)])
            else:
                x_qt = K.concat([x_qt, [quantumTran(x1)]], 0)
        y_qt = None
        for i, x1 in enumerate(y):
            if i == 0:
                y_qt = K.convert_to_tensor([quantumTran(x1)])
            else:
                y_qt = K.concat([y_qt, [quantumTran(x1)]], 0)
        data_ret = K.cast(K.power(K.abs(x_qt @ K.transpose(y_qt)), 2), "float32")
        return data_ret

    clf = SVC(kernel=kernel)
    clf.fit(data_x, data_y)
    return clf

"""
## Create Traditional SVC
"""

def standard_kernel(data_x, data_y, method):
    methods = ["linear", "poly", "rbf", "sigmoid"]
    if method not in methods:
        raise ValueError("method must be one of %r." % methods)
    clf = SVC(kernel=method)
    clf.fit(data_x, data_y)
    return clf

"""
## Test

Test the accuracy of the quantum model SVC with the test data and compare it with traditional SVC.
"""

methods = ["linear", "poly", "rbf", "sigmoid"]

for method in methods:
    print()
    t = time()

    k = standard_kernel(data_x=x_train, data_y=y_train, method=method)
    y_pred = k.predict(x_test)
    print("Accuracy:(%s as kernel)" % method, metrics.accuracy_score(y_test, y_pred))

    print("time:", time() - t, "seconds")

print()
t = time()

k = quantum_kernel(quantumTran=func_qt, data_x=x_train, data_y=y_train)
y_pred = k.predict(x_test)
print("Accuracy:(qml as kernel)", metrics.accuracy_score(y_test, y_pred))

print("time:", time() - t, "seconds")
# Output:
#   

#   Accuracy:(linear as kernel) 0.78

#   time: 0.007764101028442383 seconds

#   

#   Accuracy:(poly as kernel) 0.75

#   time: 0.024492979049682617 seconds

#   

#   Accuracy:(rbf as kernel) 0.765

#   time: 0.011505126953125 seconds

#   

#   Accuracy:(sigmoid as kernel) 0.695

#   time: 0.010205984115600586 seconds

#   

#   Accuracy:(qml as kernel) 0.66

#   time: 3.0243749618530273 seconds


"""
## Issue with `SKLearn`

Due to the limitation of `SKLearn`, `SKLearn`'s `SVC` is not fully compatible with quantum machine model (QML). 

This is because QML outputs a result as complex number (coordinate on the bloch sphere) whereas SKLearn only accept float. This is causing the result output by QML must be converted into float before it can be used in SVC, leading to a potential loss of accuracy.

## Conclusion

Due to the present limitation of SKLearn, quantum SVC is worse than traditional SVC in both accuracy and speed. However, if the limitation is removed, quantum SVC might be able to outperform traditional SVC in both accuracy.
"""



================================================
FILE: docs/source/tutorials/sklearn_svc_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 结合SKLearn实现的支持向量分类

[_Mark (Zixuan) Song_](https://marksong.tech) 撰写

本示例结合了`sklearn`库中的`SVC`类，实现了支持向量分类。
"""

"""
## 概述

本示例的目的是将量子机器学习（QML）转换器嵌入到SVC管道中并且介绍`tensorcircuit`与`scikit-learn`的一种连接方式。
"""

"""
## 设置

安装 `scikit-learn` 和 `requests`. 本模型测试数据为 [德国信用]The data that is going to be used is [German Credit Data by UCI](http://home.cse.ust.hk/~qyang/221/Assignments/German/GermanData.csv)

```bash
pip install scikit-learn requests
```
"""

import tensorcircuit as tc
import tensorflow as tf
from sklearn.svm import SVC
from sklearn import metrics
from time import time
import requests

K = tc.set_backend("tensorflow")

"""
## 数据处理

数据集包含20个变量，每个变量都是整数值。为了使模型能够使用数据，我们需要将数据归一化为0到1之间。
"""

def load_GCN_data():
    link2gcn = "http://home.cse.ust.hk/~qyang/221/Assignments/German/GermanData.csv"
    data = requests.get(link2gcn)
    data = data.text
    data = data.split("\n")[:-1]
    x = None
    y = None

    def destring(string):
        string = string.split(",")
        return_array = []
        for i, v in enumerate(string):
            if v[0] == "A":
                return_array.append(int(v[1 + len(str(i)) :]))
            else:
                return_array.append(int(v))
        return K.cast([return_array[:-1]], dtype="float32"), K.cast(
            [return_array[-1] - 1], dtype="int32"
        )

    for i in data:
        if x is None:
            temp_x, temp_y = destring(i)
            x = K.cast(temp_x, dtype="float32")
            y = K.cast(temp_y, dtype="int32")
        else:
            temp_x, temp_y = destring(i)
            x = K.concat([x, temp_x], axis=0)
            y = K.concat([y, temp_y], axis=0)
    x = K.transpose(x)
    nx = None
    for i in x:
        max_i = K.cast(K.max(i), dtype="float32")
        temp_nx = [K.divide(i, max_i)]
        nx = K.concat([nx, temp_nx], axis=0) if nx is not None else temp_nx
    x = K.transpose(nx)
    return (x[:800], y[:800]), (x[800:], y[800:])


(x_train, y_train), (x_test, y_test) = load_GCN_data()

"""
## 量子模型

这个量子模型是输入为1x20的矩阵，并输出为5个量子比特的状态。模型如下所示：
"""

def quantumTran(inputs):
    c = tc.Circuit(5)
    for i in range(4):
        if i % 2 == 0:
            for j in range(5):
                c.rx(j, theta=(0 if i * 5 + j >= 20 else inputs[i * 5 + j]))
        else:
            for j in range(5):
                c.rz(j, theta=(0 if i * 5 + j >= 20 else inputs[i * 5 + j]))
            for j in range(4):
                c.cnot(j, j + 1)
    return c.state()


func_qt = tc.interfaces.tensorflow_interface(quantumTran, ydtype=tf.complex64, jit=True)

"""
## 将量子模型打包成SVC

将量子模型打包成`SKLearn`能使用的SVC模型。
"""

def quantum_kernel(quantumTran, data_x, data_y):
    def kernel(x, y):
        x = K.convert_to_tensor(x)
        y = K.convert_to_tensor(y)
        x_qt = None
        for i, x1 in enumerate(x):
            if i == 0:
                x_qt = K.convert_to_tensor([quantumTran(x1)])
            else:
                x_qt = K.concat([x_qt, [quantumTran(x1)]], 0)
        y_qt = None
        for i, x1 in enumerate(y):
            if i == 0:
                y_qt = K.convert_to_tensor([quantumTran(x1)])
            else:
                y_qt = K.concat([y_qt, [quantumTran(x1)]], 0)
        data_ret = K.cast(K.power(K.abs(x_qt @ K.transpose(y_qt)), 2), "float32")
        return data_ret

    clf = SVC(kernel=kernel)
    clf.fit(data_x, data_y)
    return clf

"""
## 创建传统SVC模型
"""

def standard_kernel(data_x, data_y, method):
    methods = ["linear", "poly", "rbf", "sigmoid"]
    if method not in methods:
        raise ValueError("method must be one of %r." % methods)
    clf = SVC(kernel=method)
    clf.fit(data_x, data_y)
    return clf

"""
## 测试对比

测试量子SVC模型并于传统SVC模型进行对比。
"""

methods = ["linear", "poly", "rbf", "sigmoid"]

for method in methods:
    print()
    t = time()

    k = standard_kernel(data_x=x_train, data_y=y_train, method=method)
    y_pred = k.predict(x_test)
    print("Accuracy:(%s as kernel)" % method, metrics.accuracy_score(y_test, y_pred))

    print("time:", time() - t, "seconds")

print()
t = time()

k = quantum_kernel(quantumTran=func_qt, data_x=x_train, data_y=y_train)
y_pred = k.predict(x_test)
print("Accuracy:(qml as kernel)", metrics.accuracy_score(y_test, y_pred))

print("time:", time() - t, "seconds")
# Output:
#   

#   Accuracy:(linear as kernel) 0.78

#   time: 0.00810384750366211 seconds

#   

#   Accuracy:(poly as kernel) 0.75

#   time: 0.024804115295410156 seconds

#   

#   Accuracy:(rbf as kernel) 0.765

#   time: 0.011444091796875 seconds

#   

#   Accuracy:(sigmoid as kernel) 0.695

#   time: 0.010396003723144531 seconds

#   

#   Accuracy:(qml as kernel) 0.66

#   time: 6.472219228744507 seconds


"""
## `SKLearn`的局限性

因为`SKLearn`的局限性，`SKLearn`的`SVC`并不完全兼容量子机器学习（QML）。

这是因为QML输出的为复数（布洛赫球上的坐标），而`SKLearn`只接受浮点数。这导致QML输出的结果必须在使用SVC之前转换为浮点数，从而可能导致精度损失。

## 结论

由于`SKLearn`的局限性，量子SVC在准确性和速度上都不如传统SVC。但是，如果这种局限性被消除，量子SVC可能会在准确性上都优于传统SVC。
"""



================================================
FILE: docs/source/tutorials/stabilizer_circuit.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Simulation of Clifford Circuits
"""

"""
## Overview

Simulating quantum circuits on a classical computer is fundamentally hard. The memory required to store a quantum state vector for $n$ qubits grows as $2^n$, an exponential scaling that quickly becomes intractable. However, a special and highly important subclass of quantum circuits, known as **Clifford circuits**, can be simulated efficiently on a classical computer.

The method used for this is the **stabilizer formalism**. Instead of tracking the $2^n$ amplitudes of the state vector, we track a small set of operators—the *stabilizers*—that leave the quantum state unchanged. For an $n$-qubit system, we only need to track $n$ such operators, and the updates for each Clifford gate can be performed in polynomial time.

This tutorial will introduce how to use `tensorcircuit.StabilizerCircuit`, which leverages the powerful `stim` library as its backend, to perform efficient Clifford circuit simulations. We will cover:

1.  **Creating and Manipulating a Stabilizer Circuit**: How to build a circuit, apply gates, and inspect its state.
2.  **Understanding the Stabilizer Tableau**: Using `StabilizerCircuit` methods to view the underlying tableau representation.
3.  **Handling Measurements and Post-selection**: Demonstrating how `StabilizerCircuit` manages these complex operations.
4.  **Application**: We will then apply these concepts to our main problem: calculating the entanglement entropy of a Clifford circuit with mid-circuit measurements, comparing its performance and results with standard state-vector simulation.
"""

"""
## Setup
"""

import numpy as np
import tensorcircuit as tc

# Set a random seed for reproducibility
np.random.seed(0)

"""
## Creating and Manipulating a Stabilizer Circuit

The `tc.StabilizerCircuit` class provides an interface very similar to the standard `tc.Circuit`, but it is restricted to Clifford gates and uses a `stim.TableauSimulator` internally. This allows for polynomial-time simulation.

Let's start by creating a simple `StabilizerCircuit`.
"""

n = 2
# Initialize a stabilizer circuit for 2 qubits
sc = tc.StabilizerCircuit(n)
print(f"Number of qubits: {sc._nqubits}")
# Output:
#   Number of qubits: 2


"""
We can apply Clifford gates just like with a normal circuit. Let's create a Bell state $|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)$.
"""

# Apply a Hadamard gate to the first qubit
sc.h(0)
# Apply a CNOT gate with qubit 0 as control and qubit 1 as target
sc.cnot(0, 1)

print("Circuit constructed. Let's inspect its properties.")
# Output:
#   Circuit constructed. Let's inspect its properties.


"""
## Understanding the Stabilizer Tableau

The core of a stabilizer simulation is the **stabilizer tableau**. We can access this directly from our `StabilizerCircuit` object.
"""

# Get the current stabilizer tableau
# This represents the final state of the circuit
final_tableau = sc.current_tableau()

print("Tableau for the Bell state |Φ+>:")
print(final_tableau)
# Output:
#   Tableau for the Bell state |Φ+>:

#   +-xz-xz-

#   | ++ ++

#   | ZX _Z

#   | _X XZ


"""
The `z_output(i)` method of a `stim.Tableau` object tells us what the initial $Z_i$ operator evolves into under the circuit's action. These evolved operators are the stabilizers of the final state.
"""

print("Stabilizers for the Bell state |Φ+>:")
for i in range(n):
    # For a state |psi> = U|0...0>, the stabilizers are U Z_i U_dag
    # which is what tableau.z_output(i) returns.
    print(f"S_{i+1}: {final_tableau.z_output(i)}")
# Output:
#   Stabilizers for the Bell state |Φ+>:

#   S_1: +XX

#   S_2: +ZZ


"""
As expected, the stabilizers for the Bell state are correctly identified as `X_0 X_1` and `Z_0 Z_1`.

We can also get the full state vector, though this operation is computationally expensive and defeats the purpose of stabilizer simulation for large systems. It is, however, useful for verifying results on small circuits.

"""

# Get the state vector from the tableau
state_vector = sc.state()

print("State vector for the Bell state:")
print(np.round(state_vector, 3))
# Output:
#   State vector for the Bell state:

#   [0.707+0.j 0.   +0.j 0.   +0.j 0.707+0.j]


"""
The result is $\frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)$, which is correct.

"""

"""
## Handling Measurements and Post-selection

`StabilizerCircuit` provides methods to handle both probabilistic and deterministic (post-selected) measurements.

### Probabilistic Measurement
The `measure` method returns a random outcome based on the state's probabilities. For a stabilizer state, if a measurement operator anti-commutes with a stabilizer, the outcome is random (0 or 1 with 50% probability). If it commutes, the outcome is deterministic.


"""

sc_plus = tc.StabilizerCircuit(2)
sc_plus.h(0)

# The stabilizer is X_0. Z_0 anti-commutes with X_0.
print(
    "The first stabilizer for |+0> state:", sc_plus.current_tableau().z_output(0)
)  # This should show an X

# Measure multiple times to see the randomness
print("Measuring a qubit in the |+> state:")
outcomes = [sc_plus.measure(0) for _ in range(10)]
print(f"10 measurement outcomes: {outcomes}")
# Note: Since measure does not collapse the state in StabilizerCircuit, each measurement is independent.
# For a collapsing measurement, use cond_measure.
outcomes = [sc_plus.measure(0, 1, with_prob=True) for _ in range(10)]
print(outcomes)
# Output:
#   The first stabilizer for |+0> state: +X

#   Measuring a qubit in the |+> state:

#   10 measurement outcomes: [[True], [False], [False], [False], [False], [False], [False], [True], [False], [True]]

#   [([False, False], 0.5), ([False, False], 0.5), ([False, False], 0.5), ([False, False], 0.5), ([True, False], 0.5), ([True, False], 0.5), ([True, False], 0.5), ([False, False], 0.5), ([False, False], 0.5), ([True, False], 0.5)]


"""

### Post-selection
The `mid_measurement` (or `post_selection`) method allows us to project the state onto a specific measurement outcome. This is a non-unitary operation that deterministically collapses the state and updates the stabilizer tableau accordingly. This is crucial for our main application.
"""

# Create a Bell state
sc_bell = tc.StabilizerCircuit(2)
sc_bell.h(0)
sc_bell.cnot(0, 1)

print("Original Bell state stabilizers:")
for i in range(2):
    print(sc_bell.current_tableau().z_output(i))

# Now, perform a mid-measurement on qubit 0 and keep the '0' outcome
sc_bell.post_select(0, keep=0)

print("\nStabilizers after post-selecting qubit 0 to be |0>:")
for i in range(2):
    print(sc_bell.current_tableau().z_output(i))

# Let's check the final state vector
final_state = sc_bell.state()
print(f"\nFinal state vector: {np.round(final_state, 3)}")
# Output:
#   Original Bell state stabilizers:

#   +XX

#   +ZZ

#   

#   Stabilizers after post-selecting qubit 0 to be |0>:

#   +Z_

#   +ZZ

#   

#   Final state vector: [1.+0.j 0.+0.j 0.+0.j 0.+0.j]


"""
## Application: Entanglement Entropy with Mid-Circuit Measurements

Now, let's use `tc.StabilizerCircuit` to solve the problem efficiently. We will create a random Clifford circuit with mid-circuit measurements and then calculate the entanglement entropy directly using a built-in method.

### 1. Generating the Circuit

We'll write a function that builds a random Clifford circuit using `tc.StabilizerCircuit`.

"""

def random_stabilizer_circuit_with_mid_measurement(num_qubits, depth):
    """Generates a random Clifford circuit using tc.StabilizerCircuit."""
    sc = tc.StabilizerCircuit(num_qubits)

    for _ in range(depth):
        # Apply random gates to random pairs
        for j in range(num_qubits - 1):
            sc.random_gate(j, j + 1)

        # With 20% probability, perform a mid-circuit measurement
        for j in range(num_qubits - 1):
            if np.random.uniform() < 0.2:
                sc.cond_measure(j)
    return sc

num_qubits = 12
depth = 12
cut = [i for i in range(num_qubits // 2)]

# Generate the stabilizer circuit
stabilizer_circuit = random_stabilizer_circuit_with_mid_measurement(num_qubits, depth)

"""
## 2. Calculating Entropy from the Stabilizer Circuit

With the `StabilizerCircuit` object, calculating the entanglement entropy is a one-liner. The `entanglement_entropy` method implements the rank-based formula, providing a highly efficient calculation.

"""

entropy = stabilizer_circuit.entanglement_entropy(cut)

print(f"Entanglement Entropy (from StabilizerCircuit): {entropy}")
# Output:
#   Entanglement Entropy (from StabilizerCircuit): 2.0794415416798357


"""
## Final Conclusion

This tutorial demonstrates the power and convenience of `tensorcircuit.StabilizerCircuit`. By providing a user-friendly interface that mirrors `tc.Circuit` while using the highly optimized `stim` library as its backend, it enables efficient simulation of large-scale Clifford circuits. Operations that are computationally prohibitive with state-vector methods, like calculating entanglement entropy in circuits with dozens or hundreds of qubits, become feasible. This makes `StabilizerCircuit` an essential tool for research in quantum error correction, fault tolerance, and any domain involving Clifford dynamics.
"""



================================================
FILE: docs/source/tutorials/template.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Title
"""

"""
## Overview

[Include a paragraph or two explaining what this example demonstrates, who should be interested in it, and what you need to know before you get started.]
"""

"""
## Background
 
[If the topic in this tutorial is involved, a background section may be required, possibly with heavy math:]

$$ \frac{\pi^2}{6} = \sum_{i=1}^\infty \frac{1}{i^2}$$
"""

"""
## Setup

[Basic setups, such as package imports and configs on imports, better include all setup codes in one block]
"""

import numpy as np
import tensorcircuit as tc

tc.set_backend("tensorflow")
print(tc.__version__)
# Output:
#   0.0.1


"""
## Problem related sections

[Sections with explanation and code, such as Methods, Results, Comparisons, Discussions, Implementations, etc.]
"""

# Build the quantum function


def f(weights, n, nlayers):
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
    for j in range(nlayers):
        for i in range(n):
            c.rx(i, theta=weights[j, i])
    return c.wavefunction()[0]

"""
## Guidelines and styles

* Include copyright line at the top.
* Use one `H1` header for the title.
* Avoid using `H1` headers for section titles. Use `H2` and `H3` instead.
* Include an overview section in plain text before any code.
* Put all your installs and imports in the setup section.
* Keep code and text cells as brief as possible, remove all unused code in the tutorial.
* Avoid leaving an empty cell at the end of the notebook.
* Use `black` to format the jupyter notebooks.
* Keep examples quick. It should at most take several minutes to run all commands from one tutorial.
* Restart kernel and run all command from the beginning before committing.
"""

"""
## References

1. citation format in APS form

2. research paper references should be included if applicable

3. [link](http://fake.urls) for the references should also be included if applicable
"""



================================================
FILE: docs/source/tutorials/template_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Title
"""

"""
## Overview

[Include a paragraph or two explaining what this example demonstrates, who should be interested in it, and what you need to know before you get started.]
"""

"""
## Background
 
[If the topic in this tutorial is involved, a background section may be required, possibly with heavy math:]

$$ \frac{\pi^2}{6} = \sum_{i=1}^\infty \frac{1}{i^2}$$
"""

"""
## Setup

[Basic setups, such as package imports and configs on imports, better include all setup codes in one block]
"""

import numpy as np
import tensorcircuit as tc

tc.set_backend("tensorflow")
print(tc.__version__)
# Output:
#   0.0.1


"""
## Problem related sections

[Sections with explanation and code, such as Methods, Results, Comparisons, Discussions, Implementations, etc.]
"""

# Build the quantum function


def f(weights, n, nlayers):
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
    for j in range(nlayers):
        for i in range(n):
            c.rx(i, theta=weights[j, i])
    return c.wavefunction()[0]

"""
## Guidelines and styles

* Include copyright line at the top.
* Use one `H1` header for the title.
* Avoid using `H1` headers for section titles. Use `H2` and `H3` instead.
* Include an overview section in plain text before any code.
* Put all your installs and imports in the setup section.
* Keep code and text cells as brief as possible, remove all unused code in the tutorial.
* Avoid leaving an empty cell at the end of the notebook.
* Use `black` to format the jupyter notebooks.
* Keep examples quick. It should at most take several minutes to run all commands from one tutorial.
* Restart kernel and run all command from the beginning before committing.
"""

"""
## References

1. citation format in APS form

2. research paper references should be included if applicable

3. [link](http://fake.urls) for the references should also be included if applicable
"""



================================================
FILE: docs/source/tutorials/tfim_vqe.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# VQE on 1D TFIM
"""

"""
## Overview

The main aim of this tutorial is not about the physics perspective of VQE, instead, we demonstrate
the main ingredients of TensorCircuit by this simple VQE toy model.
"""

"""
## Background

Basically, we train a parameterized quantum circuit with repetitions of $e^{i\theta} ZZ$ and $e^{i\theta X}$ layers as $U(\rm{\theta})$. And the objective to be minimized is this task is $\mathcal{L}(\rm{\theta})=\langle 0^n\vert U(\theta)^\dagger H U(\theta)\vert 0^n\rangle$. The Hamiltonian is from TFIM as $H = \sum_{i} Z_iZ_{i+1} -\sum_i X_i$.
"""

"""
## Setup
"""

from functools import partial
import numpy as np
import tensorflow as tf
import jax
from jax.config import config

config.update("jax_enable_x64", True)
from jax import numpy as jnp
from jax.experimental import optimizers
import tensorcircuit as tc

"""
To enable automatic differentiation support, we should set the TensorCircuit backend beyond the default one "NumPy".
And we can also set the high precision complex128 for the simulation.
"""

tc.set_backend("tensorflow")
tc.set_dtype("complex128")

print(
    "complex dtype of simulation:",
    tc.dtypestr,
    "\nreal dtype of simulation:",
    tc.rdtypestr,
    "\nbackend package of simulation:",
    tc.backend.name,
)
# Output:
#   complex dtype of simulation: complex128 

#   real dtype of simulation: float64 

#   backend package of simulation: tensorflow


# zz gate matrix to be utilized
zz = np.kron(tc.gates._z_matrix, tc.gates._z_matrix)
print(zz)
# Output:
#   [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
## Higher-level API
"""

"""
We first design the Hamiltonian energy expectation function with the input as quantum circuit.
"""

def tfi_energy(c: tc.Circuit, j: float = 1.0, h: float = -1.0):
    e = 0.0
    n = c._nqubits
    for i in range(n):
        e += h * c.expectation((tc.gates.x(), [i]))  # <X_i>
    for i in range(n - 1):  # OBC
        e += j * c.expectation(
            (tc.gates.z(), [i]), (tc.gates.z(), [(i + 1) % n])
        )  # <Z_iZ_{i+1}>
    return tc.backend.real(e)

"""
Now we make the quantum function with $\rm{\theta}$ as input and energy expectation $\mathcal{L}$ as output.
"""

def vqe_tfim(param, n, nlayers):
    c = tc.Circuit(n)
    paramc = tc.backend.cast(
        param, tc.dtypestr
    )  # We assume the input param with dtype float64
    for i in range(n):
        c.H(i)
    for j in range(nlayers):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=zz, theta=paramc[2 * j, i])
        for i in range(n):
            c.rx(i, theta=paramc[2 * j + 1, i])
    e = tfi_energy(c)
    return e

"""
To train the parameterized circuit, we should utilize the gradient information $\frac{\partial \mathcal{L}}{\partial \rm{\theta}}$ with gradient descent.
We also use ``jit`` to wrap the value and grad function for a substantial speedup. Note how (1, 2) args of ``vqe_tfim`` are labeled as static since they are just integers for qubit number and layer number instead of tensors.
"""

vqe_tfim_vag = tc.backend.jit(
    tc.backend.value_and_grad(vqe_tfim), static_argnums=(1, 2)
)

def train_step_tf(n, nlayers, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )
    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_vag(param, n, nlayers)
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e


train_step_tf(6, 3, 2000)
# Output:
#   tf.Tensor(-5.2044235531710905, shape=(), dtype=float64)

#   tf.Tensor(-7.168460907768175, shape=(), dtype=float64)

#   tf.Tensor(-7.229007202330065, shape=(), dtype=float64)

#   tf.Tensor(-7.2368387790165105, shape=(), dtype=float64)

#   tf.Tensor(-7.246179597523659, shape=(), dtype=float64)

#   tf.Tensor(-7.262673966580785, shape=(), dtype=float64)

#   tf.Tensor(-7.286129364991173, shape=(), dtype=float64)

#   tf.Tensor(-7.291252895716095, shape=(), dtype=float64)

#   tf.Tensor(-7.2930457160020765, shape=(), dtype=float64)

#   tf.Tensor(-7.293225326335964, shape=(), dtype=float64)

#   <tf.Tensor: shape=(), dtype=float64, numpy=-7.293297606006469>

"""
### Batched VQE Example

We can even run a batched version of VQE optimization, namely, we simultaneously optimize parameterized circuits for different random initializations, so that we can try our best to avoid local minimums and locate the best of the converged energies.
"""

vqe_tfim_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim), static_argnums=(1, 2)
)

def batched_train_step_tf(batch, n, nlayers, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[batch, nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )
    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_vvag(param, n, nlayers)
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e


batched_train_step_tf(16, 6, 3, 2000)
# Output:
#   tf.Tensor(

#   [-4.56780182 -5.32411397 -5.34948039 -5.49728838 -5.51974631 -4.89464895

#    -5.23113926 -5.70097167 -5.4384308  -5.27898261 -4.73926061 -5.43748391

#    -5.02246224 -4.46749643 -5.34320604 -5.29828815], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.15906597 -7.20867528 -7.16615816 -7.16164269 -7.15427498 -7.17176534

#    -7.15677645 -7.19769858 -7.1876547  -7.17160745 -7.14313137 -7.16458417

#    -7.12556993 -7.1043696  -7.17233218 -7.17955502], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.22332735 -7.28775096 -7.22854626 -7.28800389 -7.22006811 -7.2773814

#    -7.22241623 -7.23446324 -7.23115651 -7.23081143 -7.25399986 -7.26564648

#    -7.16463543 -7.27854832 -7.23574558 -7.28935649], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.23956454 -7.29093555 -7.23464822 -7.2914774  -7.22326999 -7.29014637

#    -7.24891067 -7.2505597  -7.23879431 -7.23826618 -7.28737831 -7.29193732

#    -7.22649018 -7.29136679 -7.25276205 -7.29214669], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.24561853 -7.29413883 -7.23950499 -7.29230127 -7.22749993 -7.29051998

#    -7.28702174 -7.289441   -7.25016979 -7.26370483 -7.29320874 -7.29451577

#    -7.22882824 -7.29213765 -7.27040912 -7.29358236], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.24997971 -7.294748   -7.25420008 -7.29271584 -7.24577837 -7.29082466

#    -7.29171805 -7.29016935 -7.28645108 -7.29170429 -7.29499124 -7.29520514

#    -7.23115011 -7.29305292 -7.28793637 -7.2949226 ], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.25300306 -7.29512508 -7.28240557 -7.29287622 -7.28264095 -7.29125472

#    -7.29399162 -7.29066326 -7.29233232 -7.29290676 -7.29521188 -7.29530935

#    -7.23475933 -7.29429836 -7.29053038 -7.29559969], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.25706762 -7.29527205 -7.29168082 -7.29292216 -7.29221412 -7.29183888

#    -7.29474989 -7.29119684 -7.29306204 -7.29300514 -7.29525079 -7.29538907

#    -7.24226472 -7.29544118 -7.29086393 -7.29576418], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.26218683 -7.29529443 -7.29438024 -7.29294969 -7.29426403 -7.29243576

#    -7.29491073 -7.29171828 -7.29384631 -7.29301505 -7.29527304 -7.29545727

#    -7.25772904 -7.29581457 -7.2910381  -7.29582192], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.26654138 -7.29529938 -7.29515899 -7.29297612 -7.29499748 -7.29281659

#    -7.29500531 -7.29227399 -7.29455691 -7.29302087 -7.29529063 -7.29551808

#    -7.2892432  -7.29593328 -7.29120759 -7.29585771], shape=(16,), dtype=float64)

#   <tf.Tensor: shape=(16,), dtype=float64, numpy=

#   array([-7.29011428, -7.29530356, -7.29549915, -7.29300424, -7.29529224,

#          -7.29296047, -7.29508027, -7.29270021, -7.29499648, -7.29302725,

#          -7.29530574, -7.2955733 , -7.29593608, -7.29604964, -7.29138482,

#          -7.29589095])>

"""
### Different Backends

We can change the backends at runtime without even changing one line of the code!

However, in normal user cases, we strongly recommend the users stick to one backend in one jupyter or python script.
One can enjoy the facility provided by other backends by changing the ``set_backend`` line and running the same script again. This approach is much safer than using multiple backends in the same file unless you know the lower-level details of TensorCircuit enough.
"""

tc.set_backend("jax")  # change to jax backend

vqe_tfim_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim), static_argnums=(1, 2)
)


def batched_train_step_jax(batch, n, nlayers, maxiter=10000):
    key = jax.random.PRNGKey(42)
    param = jax.random.normal(key, shape=[batch, nlayers * 2, n]) * 0.1
    opt_init, opt_update, get_params = optimizers.adam(step_size=1e-2)
    opt_state = opt_init(param)

    def update(i, opt_state):
        param = get_params(opt_state)
        (value, gradient) = vqe_tfim_vvag(param, n, nlayers)
        return value, opt_update(i, gradient, opt_state)

    for i in range(maxiter):
        value, opt_state = update(i, opt_state)
        param = get_params(opt_state)
        if i % 200 == 0:
            print(value)
    return value


batched_train_step_jax(16, 6, 3, 2000)
# Output:
#   WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

#   [-5.67575948 -5.44768444 -5.7821556  -5.36699503 -5.00485098 -5.59416181

#    -5.13421084 -5.70462279 -5.73699416 -5.25819658 -4.70729299 -5.82823766

#    -5.69154358 -5.51112311 -5.46091316 -5.31649863]

#   [-7.16831387 -7.17873365 -7.21905991 -7.17714641 -7.21910053 -7.17729778

#    -7.23594046 -7.1978075  -7.2311691  -7.18566164 -7.15141273 -7.1760751

#    -7.20727055 -7.22174427 -7.15227955 -7.15343225]

#   [-7.24047827 -7.23486717 -7.26382185 -7.25267406 -7.23938877 -7.24135079

#    -7.28655961 -7.24413064 -7.28070556 -7.24825735 -7.23400189 -7.25234153

#    -7.25756263 -7.2505181  -7.22647645 -7.2589444 ]

#   [-7.28642159 -7.23707926 -7.28988032 -7.28627451 -7.28716418 -7.25068739

#    -7.29122589 -7.2510777  -7.2906953  -7.25976327 -7.23891735 -7.29227009

#    -7.28973637 -7.26238069 -7.245065   -7.29155041]

#   [-7.29198674 -7.24196434 -7.29188725 -7.29243688 -7.2926968  -7.26254168

#    -7.29233808 -7.26729904 -7.29277165 -7.28066403 -7.24315235 -7.29344766

#    -7.2920645  -7.26717433 -7.26959622 -7.29307748]

#   [-7.29320541 -7.27162341 -7.29245991 -7.2934821  -7.29360574 -7.27103573

#    -7.29311302 -7.29213505 -7.29356392 -7.29162927 -7.24981922 -7.29384423

#    -7.2930642  -7.27323089 -7.29156252 -7.293684  ]

#   [-7.29384132 -7.29130123 -7.29333191 -7.29442748 -7.29396942 -7.29011734

#    -7.2936549  -7.2929216  -7.29430937 -7.29288465 -7.27758063 -7.29446428

#    -7.2939575  -7.27958774 -7.29260011 -7.29408982]

#   [-7.29433271 -7.29273603 -7.29467579 -7.29507754 -7.29438517 -7.29298445

#    -7.2940647  -7.29297896 -7.29494014 -7.29348052 -7.29142477 -7.29524322

#    -7.29455267 -7.28904449 -7.29319333 -7.29437335]

#   [-7.29480095 -7.29294578 -7.29507043 -7.29530148 -7.29479745 -7.29390014

#    -7.29439439 -7.29304737 -7.29520739 -7.29397645 -7.29276852 -7.2955135

#    -7.29492853 -7.29145529 -7.29390729 -7.29462868]

#   [-7.29530498 -7.29299558 -7.29520643 -7.29537379 -7.29505127 -7.2945667

#    -7.29466477 -7.29312996 -7.29529573 -7.29447499 -7.29327567 -7.29560796

#    -7.29520605 -7.29179062 -7.29484071 -7.29489978]

#   DeviceArray([-7.29575181, -7.29302602, -7.29529976, -7.29541094,

#                -7.29517778, -7.29488194, -7.29487651, -7.29323608,

#                -7.29532772, -7.29494698, -7.29369784, -7.29567791,

#                -7.29534388, -7.29187906, -7.29536221, -7.29516005],            dtype=float64)

"""
## Lower-level API

The higher-level API under the namespace of ``TensorCircuit`` provides a unified framework to do linear algebra and automatic differentiation which is backend agnostic.

One may also use the related APIs (ops, AD-related, jit-related) directly provided by TensorFlow or Jax, as long as one is ok to stick with one fixed backend. See the tensorflow backend example below.

"""

tc.set_backend("tensorflow")

def tfi_energy(c: tc.Circuit, j: float = 1.0, h: float = -1.0):
    e = 0.0
    n = c._nqubits
    for i in range(n):
        e += h * c.expectation((tc.gates.x(), [i]))  # <X_i>
    for i in range(n - 1):  # OBC
        e += j * c.expectation(
            (tc.gates.z(), [i]), (tc.gates.z(), [(i + 1) % n])
        )  # <Z_iZ_{i+1}>
    return tf.math.real(e)


def vqe_tfim(param, n, nlayers):
    c = tc.Circuit(n)
    paramc = tf.cast(param, tf.complex128)
    for i in range(n):
        c.H(i)
    for j in range(nlayers):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=zz, theta=paramc[2 * j, i])
        for i in range(n):
            c.rx(i, theta=paramc[2 * j + 1, i])
    e = tfi_energy(c)
    return e


@tf.function
def vqe_tfim_vag(param, n, nlayers):
    with tf.GradientTape() as tape:
        tape.watch(param)
        v = vqe_tfim(param, n, nlayers)
    grad = tape.gradient(v, param)
    return v, grad

train_step_tf(6, 3, 2000)
# Output:
#   tf.Tensor(-5.5454151788179376, shape=(), dtype=float64)

#   tf.Tensor(-7.167693061786028, shape=(), dtype=float64)

#   tf.Tensor(-7.254761404891117, shape=(), dtype=float64)

#   tf.Tensor(-7.290050014550046, shape=(), dtype=float64)

#   tf.Tensor(-7.29133881232428, shape=(), dtype=float64)

#   tf.Tensor(-7.2918048286324915, shape=(), dtype=float64)

#   tf.Tensor(-7.292590929769901, shape=(), dtype=float64)

#   tf.Tensor(-7.294195015132205, shape=(), dtype=float64)

#   tf.Tensor(-7.295013538531699, shape=(), dtype=float64)

#   tf.Tensor(-7.2951174084838835, shape=(), dtype=float64)

#   <tf.Tensor: shape=(), dtype=float64, numpy=-7.295170441938537>



================================================
FILE: docs/source/tutorials/tfim_vqe_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 一维横场 Ising 模型 VQE
"""

"""
## 概述

本教程的主要目的不是关于 VQE 物理层面的讨论，而是我们通过演示
这个简单的 VQE 玩具模型来了解张量电路的主要技术组件和用法。
"""

"""
## 背景

基本上，我们训练一个参数化的量子电路，其线路结构为重复的 $e^{i\theta} ZZ$ 和 $e^{i\theta X}$ 层的 $U(\rm{\theta})$。 而要最小化的目标是这个任务 $\mathcal{L}(\rm{\theta})=\langle 0^n\vert U(\theta)^\dagger HU(\theta)\vert 0^n \rangle$。 哈密顿量来自 TFIM，$H = \sum_{i} Z_iZ_{i+1} -\sum_i X_i$。
"""

"""
## 设置
"""

from functools import partial
import numpy as np
import tensorflow as tf
import jax
from jax.config import config

config.update("jax_enable_x64", True)
from jax import numpy as jnp
from jax.experimental import optimizers
import tensorcircuit as tc

"""
为了启用自动微分支持，我们应该将 TensorCircuit 设置为非 “numpy” 后端。
而且我们还可以设置高精度 complex128 进行模拟。
"""

tc.set_backend("tensorflow")
tc.set_dtype("complex128")

print(
    "complex dtype of simulation:",
    tc.dtypestr,
    "\nreal dtype of simulation:",
    tc.rdtypestr,
    "\nbackend package of simulation:",
    tc.backend.name,
)
# Output:
#   complex dtype of simulation: complex128 

#   real dtype of simulation: float64 

#   backend package of simulation: tensorflow


# 要使用的 zz 门矩阵
zz = np.kron(tc.gates._z_matrix, tc.gates._z_matrix)
print(zz)
# Output:
#   [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
## 更高层的 API
"""

"""
我们首先设计了以量子电路为输入的哈密顿能量期望函数。
"""

def tfi_energy(c: tc.Circuit, j: float = 1.0, h: float = -1.0):
    e = 0.0
    n = c._nqubits
    for i in range(n):
        e += h * c.expectation((tc.gates.x(), [i]))  # <X_i>
    for i in range(n - 1):  # OBC
        e += j * c.expectation(
            (tc.gates.z(), [i]), (tc.gates.z(), [(i + 1) % n])
        )  # <Z_iZ_{i+1}>
    return tc.backend.real(e)

"""
现在我们以 $\rm{\theta}$ 作为输入；并将能量期望 $\mathcal{L}$ 作为输出来制作量子函数。
"""

def vqe_tfim(param, n, nlayers):
    c = tc.Circuit(n)
    paramc = tc.backend.cast(param, tc.dtypestr)  # 我们假设输入参数的 dtype 为 float64
    for i in range(n):
        c.H(i)
    for j in range(nlayers):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=zz, theta=paramc[2 * j, i])
        for i in range(n):
            c.rx(i, theta=paramc[2 * j + 1, i])
    e = tfi_energy(c)
    return e

"""
为了训练参数化电路，我们应该利用梯度下降的梯度信息 $\frac{\partial \mathcal{L}}{\partial \rm{\theta}}$。
我们还使用 ``jit`` 来包装 value 和 grad 函数以显著加快速度。 注意 ``vqe_tfim`` 的 (1, 2) args 是如何被标记为静态的，因为它们只是量子比特数和层数的整数，而不是张量。
"""

vqe_tfim_vag = tc.backend.jit(
    tc.backend.value_and_grad(vqe_tfim), static_argnums=(1, 2)
)

def train_step_tf(n, nlayers, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )
    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_vag(param, n, nlayers)
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e


train_step_tf(6, 3, 2000)
# Output:
#   tf.Tensor(-5.2044235531710905, shape=(), dtype=float64)

#   tf.Tensor(-7.168460907768175, shape=(), dtype=float64)

#   tf.Tensor(-7.229007202330065, shape=(), dtype=float64)

#   tf.Tensor(-7.2368387790165105, shape=(), dtype=float64)

#   tf.Tensor(-7.246179597523659, shape=(), dtype=float64)

#   tf.Tensor(-7.262673966580785, shape=(), dtype=float64)

#   tf.Tensor(-7.286129364991173, shape=(), dtype=float64)

#   tf.Tensor(-7.291252895716095, shape=(), dtype=float64)

#   tf.Tensor(-7.2930457160020765, shape=(), dtype=float64)

#   tf.Tensor(-7.293225326335964, shape=(), dtype=float64)

#   <tf.Tensor: shape=(), dtype=float64, numpy=-7.293297606006469>

"""
### 批处理 VQE 示例

我们甚至可以运行批量版本的 VQE 优化，即我们针对不同的随机初始化同时优化参数化电路，这样我们就可以尽量避免局部最小值，从而找到收敛能量的最佳值。
"""

vqe_tfim_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim), static_argnums=(1, 2)
)

def batched_train_step_tf(batch, n, nlayers, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[batch, nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )
    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_vvag(param, n, nlayers)
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e


batched_train_step_tf(16, 6, 3, 2000)
# Output:
#   tf.Tensor(

#   [-4.56780182 -5.32411397 -5.34948039 -5.49728838 -5.51974631 -4.89464895

#    -5.23113926 -5.70097167 -5.4384308  -5.27898261 -4.73926061 -5.43748391

#    -5.02246224 -4.46749643 -5.34320604 -5.29828815], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.15906597 -7.20867528 -7.16615816 -7.16164269 -7.15427498 -7.17176534

#    -7.15677645 -7.19769858 -7.1876547  -7.17160745 -7.14313137 -7.16458417

#    -7.12556993 -7.1043696  -7.17233218 -7.17955502], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.22332735 -7.28775096 -7.22854626 -7.28800389 -7.22006811 -7.2773814

#    -7.22241623 -7.23446324 -7.23115651 -7.23081143 -7.25399986 -7.26564648

#    -7.16463543 -7.27854832 -7.23574558 -7.28935649], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.23956454 -7.29093555 -7.23464822 -7.2914774  -7.22326999 -7.29014637

#    -7.24891067 -7.2505597  -7.23879431 -7.23826618 -7.28737831 -7.29193732

#    -7.22649018 -7.29136679 -7.25276205 -7.29214669], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.24561853 -7.29413883 -7.23950499 -7.29230127 -7.22749993 -7.29051998

#    -7.28702174 -7.289441   -7.25016979 -7.26370483 -7.29320874 -7.29451577

#    -7.22882824 -7.29213765 -7.27040912 -7.29358236], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.24997971 -7.294748   -7.25420008 -7.29271584 -7.24577837 -7.29082466

#    -7.29171805 -7.29016935 -7.28645108 -7.29170429 -7.29499124 -7.29520514

#    -7.23115011 -7.29305292 -7.28793637 -7.2949226 ], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.25300306 -7.29512508 -7.28240557 -7.29287622 -7.28264095 -7.29125472

#    -7.29399162 -7.29066326 -7.29233232 -7.29290676 -7.29521188 -7.29530935

#    -7.23475933 -7.29429836 -7.29053038 -7.29559969], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.25706762 -7.29527205 -7.29168082 -7.29292216 -7.29221412 -7.29183888

#    -7.29474989 -7.29119684 -7.29306204 -7.29300514 -7.29525079 -7.29538907

#    -7.24226472 -7.29544118 -7.29086393 -7.29576418], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.26218683 -7.29529443 -7.29438024 -7.29294969 -7.29426403 -7.29243576

#    -7.29491073 -7.29171828 -7.29384631 -7.29301505 -7.29527304 -7.29545727

#    -7.25772904 -7.29581457 -7.2910381  -7.29582192], shape=(16,), dtype=float64)

#   tf.Tensor(

#   [-7.26654138 -7.29529938 -7.29515899 -7.29297612 -7.29499748 -7.29281659

#    -7.29500531 -7.29227399 -7.29455691 -7.29302087 -7.29529063 -7.29551808

#    -7.2892432  -7.29593328 -7.29120759 -7.29585771], shape=(16,), dtype=float64)

#   <tf.Tensor: shape=(16,), dtype=float64, numpy=

#   array([-7.29011428, -7.29530356, -7.29549915, -7.29300424, -7.29529224,

#          -7.29296047, -7.29508027, -7.29270021, -7.29499648, -7.29302725,

#          -7.29530574, -7.2955733 , -7.29593608, -7.29604964, -7.29138482,

#          -7.29589095])>

"""
### 不同的后端

我们可以在运行时更改后端，甚至无需更改一行代码！
但是，在普通用户情况下，我们强烈建议用户在一个 jupyter 或 python 脚本中坚持使用一个后端。
通过更改``set_backend``行并再次运行相同的脚本，可以享受其他后端提供的便利。 这种方法比在同一个文件中使用多个后端更安全，除非你足够了解 TensorCircuit 的底层细节。
"""

tc.set_backend("jax")  # 更改为 jax 后端

vqe_tfim_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim), static_argnums=(1, 2)
)


def batched_train_step_jax(batch, n, nlayers, maxiter=10000):
    key = jax.random.PRNGKey(42)
    param = jax.random.normal(key, shape=[batch, nlayers * 2, n]) * 0.1
    opt_init, opt_update, get_params = optimizers.adam(step_size=1e-2)
    opt_state = opt_init(param)

    def update(i, opt_state):
        param = get_params(opt_state)
        (value, gradient) = vqe_tfim_vvag(param, n, nlayers)
        return value, opt_update(i, gradient, opt_state)

    for i in range(maxiter):
        value, opt_state = update(i, opt_state)
        param = get_params(opt_state)
        if i % 200 == 0:
            print(value)
    return value


batched_train_step_jax(16, 6, 3, 2000)
# Output:
#   WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

#   [-5.67575948 -5.44768444 -5.7821556  -5.36699503 -5.00485098 -5.59416181

#    -5.13421084 -5.70462279 -5.73699416 -5.25819658 -4.70729299 -5.82823766

#    -5.69154358 -5.51112311 -5.46091316 -5.31649863]

#   [-7.16831387 -7.17873365 -7.21905991 -7.17714641 -7.21910053 -7.17729778

#    -7.23594046 -7.1978075  -7.2311691  -7.18566164 -7.15141273 -7.1760751

#    -7.20727055 -7.22174427 -7.15227955 -7.15343225]

#   [-7.24047827 -7.23486717 -7.26382185 -7.25267406 -7.23938877 -7.24135079

#    -7.28655961 -7.24413064 -7.28070556 -7.24825735 -7.23400189 -7.25234153

#    -7.25756263 -7.2505181  -7.22647645 -7.2589444 ]

#   [-7.28642159 -7.23707926 -7.28988032 -7.28627451 -7.28716418 -7.25068739

#    -7.29122589 -7.2510777  -7.2906953  -7.25976327 -7.23891735 -7.29227009

#    -7.28973637 -7.26238069 -7.245065   -7.29155041]

#   [-7.29198674 -7.24196434 -7.29188725 -7.29243688 -7.2926968  -7.26254168

#    -7.29233808 -7.26729904 -7.29277165 -7.28066403 -7.24315235 -7.29344766

#    -7.2920645  -7.26717433 -7.26959622 -7.29307748]

#   [-7.29320541 -7.27162341 -7.29245991 -7.2934821  -7.29360574 -7.27103573

#    -7.29311302 -7.29213505 -7.29356392 -7.29162927 -7.24981922 -7.29384423

#    -7.2930642  -7.27323089 -7.29156252 -7.293684  ]

#   [-7.29384132 -7.29130123 -7.29333191 -7.29442748 -7.29396942 -7.29011734

#    -7.2936549  -7.2929216  -7.29430937 -7.29288465 -7.27758063 -7.29446428

#    -7.2939575  -7.27958774 -7.29260011 -7.29408982]

#   [-7.29433271 -7.29273603 -7.29467579 -7.29507754 -7.29438517 -7.29298445

#    -7.2940647  -7.29297896 -7.29494014 -7.29348052 -7.29142477 -7.29524322

#    -7.29455267 -7.28904449 -7.29319333 -7.29437335]

#   [-7.29480095 -7.29294578 -7.29507043 -7.29530148 -7.29479745 -7.29390014

#    -7.29439439 -7.29304737 -7.29520739 -7.29397645 -7.29276852 -7.2955135

#    -7.29492853 -7.29145529 -7.29390729 -7.29462868]

#   [-7.29530498 -7.29299558 -7.29520643 -7.29537379 -7.29505127 -7.2945667

#    -7.29466477 -7.29312996 -7.29529573 -7.29447499 -7.29327567 -7.29560796

#    -7.29520605 -7.29179062 -7.29484071 -7.29489978]

#   DeviceArray([-7.29575181, -7.29302602, -7.29529976, -7.29541094,

#                -7.29517778, -7.29488194, -7.29487651, -7.29323608,

#                -7.29532772, -7.29494698, -7.29369784, -7.29567791,

#                -7.29534388, -7.29187906, -7.29536221, -7.29516005],            dtype=float64)

"""
### 更低层的 API

`TensorCircuit` 命名空间下的更高级别 API 提供了一个统一的框架来进行线性代数和自动微分，这与后端无关。
也可以使用 TensorFlow 或 Jax 直接提供的相关 API（ops、自动微分相关、可即时编译相关），只要坚持一个固定后端即可。 请参阅下面的 TensorFlow 后端示例。
"""

tc.set_backend("tensorflow")

def tfi_energy(c: tc.Circuit, j: float = 1.0, h: float = -1.0):
    e = 0.0
    n = c._nqubits
    for i in range(n):
        e += h * c.expectation((tc.gates.x(), [i]))  # <X_i>
    for i in range(n - 1):  # OBC
        e += j * c.expectation(
            (tc.gates.z(), [i]), (tc.gates.z(), [(i + 1) % n])
        )  # <Z_iZ_{i+1}>
    return tf.math.real(e)


def vqe_tfim(param, n, nlayers):
    c = tc.Circuit(n)
    paramc = tf.cast(param, tf.complex128)
    for i in range(n):
        c.H(i)
    for j in range(nlayers):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=zz, theta=paramc[2 * j, i])
        for i in range(n):
            c.rx(i, theta=paramc[2 * j + 1, i])
    e = tfi_energy(c)
    return e


@tf.function
def vqe_tfim_vag(param, n, nlayers):
    with tf.GradientTape() as tape:
        tape.watch(param)
        v = vqe_tfim(param, n, nlayers)
    grad = tape.gradient(v, param)
    return v, grad

train_step_tf(6, 3, 2000)
# Output:
#   tf.Tensor(-5.5454151788179376, shape=(), dtype=float64)

#   tf.Tensor(-7.167693061786028, shape=(), dtype=float64)

#   tf.Tensor(-7.254761404891117, shape=(), dtype=float64)

#   tf.Tensor(-7.290050014550046, shape=(), dtype=float64)

#   tf.Tensor(-7.29133881232428, shape=(), dtype=float64)

#   tf.Tensor(-7.2918048286324915, shape=(), dtype=float64)

#   tf.Tensor(-7.292590929769901, shape=(), dtype=float64)

#   tf.Tensor(-7.294195015132205, shape=(), dtype=float64)

#   tf.Tensor(-7.295013538531699, shape=(), dtype=float64)

#   tf.Tensor(-7.2951174084838835, shape=(), dtype=float64)

#   <tf.Tensor: shape=(), dtype=float64, numpy=-7.295170441938537>



================================================
FILE: docs/source/tutorials/tfim_vqe_diffreph.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# VQE on 1D TFIM with Different Hamiltonian Representation
"""

"""
## Overview
"""

"""
For the ground state preparation of a Hamiltonian $H$ in VQE, we need to calculate the expectation value of Hamiltonian $H$, i.e., $\langle 0^N \vert U^{\dagger}(\theta) H U(\theta) \vert 0^N \rangle$ and update the parameters $\theta$ in $U(\theta)$ based on gradient descent. In this tutorial, we will show four ways supported in ``TensorCircuit`` to calculate $\langle H \rangle$: 

1, $\langle H \rangle = \sum_{i} \langle h_{i} \rangle$, where $h_{i}$ are the Pauli-string operators;

2, $\langle H \rangle$ where $H$ is a sparse matrix;

3, $\langle H \rangle$ where $H$ is a dense matrix;

4, expectation value of the Matrix Product Operator (MPO).

We consider transverse field ising model (TFIM) here, which reads
$$
H = \sum_{i} \sigma_{i}^{x} \sigma_{i+1}^{x} - \sum_{i} \sigma_{i}^{z},
$$
where $\sigma_{i}^{x,z}$ are Pauli matrices of the $i$-th qubit.
"""

"""
## Setup
"""

import numpy as np
import tensorflow as tf
import tensorcircuit as tc
import tensornetwork as tn
from tensorcircuit.templates.measurements import operator_expectation
from tensorcircuit.quantum import quantum_constructor

tc.set_backend("tensorflow")
tc.set_dtype("complex128")
dtype = np.complex128

xx = tc.gates._xx_matrix  # xx gate matrix to be utilized

"""
## Parameters
"""

n = 4  # The number of qubits
nlayers = 2  # The number of circuit layers
ntrials = 2  # The number of random circuit instances

"""
## Parameterized Quantum Circuits
"""

def tfi_circuit(param, n, nlayers):
    c = tc.Circuit(n)
    for j in range(nlayers):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=xx, theta=param[2 * j, i])
        for i in range(n):
            c.rz(i, theta=param[2 * j + 1, i])
    return c

"""
##  Pauli-string Operators
"""

"""
### Energy
"""

def tfi_energy(c: tc.Circuit, j: float = 1.0, h: float = -1.0):
    e = 0.0
    n = c._nqubits
    for i in range(n):
        e += h * c.expectation((tc.gates.z(), [i]))  # <Z_i>
    for i in range(n - 1):  # OBC
        e += j * c.expectation(
            (tc.gates.x(), [i]), (tc.gates.x(), [(i + 1) % n])
        )  # <X_iX_{i+1}>
    return tc.backend.real(e)

def vqe_tfim_paulistring(param, n, nlayers):
    c = tfi_circuit(param, n, nlayers)
    e = tfi_energy(c)
    return e

vqe_tfim_paulistring_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim_paulistring)
)  # use vvag to get losses and gradients of different random circuit instances

"""
### Main Optimization Loop
"""

def batched_train_step_paulistring_tf(batch, n, nlayers, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[batch, nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )  # initial parameters
    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_paulistring_vvag(
            param.value(), n, nlayers
        )  # energy and gradients
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e


batched_train_step_paulistring_tf(ntrials, n, nlayers, 400)
# Output:
#   2022-03-16 14:09:12.304188: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA

#   To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

#   tf.Tensor([-4.00557571 -3.97372412], shape=(2,), dtype=float64)

#   tf.Tensor([-4.68208061 -4.684804  ], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.75683202, -4.73689914])>

"""
## Sparse Matrix, Dense Matrix, and MPO
"""

"""
### Hamiltonian
"""

def tfi_hamiltonian():
    h = []
    w = []

    ### Z
    for i in range(n):
        h.append([])
        w.append(-1.0)  # weight
        for j in range(n):
            if j == i:
                h[i].append(3)
            else:
                h[i].append(0)

    ### XX
    for i in range(n - 1):
        h.append([])
        w.append(1.0)  # weight
        for j in range(n):
            if j == (i + 1) % n or j == i:
                h[i + n].append(1)
            else:
                h[i + n].append(0)

    hamiltonian_sparse = tc.quantum.PauliStringSum2COO(
        tf.constant(h, dtype=tf.complex128), tf.constant(w, dtype=tf.complex128)
    )  # sparse matrix

    hamiltonian_dense = tc.quantum.PauliStringSum2Dense(
        tf.constant(h, dtype=tf.complex128), tf.constant(w, dtype=tf.complex128)
    )  # dense matrix
    return hamiltonian_sparse, hamiltonian_dense

"""
### Generate QuOperator
"""

def quoperator_mpo(tfi_mpo):
    tfi_mpo = tfi_mpo.tensors

    mpo = []
    for i in range(n):
        mpo.append(tn.Node(tfi_mpo[i]))

    for i in range(n - 1):
        tn.connect(mpo[i][1], mpo[i + 1][0])

    tfi_mpo = quantum_constructor(
        [mpo[i][-1] for i in range(n)],  # out_edges
        [mpo[i][-2] for i in range(n)],  # in_edges
        [],
        [mpo[0][0], mpo[-1][1]],  # ignore_edges
    )
    return tfi_mpo

"""
### Energy
"""

def vqe_tfim(param, n, nlayers, hamiltonian):
    c = tfi_circuit(param, n, nlayers)
    e = operator_expectation(
        c, hamiltonian
    )  # in operator_expectation, the "hamiltonian" can be sparse matrix, dense matrix or mpo
    return e

vqe_tfim_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim)
)  # use vvag to get losses and gradients of different random circuit instances

"""
### Main Optimization Loop
"""

def batched_train_step_tf(batch, n, nlayers, hamiltonian, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[batch, nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )  # initial parameters

    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_vvag(
            param.value(), n, nlayers, hamiltonian
        )  # energy and gradients
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e

"""
### Sparse Matrix, Dense Matrix, and MPO
"""

(
    hamiltonian_sparse,
    hamiltonian_dense,
) = tfi_hamiltonian()  # hamiltonian: sparse matrix, dense matrix

Jx = np.array([1.0 for _ in range(n - 1)])  # strength of xx interaction (OBC)
Bz = np.array([1.0 for _ in range(n)])  # strength of transverse field
hamiltonian_mpo = tn.matrixproductstates.mpo.FiniteTFI(
    Jx, Bz, dtype=dtype
)  # matrix product operator
hamiltonian_mpo = quoperator_mpo(hamiltonian_mpo)  # generate QuOperator from mpo
# Output:
#   2022-03-16 14:09:30.874680: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd94503abc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

#   2022-03-16 14:09:30.874726: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version

#   2022-03-16 14:09:31.014341: I tensorflow/compiler/jit/xla_compilation_cache.cc:351] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.


batched_train_step_tf(ntrials, n, nlayers, hamiltonian_sparse, 400)  # sparse matrix
# Output:
#   WARNING:tensorflow:Using a while_loop for converting SparseTensorDenseMatMul

#   tf.Tensor([-4.04418884 -3.22012342], shape=(2,), dtype=float64)

#   tf.Tensor([-4.67668625 -4.66761143], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.74512239, -4.69965641])>

batched_train_step_tf(ntrials, n, nlayers, hamiltonian_dense, 400)  # dense matrix
# Output:
#   tf.Tensor([-3.72705324 -3.99225849], shape=(2,), dtype=float64)

#   tf.Tensor([-4.70773521 -4.7330719 ], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.74236986, -4.7559722 ])>

batched_train_step_tf(ntrials, n, nlayers, hamiltonian_mpo, 400)  # mpo
# Output:
#   tf.Tensor([-3.9129593  -3.44283879], shape=(2,), dtype=float64)

#   tf.Tensor([-4.68271695 -4.67584305], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.75283209, -4.75535872])>



================================================
FILE: docs/source/tutorials/tfim_vqe_diffreph_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 具有不同哈密顿量表示的一维 TFIM 上的 VQE
"""

"""
## 概述
"""

"""
对于 VQE 中哈密顿量 $H$ 的基态准备，我们需要计算哈密顿量 $H$ 的期望值，即 $\langle 0^N \vert U^{\dagger}(\theta) HU(\ theta) \vert 0^N \rangle$ 并根据梯度下降更新 $U(\theta)$ 中的参数 $\theta$。 在本教程中，我们将展示 TensorCircuit 支持的四种计算 $\langle H \rangle$ 的方法：

1, $\langle H \rangle = \sum_{i} \langle h_{i} \rangle$，其中$h_{i}$是泡利串运算符；

2, $\langle H \rangle$ 其中 $H$ 是稀疏矩阵;

3, $\langle H \rangle$ 其中 $H$ 是密集矩阵;

4, 矩阵乘积算子 (MPO) 的期望值。

我们在这里考虑横向场 Ising 模型，也即：（TFIM），它读取

$$H = \sum_{i} \sigma_{i}^{x} \sigma_{i+1}^{x} - \sum_{i} \sigma_{i}^{z},$$

其中 $\sigma_{i}^{x,z}$ 是第 $i$ 个量子比特的泡利矩阵。
"""

"""
## 设置
"""

import numpy as np
import tensorflow as tf
import tensorcircuit as tc
import tensornetwork as tn
from tensorcircuit.templates.measurements import operator_expectation
from tensorcircuit.quantum import quantum_constructor

tc.set_backend("tensorflow")
tc.set_dtype("complex128")
dtype = np.complex128

xx = tc.gates._xx_matrix  # 要使用的 xx 门矩阵

"""
## 参数
"""

n = 4  # 量子比特数
nlayers = 2  # 电路层数
ntrials = 2  # 随机电路实例数

"""
## 参数化量子电路
"""

def tfi_circuit(param, n, nlayers):
    c = tc.Circuit(n)
    for j in range(nlayers):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=xx, theta=param[2 * j, i])
        for i in range(n):
            c.rz(i, theta=param[2 * j + 1, i])
    return c

"""
##  泡利字符串运算子
"""

"""
### 能量
"""

def tfi_energy(c: tc.Circuit, j: float = 1.0, h: float = -1.0):
    e = 0.0
    n = c._nqubits
    for i in range(n):
        e += h * c.expectation((tc.gates.z(), [i]))  # <Z_i>
    for i in range(n - 1):  # OBC
        e += j * c.expectation(
            (tc.gates.x(), [i]), (tc.gates.x(), [(i + 1) % n])
        )  # <X_iX_{i+1}>
    return tc.backend.real(e)

def vqe_tfim_paulistring(param, n, nlayers):
    c = tfi_circuit(param, n, nlayers)
    e = tfi_energy(c)
    return e

vqe_tfim_paulistring_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim_paulistring)
)  # 使用 vvag 获取不同随机电路实例的损失函数和梯度

"""
### 主优化循环
"""

def batched_train_step_paulistring_tf(batch, n, nlayers, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[batch, nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )  # 初始参数
    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_paulistring_vvag(param.value(), n, nlayers)  # 能量和梯度
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e


batched_train_step_paulistring_tf(ntrials, n, nlayers, 400)
# Output:
#   2022-03-16 14:09:12.304188: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA

#   To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

#   tf.Tensor([-4.00557571 -3.97372412], shape=(2,), dtype=float64)

#   tf.Tensor([-4.68208061 -4.684804  ], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.75683202, -4.73689914])>

"""
## 稀疏矩阵、稠密矩阵和 MPO
"""

"""
### 哈密顿量
"""

def tfi_hamiltonian():
    h = []
    w = []

    ### Z
    for i in range(n):
        h.append([])
        w.append(-1.0)  # weight
        for j in range(n):
            if j == i:
                h[i].append(3)
            else:
                h[i].append(0)

    ### XX
    for i in range(n - 1):
        h.append([])
        w.append(1.0)  # weight
        for j in range(n):
            if j == (i + 1) % n or j == i:
                h[i + n].append(1)
            else:
                h[i + n].append(0)

    hamiltonian_sparse = tc.quantum.PauliStringSum2COO(
        tf.constant(h, dtype=tf.complex128), tf.constant(w, dtype=tf.complex128)
    )  # 稀疏矩阵
    hamiltonian_dense = tc.quantum.PauliStringSum2Dense(
        tf.constant(h, dtype=tf.complex128), tf.constant(w, dtype=tf.complex128)
    )  # 密集矩阵
    return hamiltonian_sparse, hamiltonian_dense

"""
### 生成 QuOperator
"""

def quoperator_mpo(tfi_mpo):
    tfi_mpo = tfi_mpo.tensors

    mpo = []
    for i in range(n):
        mpo.append(tn.Node(tfi_mpo[i]))

    for i in range(n - 1):
        tn.connect(mpo[i][1], mpo[i + 1][0])

    tfi_mpo = quantum_constructor(
        [mpo[i][-1] for i in range(n)],  # out_edges
        [mpo[i][-2] for i in range(n)],  # in_edges
        [],
        [mpo[0][0], mpo[-1][1]],  # ignore_edges
    )
    return tfi_mpo

"""
### 能量
"""

def vqe_tfim(param, n, nlayers, hamiltonian):
    c = tfi_circuit(param, n, nlayers)
    e = operator_expectation(
        c, hamiltonian
    )  # 在 operator_expectation 中，“hamiltonian” 可以是稀疏矩阵、密集矩阵或 mpo
    return e

vqe_tfim_vvag = tc.backend.jit(
    tc.backend.vectorized_value_and_grad(vqe_tfim)
)  # use vvag to get losses and gradients of different random circuit instances

"""
### 主优化循环
"""

def batched_train_step_tf(batch, n, nlayers, hamiltonian, maxiter=10000):
    param = tf.Variable(
        initial_value=tf.random.normal(
            shape=[batch, nlayers * 2, n], stddev=0.1, dtype=getattr(tf, tc.rdtypestr)
        )
    )  # 初始参数

    opt = tf.keras.optimizers.Adam(1e-2)
    for i in range(maxiter):
        e, grad = vqe_tfim_vvag(param.value(), n, nlayers, hamiltonian)  # 能量和梯度
        opt.apply_gradients([(grad, param)])
        if i % 200 == 0:
            print(e)
    return e

"""
### 稀疏矩阵、密集矩阵和 MPO
"""

(
    hamiltonian_sparse,
    hamiltonian_dense,
) = tfi_hamiltonian()  # hamiltonian：稀疏矩阵，稠密矩阵

Jx = np.array([1.0 for _ in range(n - 1)])  # xx 相互作用的强度 (OBC)
Bz = np.array([1.0 for _ in range(n)])  # 横向场强
hamiltonian_mpo = tn.matrixproductstates.mpo.FiniteTFI(
    Jx, Bz, dtype=dtype
)  # 矩阵乘积算子
hamiltonian_mpo = quoperator_mpo(hamiltonian_mpo)  # 从 mpo 生成 QuOperator
# Output:
#   2022-03-16 14:09:30.874680: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd94503abc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

#   2022-03-16 14:09:30.874726: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version

#   2022-03-16 14:09:31.014341: I tensorflow/compiler/jit/xla_compilation_cache.cc:351] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.


batched_train_step_tf(ntrials, n, nlayers, hamiltonian_sparse, 400)  # 稀疏矩阵
# Output:
#   WARNING:tensorflow:Using a while_loop for converting SparseTensorDenseMatMul

#   tf.Tensor([-4.04418884 -3.22012342], shape=(2,), dtype=float64)

#   tf.Tensor([-4.67668625 -4.66761143], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.74512239, -4.69965641])>

batched_train_step_tf(ntrials, n, nlayers, hamiltonian_dense, 400)  # 密集矩阵
# Output:
#   tf.Tensor([-3.72705324 -3.99225849], shape=(2,), dtype=float64)

#   tf.Tensor([-4.70773521 -4.7330719 ], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.74236986, -4.7559722 ])>

batched_train_step_tf(ntrials, n, nlayers, hamiltonian_mpo, 400)  # mpo
# Output:
#   tf.Tensor([-3.9129593  -3.44283879], shape=(2,), dtype=float64)

#   tf.Tensor([-4.68271695 -4.67584305], shape=(2,), dtype=float64)

#   <tf.Tensor: shape=(2,), dtype=float64, numpy=array([-4.75283209, -4.75535872])>



================================================
FILE: docs/source/tutorials/torch_qml.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# QML in PyTorch
"""

"""
## Overview

In this tutorial, we show the MNIST binary classification QML example with the same setup as [mnist_qml](mnist_qml.ipynb). This time, we use the PyTorch machine learning pipeline to build the QML model.
Again, this note is not about the best QML practice or the best PyTorch pipeline practice, instead, it is just a demonstration of the integration between PyTorch and TensorCircuit.
"""

"""
## Setup
"""

import time
import numpy as np
import tensorflow as tf
import torch

import tensorcircuit as tc

K = tc.set_backend("tensorflow")

# Use TensorFlow as the backend, while wrapping the quantum function in the PyTorch interface

# We load and preprocess the dataset as the previous notebook using tensorflow and jax backend

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train[..., np.newaxis] / 255.0


def filter_pair(x, y, a, b):
    keep = (y == a) | (y == b)
    x, y = x[keep], y[keep]
    y = y == a
    return x, y


x_train, y_train = filter_pair(x_train, y_train, 1, 5)
x_train_small = tf.image.resize(x_train, (3, 3)).numpy()
x_train_bin = np.array(x_train_small > 0.5, dtype=np.float32)
x_train_bin = np.squeeze(x_train_bin).reshape([-1, 9])
y_train_torch = torch.tensor(y_train, dtype=torch.float32)
x_train_torch = torch.tensor(x_train_bin)
x_train_torch.shape, y_train_torch.shape
# Output:
#   (torch.Size([12163, 9]), torch.Size([12163]))

"""
## Wrap Quantum Function Using ``torch_interface``
"""

n = 9
nlayers = 3

# We define the quantum function,
# note how this function is running on tensorflow


def qpred(x, weights):
    c = tc.Circuit(n)
    for i in range(n):
        c.rx(i, theta=x[i])
    for j in range(nlayers):
        for i in range(n - 1):
            c.cnot(i, i + 1)
        for i in range(n):
            c.rx(i, theta=weights[2 * j, i])
            c.ry(i, theta=weights[2 * j + 1, i])
    ypred = c.expectation_ps(z=[n // 2])
    ypred = K.real(ypred)
    return K.sigmoid(ypred)


# Wrap the function into pytorch form but with tensorflow speed!
qpred_torch = tc.interfaces.torch_interface(qpred, jit=True)

"""
After we have the AD aware function in PyTorch format, we can further wrap it as a torch Module (network layer).
"""

class QuantumNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.q_weights = torch.nn.Parameter(torch.randn([2 * nlayers, n]))

    def forward(self, inputs):
        ypred = qpred_torch(inputs, self.q_weights)
        return ypred

net = QuantumNet()
net(x_train_torch[0])
# Output:
#   tensor(0.4539, grad_fn=<FunBackward>)

criterion = torch.nn.BCELoss()
opt = torch.optim.Adam(net.parameters(), lr=1e-2)
nepochs = 500
nbatch = 32
times = []

for epoch in range(nepochs):
    index = np.random.randint(low=0, high=100, size=nbatch)
    # index = np.arange(nbatch)
    inputs, labels = x_train_torch[index], y_train_torch[index]
    opt.zero_grad()

    with torch.set_grad_enabled(True):
        time0 = time.time()
        yps = []
        for i in range(nbatch):
            yp = net(inputs[i])
            yps.append(yp)
        yps = torch.stack(yps)
        loss = criterion(
            torch.reshape(yps, [nbatch, 1]), torch.reshape(labels, [nbatch, 1])
        )
        loss.backward()
        if epoch % 100 == 0:
            print(loss)
        opt.step()
        time1 = time.time()
        times.append(time1 - time0)

print("training time per step: ", np.mean(time1 - time0))
# Output:
#   tensor(0.7287, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5947, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5804, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6358, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6503, grad_fn=<BinaryCrossEntropyBackward0>)

#   training time per step:  0.12587213516235352


"""
## Batched Version

Now let's try vectorized version to speed up the batch input processing. Note how intrisically, ``tf.vectorized_map`` helps in the batch pipeline.
"""

qpred_vmap = K.vmap(qpred, vectorized_argnums=0)

# `qpred_vmap` is a tensorflow function with vectorization capacity

qpred_batch = tc.interfaces.torch_interface(qpred_vmap, jit=True)

# We further wrap the function as a PyTorch one

# Test the AD capacity of the PyTorch function

w = torch.ones([2 * nlayers, n])
w.requires_grad_()
with torch.set_grad_enabled(True):
    yps = qpred_batch(x_train_torch[:3], w)
    loss = torch.sum(yps)
    loss.backward()
print(w.grad)
# Output:
#   tensor([[-6.2068e-03, -3.0100e-05, -1.0997e-02, -1.8381e-02, -9.1800e-02,

#             1.2481e-01, -6.5200e-02,  1.1176e-08,  7.4506e-09],

#           [-3.2353e-03,  3.4989e-03, -1.1344e-02, -1.6136e-02,  1.9075e-02,

#             2.1119e-02,  2.6881e-02, -1.1176e-08,  0.0000e+00],

#           [-1.1777e-02, -1.1572e-03, -5.0570e-03,  6.4838e-03, -5.5077e-02,

#            -3.4250e-02, -7.4506e-09, -1.1176e-08,  3.7253e-09],

#           [-1.4748e-02, -2.3818e-02, -4.3567e-02, -4.7879e-02,  1.2331e-01,

#             1.4314e-01,  3.7253e-09,  1.1176e-08,  3.7253e-09],

#           [-3.7253e-09,  3.7253e-09,  0.0000e+00,  0.0000e+00, -2.7574e-02,

#             7.4506e-09,  7.4506e-09, -1.1176e-08,  0.0000e+00],

#           [ 3.7253e-09,  3.7253e-09,  1.4901e-08, -7.4506e-09,  7.1655e-02,

#            -7.4506e-09,  3.7253e-09,  1.4901e-08,  0.0000e+00]])


class QuantumNetV2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.q_weights = torch.nn.Parameter(torch.randn([2 * nlayers, n]))

    def forward(self, inputs):
        ypred = qpred_batch(inputs, self.q_weights)
        return ypred

net2 = QuantumNetV2()
net2(x_train_torch[:3])
# Output:
#   tensor([0.4706, 0.4706, 0.4767], grad_fn=<FunBackward>)

"""
With the help of vmap infrastructure borrowed from TensorFlow, the performance of training is greatly improved!
"""

criterion = torch.nn.BCELoss()
opt = torch.optim.Adam(net2.parameters(), lr=1e-2)
nepochs = 500
nbatch = 32
times = []
for epoch in range(nepochs):
    index = np.random.randint(low=0, high=100, size=nbatch)
    # index = np.arange(nbatch)
    inputs, labels = x_train_torch[index], y_train_torch[index]
    opt.zero_grad()

    with torch.set_grad_enabled(True):
        time0 = time.time()
        yps = net2(inputs)
        loss = criterion(
            torch.reshape(yps, [nbatch, 1]), torch.reshape(labels, [nbatch, 1])
        )
        loss.backward()
        if epoch % 100 == 0:
            print(loss)
        opt.step()
        time1 = time.time()
        times.append(time1 - time0)
print("training time per step: ", np.mean(times[1:]))
# Output:
#   tensor(0.6973, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6421, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6419, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6498, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6466, grad_fn=<BinaryCrossEntropyBackward0>)

#   training time per step:  0.009107916531916371


"""
## Hybrid Model with Classical Post-processing

We now build a quantum-classical hybrid machine learning model pipeline where the output measurement results are further fed into a classical fully connected layer.
"""

def qpreds(x, weights):
    c = tc.Circuit(n)
    for i in range(n):
        c.rx(i, theta=x[i])
    for j in range(nlayers):
        for i in range(n - 1):
            c.cnot(i, i + 1)
        for i in range(n):
            c.rx(i, theta=weights[2 * j, i])
            c.ry(i, theta=weights[2 * j + 1, i])

    return K.stack([K.real(c.expectation_ps(z=[i])) for i in range(n)])


qpreds_vmap = K.vmap(qpreds, vectorized_argnums=0)
qpreds_batch = tc.interfaces.torch_interface(qpreds_vmap, jit=True)

qpreds_batch(x_train_torch[:2], torch.ones([2 * nlayers, n]))
# Output:
#   tensor([[ 0.2839,  0.3786,  0.0158,  0.1512,  0.1395,  0.1364,  0.1403,  0.1423,

#            -0.1285],

#           [ 0.2839,  0.3786,  0.0158,  0.1512,  0.1395,  0.1364,  0.1403,  0.1423,

#            -0.1285]])

class QuantumNetV3(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.q_weights = torch.nn.Parameter(torch.randn([2 * nlayers, n]))

    def forward(self, inputs):
        ypred = qpreds_batch(inputs, self.q_weights)
        return ypred

net3 = QuantumNetV3()
net3(x_train_bin[:2])
# Output:
#   tensor([[ 0.2931,  0.5393, -0.0369, -0.0450,  0.0511, -0.0121,  0.0156, -0.0406,

#            -0.1330],

#           [ 0.2931,  0.5393, -0.0369, -0.0450,  0.0511, -0.0121,  0.0156, -0.0406,

#            -0.1330]], grad_fn=<FunBackward>)

"""
We now build a hybrid model with the quantum layer ``net3`` and append a Linear layer behind it.
"""

model = torch.nn.Sequential(QuantumNetV3(), torch.nn.Linear(9, 1), torch.nn.Sigmoid())

model(x_train_bin[:2])
# Output:
#   tensor([[0.5500],

#           [0.5500]], grad_fn=<SigmoidBackward0>)

criterion = torch.nn.BCELoss()
opt = torch.optim.Adam(model.parameters(), lr=1e-2)
nepochs = 500
nbatch = 32
times = []
for epoch in range(nepochs):
    index = np.random.randint(low=0, high=100, size=nbatch)
    # index = np.arange(nbatch)
    inputs, labels = x_train_torch[index], y_train_torch[index]
    opt.zero_grad()

    with torch.set_grad_enabled(True):
        time0 = time.time()
        yps = model(inputs)
        loss = criterion(
            torch.reshape(yps, [nbatch, 1]), torch.reshape(labels, [nbatch, 1])
        )
        loss.backward()
        if epoch % 100 == 0:
            print(loss)
        opt.step()
        time1 = time.time()
        times.append(time1 - time0)
print("training time per step: ", np.mean(times[1:]))
# Output:
#   tensor(0.6460, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6086, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5199, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5697, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5248, grad_fn=<BinaryCrossEntropyBackward0>)

#   training time per step:  0.020270218113381304




================================================
FILE: docs/source/tutorials/torch_qml_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 使用 PyTorch 进行量子机器学习
"""

"""
## 概述

在本教程中，我们展示了具有与 [mnist_qml](mnist_qml.ipynb) 相同设置的 MNIST 二元分类 QML 示例。
这一次，我们使用 PyTorch 机器学习管道来构建 QML 模型。
同样，本教程不是关于最佳 QML 实践或最佳 PyTorch 管道实践，而是关于 PyTorch 和 TensorCircuit 之间集成的演示。
"""

"""
## 设置
"""

import time
import numpy as np
import tensorflow as tf
import torch

import tensorcircuit as tc

K = tc.set_backend("tensorflow")

# 使用 TensorFlow 作为后端，同时将量子函数封装在 PyTorch 接口中

# 我们使用和之前 notebook 相同的数据集与预处理

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train[..., np.newaxis] / 255.0


def filter_pair(x, y, a, b):
    keep = (y == a) | (y == b)
    x, y = x[keep], y[keep]
    y = y == a
    return x, y


x_train, y_train = filter_pair(x_train, y_train, 1, 5)
x_train_small = tf.image.resize(x_train, (3, 3)).numpy()
x_train_bin = np.array(x_train_small > 0.5, dtype=np.float32)
x_train_bin = np.squeeze(x_train_bin).reshape([-1, 9])
y_train_torch = torch.tensor(y_train, dtype=torch.float32)
x_train_torch = torch.tensor(x_train_bin)
x_train_torch.shape, y_train_torch.shape
# Output:
#   (torch.Size([12163, 9]), torch.Size([12163]))

"""
## 使用 ``torch_interface`` 包装量子函数
"""

n = 9
nlayers = 3

# 我们定义量子函数，
# 注意这个函数是在 tensorflow 上运行的


def qpred(x, weights):
    c = tc.Circuit(n)
    for i in range(n):
        c.rx(i, theta=x[i])
    for j in range(nlayers):
        for i in range(n - 1):
            c.cnot(i, i + 1)
        for i in range(n):
            c.rx(i, theta=weights[2 * j, i])
            c.ry(i, theta=weights[2 * j + 1, i])
    ypred = c.expectation_ps(z=[n // 2])
    ypred = K.real(ypred)
    return K.sigmoid(ypred)


# 将函数包装成 pytorch 形式，但具有 tensorflow 速度！
qpred_torch = tc.interfaces.torch_interface(qpred, jit=True)

"""
在我们拥有 PyTorch 格式的可微功能之后，我们可以进一步将其包装为一个 Torch 模块（网络层）。
"""

class QuantumNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.q_weights = torch.nn.Parameter(torch.randn([2 * nlayers, n]))

    def forward(self, inputs):
        ypred = qpred_torch(inputs, self.q_weights)
        return ypred

net = QuantumNet()
net(x_train_torch[0])
# Output:
#   tensor(0.4539, grad_fn=<FunBackward>)

criterion = torch.nn.BCELoss()
opt = torch.optim.Adam(net.parameters(), lr=1e-2)
nepochs = 500
nbatch = 32
times = []

for epoch in range(nepochs):
    index = np.random.randint(low=0, high=100, size=nbatch)
    # index = np.arange(nbatch)
    inputs, labels = x_train_torch[index], y_train_torch[index]
    opt.zero_grad()

    with torch.set_grad_enabled(True):
        time0 = time.time()
        yps = []
        for i in range(nbatch):
            yp = net(inputs[i])
            yps.append(yp)
        yps = torch.stack(yps)
        loss = criterion(
            torch.reshape(yps, [nbatch, 1]), torch.reshape(labels, [nbatch, 1])
        )
        loss.backward()
        if epoch % 100 == 0:
            print(loss)
        opt.step()
        time1 = time.time()
        times.append(time1 - time0)

print("training time per step: ", np.mean(time1 - time0))
# Output:
#   tensor(0.7287, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5947, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5804, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6358, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6503, grad_fn=<BinaryCrossEntropyBackward0>)

#   training time per step:  0.12587213516235352


"""
## 批处理版本

现在让我们尝试使用向量化版本来加快批量输入处理。请注意，本质上是 ``tf.vectorized_map`` 帮助批处理管道的。
"""

qpred_vmap = K.vmap(qpred, vectorized_argnums=0)

# `qpred_vmap` 是一个具有向量化能力的 TensorFlow 函数

qpred_batch = tc.interfaces.torch_interface(qpred_vmap, jit=True)

# 我们进一步将函数包装为 PyTorch 函数

# 测试 PyTorch 函数的 AD 支持

w = torch.ones([2 * nlayers, n])
w.requires_grad_()
with torch.set_grad_enabled(True):
    yps = qpred_batch(x_train_torch[:3], w)
    loss = torch.sum(yps)
    loss.backward()
print(w.grad)
# Output:
#   tensor([[-6.2068e-03, -3.0100e-05, -1.0997e-02, -1.8381e-02, -9.1800e-02,

#             1.2481e-01, -6.5200e-02,  1.1176e-08,  7.4506e-09],

#           [-3.2353e-03,  3.4989e-03, -1.1344e-02, -1.6136e-02,  1.9075e-02,

#             2.1119e-02,  2.6881e-02, -1.1176e-08,  0.0000e+00],

#           [-1.1777e-02, -1.1572e-03, -5.0570e-03,  6.4838e-03, -5.5077e-02,

#            -3.4250e-02, -7.4506e-09, -1.1176e-08,  3.7253e-09],

#           [-1.4748e-02, -2.3818e-02, -4.3567e-02, -4.7879e-02,  1.2331e-01,

#             1.4314e-01,  3.7253e-09,  1.1176e-08,  3.7253e-09],

#           [-3.7253e-09,  3.7253e-09,  0.0000e+00,  0.0000e+00, -2.7574e-02,

#             7.4506e-09,  7.4506e-09, -1.1176e-08,  0.0000e+00],

#           [ 3.7253e-09,  3.7253e-09,  1.4901e-08, -7.4506e-09,  7.1655e-02,

#            -7.4506e-09,  3.7253e-09,  1.4901e-08,  0.0000e+00]])


class QuantumNetV2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.q_weights = torch.nn.Parameter(torch.randn([2 * nlayers, n]))

    def forward(self, inputs):
        ypred = qpred_batch(inputs, self.q_weights)
        return ypred

net2 = QuantumNetV2()
net2(x_train_torch[:3])
# Output:
#   tensor([0.4706, 0.4706, 0.4767], grad_fn=<FunBackward>)

"""
借助借用自 TensorFlow 的 vmap 基础设施，训练的性能大大提升！
"""

criterion = torch.nn.BCELoss()
opt = torch.optim.Adam(net2.parameters(), lr=1e-2)
nepochs = 500
nbatch = 32
times = []
for epoch in range(nepochs):
    index = np.random.randint(low=0, high=100, size=nbatch)
    # index = np.arange(nbatch)
    inputs, labels = x_train_torch[index], y_train_torch[index]
    opt.zero_grad()

    with torch.set_grad_enabled(True):
        time0 = time.time()
        yps = net2(inputs)
        loss = criterion(
            torch.reshape(yps, [nbatch, 1]), torch.reshape(labels, [nbatch, 1])
        )
        loss.backward()
        if epoch % 100 == 0:
            print(loss)
        opt.step()
        time1 = time.time()
        times.append(time1 - time0)
print("training time per step: ", np.mean(times[1:]))
# Output:
#   tensor(0.6973, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6421, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6419, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6498, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6466, grad_fn=<BinaryCrossEntropyBackward0>)

#   training time per step:  0.009107916531916371


"""
## 具有经典后处理的混合模型

我们现在构建了一个量子经典混合机器学习模型管道，其中输出测量结果被进一步馈送到经典的全连接层。
"""

def qpreds(x, weights):
    c = tc.Circuit(n)
    for i in range(n):
        c.rx(i, theta=x[i])
    for j in range(nlayers):
        for i in range(n - 1):
            c.cnot(i, i + 1)
        for i in range(n):
            c.rx(i, theta=weights[2 * j, i])
            c.ry(i, theta=weights[2 * j + 1, i])

    return K.stack([K.real(c.expectation_ps(z=[i])) for i in range(n)])


qpreds_vmap = K.vmap(qpreds, vectorized_argnums=0)
qpreds_batch = tc.interfaces.torch_interface(qpreds_vmap, jit=True)

qpreds_batch(x_train_torch[:2], torch.ones([2 * nlayers, n]))
# Output:
#   tensor([[ 0.2839,  0.3786,  0.0158,  0.1512,  0.1395,  0.1364,  0.1403,  0.1423,

#            -0.1285],

#           [ 0.2839,  0.3786,  0.0158,  0.1512,  0.1395,  0.1364,  0.1403,  0.1423,

#            -0.1285]])

class QuantumNetV3(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.q_weights = torch.nn.Parameter(torch.randn([2 * nlayers, n]))

    def forward(self, inputs):
        ypred = qpreds_batch(inputs, self.q_weights)
        return ypred

net3 = QuantumNetV3()
net3(x_train_bin[:2])
# Output:
#   tensor([[ 0.2931,  0.5393, -0.0369, -0.0450,  0.0511, -0.0121,  0.0156, -0.0406,

#            -0.1330],

#           [ 0.2931,  0.5393, -0.0369, -0.0450,  0.0511, -0.0121,  0.0156, -0.0406,

#            -0.1330]], grad_fn=<FunBackward>)

"""
我们现在用量子层 ``net3``构建一个混合模型，并在后面附加一个线性层。
"""

model = torch.nn.Sequential(QuantumNetV3(), torch.nn.Linear(9, 1), torch.nn.Sigmoid())

model(x_train_bin[:2])
# Output:
#   tensor([[0.5500],

#           [0.5500]], grad_fn=<SigmoidBackward0>)

criterion = torch.nn.BCELoss()
opt = torch.optim.Adam(model.parameters(), lr=1e-2)
nepochs = 500
nbatch = 32
times = []
for epoch in range(nepochs):
    index = np.random.randint(low=0, high=100, size=nbatch)
    # index = np.arange(nbatch)
    inputs, labels = x_train_torch[index], y_train_torch[index]
    opt.zero_grad()

    with torch.set_grad_enabled(True):
        time0 = time.time()
        yps = model(inputs)
        loss = criterion(
            torch.reshape(yps, [nbatch, 1]), torch.reshape(labels, [nbatch, 1])
        )
        loss.backward()
        if epoch % 100 == 0:
            print(loss)
        opt.step()
        time1 = time.time()
        times.append(time1 - time0)
print("training time per step: ", np.mean(times[1:]))
# Output:
#   tensor(0.6460, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.6086, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5199, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5697, grad_fn=<BinaryCrossEntropyBackward0>)

#   tensor(0.5248, grad_fn=<BinaryCrossEntropyBackward0>)

#   training time per step:  0.020270218113381304




================================================
FILE: docs/source/tutorials/vqe_h2o.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Variational Quantum Eigensolver (VQE) on Molecules
"""

"""
## Overview

VQE is a variational algorithm for calculating the ground state of some given hamiltonian H which we call it $\psi_g$ that satisfied $H \left|\psi_g\right> =E_g\left|\psi_g\right>$. For an arbitrary normalized wavefunction $\psi_f$, the expectation value $\left<\psi_f|H|\psi_f \right>$ is always not lower than the ground state energy unless $\psi_f = \psi_g$ to some phase factor (here we assume there is no degeneracy in ground state). Based on that fact, if we use a parameterized wavefunction $\psi_\theta$, e.g. given by a parameterized quantum circuit (PQC) with parameters $\theta$, we can give an approximation for the ground state enery and wavefunction by minimizing the expectation value of $H$. In practical quantum hardware, this algorithm can be realized in a quantum-neural hybrid paradigm with the gradient calculated using finite difference or paremeter shift in quantum hardware and the optimization using gradient descent method in classical computer. While in a numerical simulation, we can just calculate the gradients using automatic differentiation. 

Calculating the ground state energy for molecules is often important for quantum chemistry tasks since it can be used to find out the atom structure of the molecules. In the simulation of molecules, we do not consider the motion of nuclei which means we fix the nuclear coordinates of its constituent atoms. We only consider the electrons in the molecules since the nuclei are way heavier than the electrons and thus the energy carried by phonons is negligible or can be reconsidered using Born-Oppenheimer approximation. Strictly speaking, the eletrons lives in continuous space, thus the Hilbert space is of infinite dimensions. To conduct a practical calculation, we only preserve some important single-particle basis, e.g. the low energy atomic orbitals. In the second quantization formalism, we can represent these atomic orbitals as $c_i^\dagger|0>$. By considering the interactions of nuclei and electrons as background and also the electron-electron interaction, a molecules hamiltonian can in generally be represented as $H = \sum_{i, j} h_{i,j} c_i^\dagger c_j + \sum_{i, j, k, l} \alpha_{i, j, k, l} c_i^\dagger c_j^\dagger c_k c_l$. Notice that the spin index is also absorbed into the orbital index. There are many softwares that can give these parameters in H such as `pyscf` which we will use later in this tutorial. Now we have a fermionic description for moleculars. By using a mapping from fermionic operators to spin operators such as Jordan-Wigner transformation or Bravyi-Kitaev transformation, we can map the fermionic hamiltonian to a spin hamiltonian which is more compatible with quantum computer. For a spin hamiltonian, we can easily use a PQC to construct a trail wavefunction and conduct the VQE algorithm. In the following part of this tutorial, we will demonstrate a complete example of how to use TensorCircuit to simulate VQE algorithm on Molecules.
"""

"""
## Setup

We should first ``pip install openfermion openfermionpyscf`` to generate fermionic and qubit Hamiltonian of H2O molecule based on quantum chemistry computation provided by openfermion and pyscf.
"""

import numpy as np
from openfermion.chem import MolecularData
from openfermion.transforms import (
    get_fermion_operator,
    jordan_wigner,
    binary_code_transform,
    checksum_code,
    reorder,
)
from openfermion.chem import geometry_from_pubchem
from openfermion.utils import up_then_down
from openfermion.linalg import LinearQubitOperator
from openfermionpyscf import run_pyscf
import tensorflow as tf

import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## Generate Hamiltonian
"""

"""
* Get molecule energy info and molecule orbitals
"""

multiplicity = 1
basis = "sto-3g"
# 14 spin orbitals for H2O
geometry = geometry_from_pubchem("h2o")
description = "h2o"
molecule = MolecularData(geometry, basis, multiplicity, description=description)
molecule = run_pyscf(molecule, run_mp2=True, run_cisd=True, run_ccsd=True, run_fci=True)
print(molecule.fci_energy, molecule.ccsd_energy, molecule.hf_energy)
# Output:
#   -75.0155301894916 -75.01540899923558 -74.96444758276998


"""
* Get Fermionic Hamiltonian
"""

mh = molecule.get_molecular_hamiltonian()

fh = get_fermion_operator(mh)

print(fh.terms[((0, 1), (0, 0))])  # coefficient of C0^\dagger C_0
# Output:
#   -32.68991541360029


"""
* Transform into qubit Hamiltonian
"""

# The normal transformation such as JW or BK requires 14 qubits for H2O's 14 orbitals

a = jordan_wigner(fh)
LinearQubitOperator(a).n_qubits
# Output:
#   14

"""
We can use binary code to save two further qubits, as the number of spin up and spin down filling is both 5 (5/odd electrons in 7 orbitals)
"""

b = binary_code_transform(reorder(fh, up_then_down), 2 * checksum_code(7, 1))
# 7 is 7 spin polarized orbitals, and 1 is for odd occupation
LinearQubitOperator(b).n_qubits
# Output:
#   12

print(b.terms[((0, "Z"),)])  # coefficient of Z_0 Pauli-string
# Output:
#   12.412562749393349


"""
* Transform the qubit Hamiltonian in openfermion to the format in TensorCircuit
"""

lsb, wb = tc.templates.chems.get_ps(b, 12)
lsa, wa = tc.templates.chems.get_ps(a, 14)

"""
* Inspect Hamiltonian in matrix form
"""

ma = tc.quantum.PauliStringSum2COO_numpy(lsa, wa)

mb = tc.quantum.PauliStringSum2COO_numpy(lsb, wb)

mad, mbd = ma.todense(), mb.todense()

"""
The corresponding Hartree Fock product state in these two types of Hamiltonian
"""

bin(np.argmin(np.diag(mad)))
# Output:
#   '0b11111111110000'

bin(np.argmin(np.diag(mbd)))
# Output:
#   '0b111110111110'

"""
## VQE Setup

We can in principle evaluate each Pauli string of the Hamiltonian as an expectation measurement, but it costs lots of simulation time, instead we fuse them as a Hamiltonian matrix as shown above to run the VQE.
"""

"""
* Using dense matrix expectation
"""

n = 12
depth = 4
mbd_tf = tc.array_to_tensor(mbd)


def vqe(param):
    c = tc.Circuit(n)
    for i in [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]:
        c.X(i)
    for j in range(depth):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=tc.gates._xx_matrix, theta=param[j, i, 0])
        for i in range(n):
            c.rx(i, theta=param[j, i, 1])
        for i in range(n):
            c.ry(i, theta=param[j, i, 2])
        for i in range(n):
            c.rx(i, theta=param[j, i, 3])
    return tc.templates.measurements.operator_expectation(c, mbd_tf)

vags = tc.backend.jit(tc.backend.value_and_grad(vqe))
lr = tf.keras.optimizers.schedules.ExponentialDecay(
    decay_rate=0.5, decay_steps=300, initial_learning_rate=0.5e-2
)
opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))

param = tc.backend.implicit_randn(shape=[depth, n, 4], stddev=0.02, dtype="float32")
for i in range(600):
    e, g = vags(param)
    param = opt.update(g, param)
    if i % 100 == 0:
        print(e)
# Output:
#   tf.Tensor(-74.76671, shape=(), dtype=float32)

#   tf.Tensor(-74.95493, shape=(), dtype=float32)

#   tf.Tensor(-74.95319, shape=(), dtype=float32)

#   tf.Tensor(-74.954315, shape=(), dtype=float32)

#   tf.Tensor(-74.956116, shape=(), dtype=float32)

#   tf.Tensor(-74.95809, shape=(), dtype=float32)


"""
* Using sparse matrix expectation

We can also use the sparse Hamiltonian matrix for circuit expectation evaluation, the only difference is to replace ``mbd_tf`` with ``mb_tf``
"""

mb_tf = tc.backend.coo_sparse_matrix(
    np.transpose(np.stack([mb.row, mb.col])), mb.data, shape=(2**n, 2**n)
)

"""
A micro-benchmark between sparse matrix evaluation and dense matrix evaluation for expectation in terms of time, sparse always wins in terms of space, of course.
"""

def dense_expt(param):
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
        c.rx(i, theta=param[i])
    return tc.templates.measurements.operator_expectation(c, mbd_tf)


def sparse_expt(param):
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
        c.rx(i, theta=param[i])
    return tc.templates.measurements.operator_expectation(c, mb_tf)

dense_vag = tc.backend.jit(tc.backend.value_and_grad(dense_expt))
sparse_vag = tc.backend.jit(tc.backend.value_and_grad(sparse_expt))

v0, g0 = dense_vag(tc.backend.ones([n]))
v1, g1 = sparse_vag(tc.backend.ones([n]))

# consistency check

np.testing.assert_allclose(v0, v1, atol=1e-5)
np.testing.assert_allclose(g0, g1, atol=1e-5)

%timeit dense_vag(tc.backend.ones([n]))
# Output:
#   30.7 ms ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)


%timeit sparse_vag(tc.backend.ones([n]))
# Output:
#   3.6 ms ± 63 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)


"""
Therefore, sparse matrix evaluation also saves time apart from space, which is always recommended.
"""



================================================
FILE: docs/source/tutorials/vqe_h2o_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 分子上的变分量子本征求解器 (VQE)
"""

"""
## 概述

VQE 是一种变分算法，用于计算满足 $H \left|\psi_g\right> =E_g\left|\psi_g\right>$ 的给定哈密顿 H 的基态，我们称之为 $\psi_g$。对于任意归一化波函数 $\psi_f$，期望值 $\left<\psi_f|H|\psi_f \right>$ 总是不低于基态能量，除非 $\psi_f = \psi_g$  (这里我们假设基态没有简并）。基于这个事实，如果我们使用参数化波函数 $\psi_\theta$，例如由具有参数 $\theta$ 的参数化量子电路 (PQC) 给出，我们可以通过最小化 $H$ 的期望值来给出基态能量和波函数的近似值。在实际的量子硬件中，该算法可以在量子-经典混合范式中实现，在量子硬件中使用有限差分或参数移位计算梯度，在经典计算机中使用梯度下降法进行优化。在数值模拟中，我们可以使用自动微分计算梯度。


计算分子的基态能量对于量子化学任务通常很重要，因为它可以用来找出分子的原子结构。在分子的模拟中，我们不考虑原子核的运动，这意味着我们固定了其组成原子的原子核坐标。我们只考虑分子中的电子，因为原子核比电子重得多，因此声子携带的能量可以忽略不计，或者可以使用 Born-Oppenheimer 近似重新考虑。严格来说，电子存在于连续空间中，因此希尔伯特空间是无限维的。为了进行实际计算，我们只保留一些重要的单粒子基，例如低能原子轨道。在二次量子化形式中，我们可以将这些原子轨道表示为 $c_i^\dagger|0>$。通过考虑原子核和电子的相互作用作为背景以及电子 - 电子相互作用，分子哈密顿量通常可以表示为 $H = \sum_{i, j} h_{i,j} c_i^\dagger c_j + \sum_{i, j, k, l} \alpha_{i, j, k, l} c_i^\dagger c_j^\dagger c_k c_l$。请注意，自旋指数也被吸收到轨道指数中。有很多软件可以在 H 中给出这些参数，例如我们将在本教程后面使用的 pyscf。现在我们有了对分子的费米子描述。通过使用从费米子算子到自旋算子的映射，例如 Jordan-Wigner 变换或 Bravyi-Kitaev 变换，我们可以将费米子哈密顿算子映射到与量子计算机更兼容的自旋哈密顿算子。对于自旋哈密顿算子，我们可以很容易地使用 PQC 来构造轨迹波函数并进行 VQE 算法。在本教程的以下部分，我们将演示如何使用 TensorCircuit 在分子上模拟 VQE 算法的完整示例。
"""

"""
## 设置

我们应该首先 ``pip install openfermion openfermionpyscf`` 根据 openfermion 和 pyscf 提供的量子化学计算生成H2O分子的费米子和量子比特哈密顿量。
"""

import numpy as np
from openfermion.chem import MolecularData
from openfermion.transforms import (
    get_fermion_operator,
    jordan_wigner,
    binary_code_transform,
    checksum_code,
    reorder,
)
from openfermion.chem import geometry_from_pubchem
from openfermion.utils import up_then_down
from openfermion.linalg import LinearQubitOperator
from openfermionpyscf import run_pyscf
import tensorflow as tf

import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## 生成哈密顿量
"""

"""
* 获取分子能量信息和分子轨道

"""

multiplicity = 1
basis = "sto-3g"
# H2O 的 14 个自旋轨道
geometry = geometry_from_pubchem("h2o")
description = "h2o"
molecule = MolecularData(geometry, basis, multiplicity, description=description)
molecule = run_pyscf(molecule, run_mp2=True, run_cisd=True, run_ccsd=True, run_fci=True)
print(molecule.fci_energy, molecule.ccsd_energy, molecule.hf_energy)
# Output:
#   -75.0155301894916 -75.01540899923558 -74.96444758276998


"""
* 获取费米子哈密顿量
"""

mh = molecule.get_molecular_hamiltonian()

fh = get_fermion_operator(mh)

print(fh.terms[((0, 1), (0, 0))])  # 获取费米子哈密顿量
# Output:
#   -32.68991541360029


"""
* 转换为量子比特哈密顿量
"""

# 对于 H2O 的 14 个轨道，诸如 JW 或 BK 之类的正常变换需要 14 个量子位

a = jordan_wigner(fh)
LinearQubitOperator(a).n_qubits
# Output:
#   14

"""
我们可以使用二进制编码来保存另外两个量子位，因为自旋向上和自旋向下填充的数量都是 5（5/7 个轨道中的奇数电子）
"""

b = binary_code_transform(reorder(fh, up_then_down), 2 * checksum_code(7, 1))
# 7 是 7 个自旋极化轨道，1 是奇数占用
LinearQubitOperator(b).n_qubits
# Output:
#   12

print(b.terms[((0, "Z"),)])  # Z_0 泡利字符串的系数
# Output:
#   12.412562749393349


"""
* 将 openfermion 中的量子比特哈密顿量转换为 TensorCircuit 中的格式
"""

lsb, wb = tc.templates.chems.get_ps(b, 12)
lsa, wa = tc.templates.chems.get_ps(a, 14)

"""
* 以矩阵形式检查哈密顿量
"""

ma = tc.quantum.PauliStringSum2COO_numpy(lsa, wa)

mb = tc.quantum.PauliStringSum2COO_numpy(lsb, wb)

mad, mbd = ma.todense(), mb.todense()

"""
这两种哈密顿量对应的 Hartree Fock 乘积状态
"""

bin(np.argmin(np.diag(mad)))
# Output:
#   '0b11111111110000'

bin(np.argmin(np.diag(mbd)))
# Output:
#   '0b111110111110'

"""
## VQE 设置

原则上，我们可以将哈密顿量的每个泡利串评估为期望测量，但它会花费大量模拟时间，相反，我们将它们融合为如上所示的哈密顿矩阵来运行 VQE。
"""

"""
* 使用密集矩阵期望
"""

n = 12
depth = 4
mbd_tf = tc.array_to_tensor(mbd)


def vqe(param):
    c = tc.Circuit(n)
    for i in [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]:
        c.X(i)
    for j in range(depth):
        for i in range(n - 1):
            c.exp1(i, i + 1, unitary=tc.gates._xx_matrix, theta=param[j, i, 0])
        for i in range(n):
            c.rx(i, theta=param[j, i, 1])
        for i in range(n):
            c.ry(i, theta=param[j, i, 2])
        for i in range(n):
            c.rx(i, theta=param[j, i, 3])
    return tc.templates.measurements.operator_expectation(c, mbd_tf)

vags = tc.backend.jit(tc.backend.value_and_grad(vqe))
lr = tf.keras.optimizers.schedules.ExponentialDecay(
    decay_rate=0.5, decay_steps=300, initial_learning_rate=0.5e-2
)
opt = tc.backend.optimizer(tf.keras.optimizers.Adam(lr))

param = tc.backend.implicit_randn(shape=[depth, n, 4], stddev=0.02, dtype="float32")
for i in range(600):
    e, g = vags(param)
    param = opt.update(g, param)
    if i % 100 == 0:
        print(e)
# Output:
#   tf.Tensor(-74.76671, shape=(), dtype=float32)

#   tf.Tensor(-74.95493, shape=(), dtype=float32)

#   tf.Tensor(-74.95319, shape=(), dtype=float32)

#   tf.Tensor(-74.954315, shape=(), dtype=float32)

#   tf.Tensor(-74.956116, shape=(), dtype=float32)

#   tf.Tensor(-74.95809, shape=(), dtype=float32)


"""
* 使用稀疏矩阵期望

我们还可以使用稀疏哈密顿矩阵进行电路期望评估，唯一的区别是将 ``mbd_tf`` 替换为 ``mb_tf``
"""

mb_tf = tc.backend.coo_sparse_matrix(
    np.transpose(np.stack([mb.row, mb.col])), mb.data, shape=(2**n, 2**n)
)

"""
稀疏矩阵评估和密集矩阵评估之间的一个微基准，用于比较计算期望的时间，当然，稀疏总是在空间方面获胜。
"""

def dense_expt(param):
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
        c.rx(i, theta=param[i])
    return tc.templates.measurements.operator_expectation(c, mbd_tf)


def sparse_expt(param):
    c = tc.Circuit(n)
    for i in range(n):
        c.H(i)
        c.rx(i, theta=param[i])
    return tc.templates.measurements.operator_expectation(c, mb_tf)

dense_vag = tc.backend.jit(tc.backend.value_and_grad(dense_expt))
sparse_vag = tc.backend.jit(tc.backend.value_and_grad(sparse_expt))

v0, g0 = dense_vag(tc.backend.ones([n]))
v1, g1 = sparse_vag(tc.backend.ones([n]))

# 一致性检查

np.testing.assert_allclose(v0, v1, atol=1e-5)
np.testing.assert_allclose(g0, g1, atol=1e-5)

%timeit dense_vag(tc.backend.ones([n]))
# Output:
#   30.7 ms ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)


%timeit sparse_vag(tc.backend.ones([n]))
# Output:
#   3.6 ms ± 63 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)


"""
因此，稀疏矩阵求值除了节省空间外，还可以节省时间，这总是被推荐的。
"""



================================================
FILE: docs/source/whitepaper/3-circuits-gates.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Circuits and Gates
"""

"""
## Overview

In TensorCircuit, a quantum circuit on $n$ qubits -- which supports both noiseless and noisy simulations via Monte Carlo trajectory methods --  is created by the ``tc.Circuit(n)`` API. Here we show how to create basic circuits, apply gates to them, and compute various outputs.  
"""

"""
## Setup
"""

import inspect
import numpy as np
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
In TensorCircuit the default runtime datatype is complex64, but if higher precision is required this can be set as follows
"""

tc.set_dtype("complex128")
# Output:
#   ('complex128', 'float64')

"""
## Basic Circuits and Outputs

Quantum circuits can be constructed as follows.
"""

c = tc.Circuit(2)
c.h(0)
c.cnot(0, 1)
c.rx(1, theta=0.2)

"""
**Output: state**

From this, various outputs can be computed.

The full wavefunction can be obtained via

"""

c.state()
# Output:
#   <tf.Tensor: shape=(4,), dtype=complex128, numpy=

#   array([0.70357418+0.j        , 0.        -0.07059289j,

#          0.        -0.07059289j, 0.70357418+0.j        ])>

"""
The full wavefunction can also be used to generate the reduced density matrix of a subset of the qubits

"""

# reduced density matrix for qubit 1
s = c.state()
tc.quantum.reduced_density_matrix(s, cut=[0])  # cut: list of qubit indices to trace out
# Output:
#   <tf.Tensor: shape=(2, 2), dtype=complex128, numpy=

#   array([[0.5+0.j, 0. +0.j],

#          [0. +0.j, 0.5+0.j]])>

"""
Amplitudes of individual basis vectors are computed by passing the corresponding bit-string value to the ``amplitude`` function. For example, the amplitude of the $\vert{10}\rangle$ basis vector is computed by

"""

c.amplitude("10")
# Output:
#   <tf.Tensor: shape=(), dtype=complex128, numpy=-0.0705928857402556j>

"""
The unitary matrix corresponding to the entire quantum circuit can also be output.
"""

c.matrix()
# Output:
#   <tf.Tensor: shape=(4, 4), dtype=complex128, numpy=

#   array([[ 0.70357418+0.j        ,  0.        -0.07059289j,

#            0.70357418+0.j        ,  0.        -0.07059289j],

#          [ 0.        -0.07059289j,  0.70357418+0.j        ,

#            0.        -0.07059289j,  0.70357418+0.j        ],

#          [ 0.        -0.07059289j,  0.70357418+0.j        ,

#            0.        +0.07059289j, -0.70357418+0.j        ],

#          [ 0.70357418+0.j        ,  0.        -0.07059289j,

#           -0.70357418+0.j        ,  0.        +0.07059289j]])>

"""
**Output: measurement**

Random samples corresponding to $Z$-measurements on all qubits can be generated using the following API, which will output a $(\text{bitstring}, \text{probability})$ tuple, comprising a binary string corresponding to the measurement outcomes of a Z measurement on all the qubits and the associated probability of obtaining that outcome.  Z measurements on a subset of qubits can be performed with the ``measure`` command
"""

c.sample()
# Output:
#   (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>,

#    <tf.Tensor: shape=(), dtype=float64, numpy=0.4950166615971341>)

c.measure(0, with_prob=True)
# Output:
#   (<tf.Tensor: shape=(1,), dtype=float64, numpy=array([1.])>,

#    <tf.Tensor: shape=(), dtype=float64, numpy=0.5000000171142709>)

c.measure(0, 1, with_prob=True)
# Output:
#   (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>,

#    <tf.Tensor: shape=(), dtype=float64, numpy=0.4950166615971341>)

"""
**Output: expectation**
    
Expectation values such as $\langle X_0 \rangle$, $\langle X_1 + Z_1\rangle$ or $\langle Z_0 Z_1\rangle$ can be computed via the ${\sf expectation}$ method of a circuit object
"""

print(c.expectation([tc.gates.x(), [0]]))  # <X0>
print(c.expectation([tc.gates.x() + tc.gates.z(), [1]]))  # <X1 + Z1>
print(c.expectation([tc.gates.z(), [0]], [tc.gates.z(), [1]]))  # <Z0 Z1>
# Output:
#   tf.Tensor(0j, shape=(), dtype=complex128)

#   tf.Tensor(0j, shape=(), dtype=complex128)

#   tf.Tensor((0.9800665437029109+0j), shape=(), dtype=complex128)


# user-defined operator

c.expectation([np.array([[3, 2], [2, -3]]), [0]])
# Output:
#   <tf.Tensor: shape=(), dtype=complex128, numpy=0j>

"""
While expectations of products of Pauli operators, e.g. $\langle Z_0 X_1\rangle$ can be computed using ``c.expectation`` as above, TensorCircuit provides another way of computing such expressions which may be more convenient for longer Pauli strings, and longer Pauli strings can similarly be computed by providing lists of indices corresponding to the qubits that the $X,Y,Z$ operators act on.
"""

c.expectation_ps(x=[1], z=[0])
# Output:
#   <tf.Tensor: shape=(), dtype=complex128, numpy=0j>

"""
## Built-in Gates
"""

"""
TensorCircuit provides support for a wide variety of commonly encountered quantum gates.  The full list is as below.
"""

for g in tc.Circuit.sgates:
    gf = getattr(tc.gates, g)
    print(g)
    print(tc.gates.matrix_for_gate(gf()))
# Output:
#   i

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j]]

#   x

#   [[0.+0.j 1.+0.j]

#    [1.+0.j 0.+0.j]]

#   y

#   [[0.+0.j 0.-1.j]

#    [0.+1.j 0.+0.j]]

#   z

#   [[ 1.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j]]

#   h

#   [[ 0.70710678+0.j  0.70710678+0.j]

#    [ 0.70710678+0.j -0.70710678+0.j]]

#   t

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710678+0.70710678j]]

#   s

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   td

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710677-0.70710677j]]

#   sd

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.-1.j]]

#   wroot

#   [[ 0.70710678+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710678+0.j ]]

#   cnot

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   cz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -1.+0.j]]

#   swap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   cy

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.-1.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]]

#   iswap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]

#    [0.+0.j 0.+1.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   ox

#   [[0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oy

#   [[0.+0.j 0.-1.j 0.+0.j 0.+0.j]

#    [0.+1.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]]

#   toffoli

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   fredkin

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]]


for g in tc.Circuit.vgates:
    print(g, inspect.signature(getattr(tc.gates, g).f))
# Output:
#   r (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   cr (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   rx (theta: float = 0) -> tensorcircuit.gates.Gate

#   ry (theta: float = 0) -> tensorcircuit.gates.Gate

#   rz (theta: float = 0) -> tensorcircuit.gates.Gate

#   crx (*args: Any, **kws: Any) -> Any

#   cry (*args: Any, **kws: Any) -> Any

#   crz (*args: Any, **kws: Any) -> Any

#   orx (*args: Any, **kws: Any) -> Any

#   ory (*args: Any, **kws: Any) -> Any

#   orz (*args: Any, **kws: Any) -> Any

#   any (unitary: Any, name: str = 'any') -> tensorcircuit.gates.Gate

#   exp (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate

#   exp1 (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate


"""
Also, we have built-in matrices as
"""

for name in dir(tc.gates):
    if name.endswith("_matrix"):
        print(name, ":\n", getattr(tc.gates, name))
# Output:
#   _cnot_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]

#    [0. 0. 1. 0.]]

#   _cy_matrix :

#    [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -0.-1.j]

#    [ 0.+0.j  0.+0.j  0.+1.j  0.+0.j]]

#   _cz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0.  1.  0.  0.]

#    [ 0.  0.  1.  0.]

#    [ 0.  0.  0. -1.]]

#   _fredkin_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]]

#   _h_matrix :

#    [[ 0.70710678  0.70710678]

#    [ 0.70710678 -0.70710678]]

#   _i_matrix :

#    [[1. 0.]

#    [0. 1.]]

#   _ii_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 0. 0. 1.]]

#   _s_matrix :

#    [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   _swap_matrix :

#    [[1. 0. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]]

#   _t_matrix :

#    [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710678+0.70710678j]]

#   _toffoli_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]]

#   _wroot_matrix :

#    [[ 0.70710678+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710678+0.j ]]

#   _x_matrix :

#    [[0. 1.]

#    [1. 0.]]

#   _xx_matrix :

#    [[0. 0. 0. 1.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [1. 0. 0. 0.]]

#   _y_matrix :

#    [[ 0.+0.j -0.-1.j]

#    [ 0.+1.j  0.+0.j]]

#   _yy_matrix :

#    [[ 0.+0.j  0.-0.j  0.-0.j -1.+0.j]

#    [ 0.+0.j  0.+0.j  1.-0.j  0.-0.j]

#    [ 0.+0.j  1.-0.j  0.+0.j  0.-0.j]

#    [-1.+0.j  0.+0.j  0.+0.j  0.+0.j]]

#   _z_matrix :

#    [[ 1.  0.]

#    [ 0. -1.]]

#   _zz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
**Arbitrary unitaries.** User-defined unitary gates may be implemented by specifying their matrix elements as an array. As an example, the unitary $S = \begin{pmatrix} 1 & 0 \\  0 & i\end{pmatrix}$ -- which can also directly be added by calling ``c.s()``
can be implemented as
"""

c.unitary(0, unitary=np.array([[1, 0], [0, 1j]]), name="S")

# the optional name argument specifies how this gate is displayed when the circuit is output to \LaTeX

"""
**Exponential gates.** Gates of the form $e^{i\theta G}$ where matrix $G$ satisfies $G^2 = I$ admit a fast implementation via the ``exp1`` command, e.g., the two-qubit gate $e^{i\theta Z\otimes Z}$ acting on qubits $0$ and $1$
"""

c.exp1(0, 1, theta=0.2, unitary=tc.gates._zz_matrix)

"""
General exponential gates, where $G^2\neq 1$ can be implemented via the ``exp`` command:
"""

c.exp(0, theta=0.2, unitary=np.array([[2, 0], [0, 1]]))

"""
**Non-unitary gates.**
TensorCircuit also supports the application of non-unitary gates, either by providing a non-unitary matrix as the argument to  ``c.unitary`` or by supplying a complex angle $\theta$ to an exponential gate.
"""

c.unitary(0, unitary=np.array([[1, 2], [2, 3]]), name="non_unitary")
c.exp1(0, theta=0.2 + 1j, unitary=tc.gates._x_matrix)

"""
Note that the non-unitary gates will lead to an output state that is no longer normalized since normalization is often unnecessary and takes extra time which can be avoided.
"""

"""
## Specifying the Input State and Composing Circuits
"""

"""
By default, quantum circuits are applied to the initial all zero product state.  Arbitrary initial states can be set by passing an array containing the input state amplitudes to the optional ``inputs`` argument of ``tc.Circuit``.  For example, the  maximally entangled state $\frac{\vert{00}\rangle+\vert{11}\rangle}{\sqrt{2}}$ can be input as follows.
"""

c1 = tc.Circuit(2, inputs=np.array([1, 0, 0, 1] / np.sqrt(2)))

"""
Circuits that act on the same number of qubits can be composed together via the ``c.append()`` or ``c.prepend()`` commands. With ``c1`` defined as above, we can create a new circuit ``c2`` and then compose them together:
"""

c2 = tc.Circuit(2)
c2.cnot(1, 0)

c3 = c1.append(c2)
c3.state()
# Output:
#   <tf.Tensor: shape=(4,), dtype=complex128, numpy=array([0.70710678+0.j, 0.70710678+0.j, 0.        +0.j, 0.        +0.j])>

"""
## Circuit Transformation and Visualization
"""

"""
``tc.Circuit`` objects can be converted to and from Qiskit ``QuantumCircuit`` objects.
"""

c = tc.Circuit(2)
c.H(0)
c.cnot(1, 0)
cq = c.to_qiskit()

c1 = tc.Circuit.from_qiskit(cq)

# print the quantum circuit intermediate representation

c1.to_qir()
# Output:
#   [{'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex128, numpy=

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]])>,

#         edges: [

#             Edge('cnot'[3] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-1'[0] )

#         ]),

#     'index': (0,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             <tf.Tensor: shape=(2, 2, 2, 2), dtype=complex128, numpy=

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]])>,

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge(Dangling Edge)[1],

#             Edge('cnot'[2] -> 'qb-2'[0] ),

#             Edge('cnot'[3] -> 'h'[0] )

#         ]),

#     'index': (1, 0),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False}]

"""
There are two ways to visualize quantum circuits generated in TensorCircuit.  The first is to use ``c.tex()`` to get \Latex quantikz commands.

"""

c.tex()
# Output:
#   '\\begin{quantikz}\n\\lstick{$\\ket{0}$}&\\gate{h} &\\targ{} &\\qw \\\\\n\\lstick{$\\ket{0}$}&\\qw &\\ctrl{-1} &\\qw \n\\end{quantikz}'

"""
The second method uses the draw function from [qiskit](https://qiskit.org/documentation/stubs/qiskit.circuit.QuantumCircuit.draw.html).
"""

c.draw(output="mpl")
# Output:
#   <Figure size 206.852x144.48 with 1 Axes>



================================================
FILE: docs/source/whitepaper/3-circuits-gates_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 电路和门
"""

"""
## 概述

在 TensorCircuit 中，$n$ 量子比特上的量子电路——通过蒙特卡洛轨迹方法支持无噪声和有噪声的模拟——由“tc.Circuit(n)”API 创建。
在这里，我们展示了如何创建基本电路，对它们应用门，并计算各种输出。
"""

"""
## 设置
"""

import inspect
import numpy as np
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
在 TensorCircuit 中，默认的运行时数据类型是 complex64，但如果需要更高的精度，可以按照如下设置
"""

tc.set_dtype("complex128")
# Output:
#   ('complex128', 'float64')

"""
## 基本电路和输出

量子电路按照可以如下构造。
"""

c = tc.Circuit(2)
c.h(0)
c.cnot(0, 1)
c.rx(1, theta=0.2)

"""
**输出：量子态**

由此，可以计算各种输出。

完整的波函数可以通过

"""

c.state()
# Output:
#   <tf.Tensor: shape=(4,), dtype=complex128, numpy=

#   array([0.70357418+0.j        , 0.        -0.07059289j,

#          0.        -0.07059289j, 0.70357418+0.j        ])>

"""
全波函数也可用于生成部分比特上的约化密度矩阵

"""

# 量子位 1 的约化密度矩阵
s = c.state()
tc.quantum.reduced_density_matrix(s, cut=[0])  # cut：要追踪的量子位索引列表
# Output:
#   <tf.Tensor: shape=(2, 2), dtype=complex128, numpy=

#   array([[0.5+0.j, 0. +0.j],

#          [0. +0.j, 0.5+0.j]])>

"""
通过将相应的位串值传递给 ``amplitude(振幅)`` 函数来计算单个基向量的振幅。例如，$\vert{10}\rangle$ 基向量的振幅由下式计算
"""

c.amplitude("10")
# Output:
#   <tf.Tensor: shape=(), dtype=complex128, numpy=-0.0705928857402556j>

"""
也可以输出对应整个量子电路的幺正矩阵。
"""

c.matrix()
# Output:
#   <tf.Tensor: shape=(4, 4), dtype=complex128, numpy=

#   array([[ 0.70357418+0.j        ,  0.        -0.07059289j,

#            0.70357418+0.j        ,  0.        -0.07059289j],

#          [ 0.        -0.07059289j,  0.70357418+0.j        ,

#            0.        -0.07059289j,  0.70357418+0.j        ],

#          [ 0.        -0.07059289j,  0.70357418+0.j        ,

#            0.        +0.07059289j, -0.70357418+0.j        ],

#          [ 0.70357418+0.j        ,  0.        -0.07059289j,

#           -0.70357418+0.j        ,  0.        +0.07059289j]])>

"""
**输出：测量**

可以使用以下 API 生成与所有 qubits 上的 $Z$-measurements 相对应的随机样本，
该 API 将输出一个 $(\text{bitstring}, \text{probability})$ 元组，包括与测量结果对应的二进制字符串
对所有量子比特的 Z 测量以及获得该结果的相关概率。可以使用“measure”命令对量子比特子集执行 Z 测量
"""

c.sample()
# Output:
#   (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>,

#    <tf.Tensor: shape=(), dtype=float64, numpy=0.4950166615971341>)

c.measure(0, with_prob=True)
# Output:
#   (<tf.Tensor: shape=(1,), dtype=float64, numpy=array([1.])>,

#    <tf.Tensor: shape=(), dtype=float64, numpy=0.5000000171142709>)

c.measure(0, 1, with_prob=True)
# Output:
#   (<tf.Tensor: shape=(2,), dtype=float64, numpy=array([1., 1.])>,

#    <tf.Tensor: shape=(), dtype=float64, numpy=0.4950166615971341>)

"""
**输出：期望**
    
期望值，例如 $\langle X_0 \rangle$、$\langle X_1 + Z_1\rangle$ 或 $\langle Z_0 Z_1\rangle$ 可以通过电路对象的 ${\sf expectation}$ 方法计算
"""

print(c.expectation([tc.gates.x(), [0]]))  # <X0>
print(c.expectation([tc.gates.x() + tc.gates.z(), [1]]))  # <X1 + Z1>
print(c.expectation([tc.gates.z(), [0]], [tc.gates.z(), [1]]))  # <Z0 Z1>
# Output:
#   tf.Tensor(0j, shape=(), dtype=complex128)

#   tf.Tensor(0j, shape=(), dtype=complex128)

#   tf.Tensor((0.9800665437029109+0j), shape=(), dtype=complex128)


# 用户定义运算符

c.expectation([np.array([[3, 2], [2, -3]]), [0]])
# Output:
#   <tf.Tensor: shape=(), dtype=complex128, numpy=0j>

"""
而泡利字符串的期望，例如 $\langle Z_0 X_1\rangle$ 可以使用上面的 c.expectation 计算，TensorCircuit 提供了另一种计算此类表达式的方法，
这对于更长的 Pauli 字符串可能更方便，更长的 Pauli 字符串可以类似地通过提供与 $X,Y,Z$ 运算符作用的量子比特相对应的索引列表计算得到。
"""

c.expectation_ps(x=[1], z=[0])
# Output:
#   <tf.Tensor: shape=(), dtype=complex128, numpy=0j>

"""
## 内置门
"""

"""
TensorCircuit 为各种常见的量子门提供支持。完整列表如下。
"""

for g in tc.Circuit.sgates:
    gf = getattr(tc.gates, g)
    print(g)
    print(tc.gates.matrix_for_gate(gf()))
# Output:
#   i

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j]]

#   x

#   [[0.+0.j 1.+0.j]

#    [1.+0.j 0.+0.j]]

#   y

#   [[0.+0.j 0.-1.j]

#    [0.+1.j 0.+0.j]]

#   z

#   [[ 1.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j]]

#   h

#   [[ 0.70710678+0.j  0.70710678+0.j]

#    [ 0.70710678+0.j -0.70710678+0.j]]

#   t

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710678+0.70710678j]]

#   s

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   td

#   [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710677-0.70710677j]]

#   sd

#   [[1.+0.j 0.+0.j]

#    [0.+0.j 0.-1.j]]

#   wroot

#   [[ 0.70710678+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710678+0.j ]]

#   cnot

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   cz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -1.+0.j]]

#   swap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   cy

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.-1.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]]

#   iswap

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+1.j 0.+0.j]

#    [0.+0.j 0.+1.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   ox

#   [[0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oy

#   [[0.+0.j 0.-1.j 0.+0.j 0.+0.j]

#    [0.+1.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]

#   oz

#   [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j -1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j  1.+0.j]]

#   toffoli

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]]

#   fredkin

#   [[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]

#    [0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]]


for g in tc.Circuit.vgates:
    print(g, inspect.signature(getattr(tc.gates, g).f))
# Output:
#   r (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   cr (theta: float = 0, alpha: float = 0, phi: float = 0) -> tensorcircuit.gates.Gate

#   rx (theta: float = 0) -> tensorcircuit.gates.Gate

#   ry (theta: float = 0) -> tensorcircuit.gates.Gate

#   rz (theta: float = 0) -> tensorcircuit.gates.Gate

#   crx (*args: Any, **kws: Any) -> Any

#   cry (*args: Any, **kws: Any) -> Any

#   crz (*args: Any, **kws: Any) -> Any

#   orx (*args: Any, **kws: Any) -> Any

#   ory (*args: Any, **kws: Any) -> Any

#   orz (*args: Any, **kws: Any) -> Any

#   any (unitary: Any, name: str = 'any') -> tensorcircuit.gates.Gate

#   exp (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate

#   exp1 (unitary: Any, theta: float, name: str = 'none') -> tensorcircuit.gates.Gate


"""
此外，我们有如下内置矩阵
"""

for name in dir(tc.gates):
    if name.endswith("_matrix"):
        print(name, ":\n", getattr(tc.gates, name))
# Output:
#   _cnot_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]

#    [0. 0. 1. 0.]]

#   _cy_matrix :

#    [[ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]

#    [ 0.+0.j  0.+0.j  0.+0.j -0.-1.j]

#    [ 0.+0.j  0.+0.j  0.+1.j  0.+0.j]]

#   _cz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0.  1.  0.  0.]

#    [ 0.  0.  1.  0.]

#    [ 0.  0.  0. -1.]]

#   _fredkin_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]]

#   _h_matrix :

#    [[ 0.70710678  0.70710678]

#    [ 0.70710678 -0.70710678]]

#   _i_matrix :

#    [[1. 0.]

#    [0. 1.]]

#   _ii_matrix :

#    [[1. 0. 0. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 0. 0. 1.]]

#   _s_matrix :

#    [[1.+0.j 0.+0.j]

#    [0.+0.j 0.+1.j]]

#   _swap_matrix :

#    [[1. 0. 0. 0.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [0. 0. 0. 1.]]

#   _t_matrix :

#    [[1.        +0.j         0.        +0.j        ]

#    [0.        +0.j         0.70710678+0.70710678j]]

#   _toffoli_matrix :

#    [[1. 0. 0. 0. 0. 0. 0. 0.]

#    [0. 1. 0. 0. 0. 0. 0. 0.]

#    [0. 0. 1. 0. 0. 0. 0. 0.]

#    [0. 0. 0. 1. 0. 0. 0. 0.]

#    [0. 0. 0. 0. 1. 0. 0. 0.]

#    [0. 0. 0. 0. 0. 1. 0. 0.]

#    [0. 0. 0. 0. 0. 0. 0. 1.]

#    [0. 0. 0. 0. 0. 0. 1. 0.]]

#   _wroot_matrix :

#    [[ 0.70710678+0.j  -0.5       -0.5j]

#    [ 0.5       -0.5j  0.70710678+0.j ]]

#   _x_matrix :

#    [[0. 1.]

#    [1. 0.]]

#   _xx_matrix :

#    [[0. 0. 0. 1.]

#    [0. 0. 1. 0.]

#    [0. 1. 0. 0.]

#    [1. 0. 0. 0.]]

#   _y_matrix :

#    [[ 0.+0.j -0.-1.j]

#    [ 0.+1.j  0.+0.j]]

#   _yy_matrix :

#    [[ 0.+0.j  0.-0.j  0.-0.j -1.+0.j]

#    [ 0.+0.j  0.+0.j  1.-0.j  0.-0.j]

#    [ 0.+0.j  1.-0.j  0.+0.j  0.-0.j]

#    [-1.+0.j  0.+0.j  0.+0.j  0.+0.j]]

#   _z_matrix :

#    [[ 1.  0.]

#    [ 0. -1.]]

#   _zz_matrix :

#    [[ 1.  0.  0.  0.]

#    [ 0. -1.  0. -0.]

#    [ 0.  0. -1. -0.]

#    [ 0. -0. -0.  1.]]


"""
**任意幺正** 用户定义的幺正门可以通过将其矩阵元素指定为数组来实现。例如，S 量子门 $S = \begin{pmatrix} 1 & 0 \\ 0 & i\end{pmatrix}$ -- 也可以通过调用 ``c.s()``
直接添加可以实现为
"""

c.unitary(0, unitary=np.array([[1, 0], [0, 1j]]), name="S")

# 可选的名称参数指定当电路输出到 \LaTeX 时如何显示此门

"""
**指数门。** 形式为 $e^{i\theta G}$ 的门，其中矩阵 $G$ 满足 $G^2 = I$ 允许通过 ``exp1`` 命令快速实现，例如， 双比特门 $e^{i\theta Z\otimes Z}$ 作用于量子比特 $0$ 和 $1$
"""

c.exp1(0, 1, theta=0.2, unitary=tc.gates._zz_matrix)

"""
一般指数门，其中 $G^2\neq 1$ 可以通过 ``exp`` 命令实现：
"""

c.exp(0, theta=0.2, unitary=np.array([[2, 0], [0, 1]]))

"""
**非幺正门。**
TensorCircuit 还支持非幺正门的应用，或者通过提供一个非幺正矩阵作为“c.unitary”的参数，或者通过提供一个复角 $\theta$ 给指数门。
"""

c.unitary(0, unitary=np.array([[1, 2], [2, 3]]), name="non_unitary")
c.exp1(0, theta=0.2 + 1j, unitary=tc.gates._x_matrix)

"""
请注意，非幺正门将导致不再归一化的输出状态，因为归一化通常是不必要的并且需要额外的时间，这是可以避免的。
"""

"""
## 指定输入状态和拼接电路
"""

"""
默认情况下，量子电路应用于初始全零乘积状态。 可以通过将包含输入状态幅度的数组传递给“tc.Circuit”的可选“inputs”参数来设置任意初始状态。
例如，最大纠缠态 $\frac{\vert{00}\rangle+\vert{11}\rangle}{\sqrt{2}}$ 可以如下输入。
"""

c1 = tc.Circuit(2, inputs=np.array([1, 0, 0, 1] / np.sqrt(2)))

"""
作用于相同数量的量子比特的电路可以通过“c.append()”或“c.prepend()”命令组合在一起。
通过上面定义的“c1”，我们可以创建一个新的电路“c2”，然后将它们组合在一起：
"""

c2 = tc.Circuit(2)
c2.cnot(1, 0)

c3 = c1.append(c2)
c3.state()
# Output:
#   <tf.Tensor: shape=(4,), dtype=complex128, numpy=array([0.70710678+0.j, 0.70710678+0.j, 0.        +0.j, 0.        +0.j])>

"""
## 电路变换和可视化
"""

"""
``tc.Circuit`` 对象可以与 Qiskit ``QuantumCircuit`` 对象相互转换。
"""

c = tc.Circuit(2)
c.H(0)
c.cnot(1, 0)
cq = c.to_qiskit()

c1 = tc.Circuit.from_qiskit(cq)

# 打印量子电路中间表示

c1.to_qir()
# Output:
#   [{'gatef': h,

#     'gate': Gate(

#         name: 'h',

#         tensor:

#             <tf.Tensor: shape=(2, 2), dtype=complex128, numpy=

#             array([[ 0.70710677+0.j,  0.70710677+0.j],

#                    [ 0.70710677+0.j, -0.70710677+0.j]])>,

#         edges: [

#             Edge('cnot'[3] -> 'h'[0] ),

#             Edge('h'[1] -> 'qb-1'[0] )

#         ]),

#     'index': (0,),

#     'name': 'h',

#     'split': None,

#     'mpo': False},

#    {'gatef': cnot,

#     'gate': Gate(

#         name: 'cnot',

#         tensor:

#             <tf.Tensor: shape=(2, 2, 2, 2), dtype=complex128, numpy=

#             array([[[[1.+0.j, 0.+0.j],

#                      [0.+0.j, 0.+0.j]],

#             

#                     [[0.+0.j, 1.+0.j],

#                      [0.+0.j, 0.+0.j]]],

#             

#             

#                    [[[0.+0.j, 0.+0.j],

#                      [0.+0.j, 1.+0.j]],

#             

#                     [[0.+0.j, 0.+0.j],

#                      [1.+0.j, 0.+0.j]]]])>,

#         edges: [

#             Edge(Dangling Edge)[0],

#             Edge(Dangling Edge)[1],

#             Edge('cnot'[2] -> 'qb-2'[0] ),

#             Edge('cnot'[3] -> 'h'[0] )

#         ]),

#     'index': (1, 0),

#     'name': 'cnot',

#     'split': None,

#     'mpo': False}]

"""
有两种方法可以可视化 TensorCircuit 中生成的量子电路。 第一种是使用 ``c.tex()`` 来获取 \Latex quantikz 命令。
"""

c.tex()
# Output:
#   '\\begin{quantikz}\n\\lstick{$\\ket{0}$}&\\gate{h} &\\targ{} &\\qw \\\\\n\\lstick{$\\ket{0}$}&\\qw &\\ctrl{-1} &\\qw \n\\end{quantikz}'

"""
第二种方法使用 [qiskit](https://qiskit.org/documentation/stubs/qiskit.circuit.QuantumCircuit.draw.html) 中的绘图功能。
"""

c.draw(output="mpl")
# Output:
#   <Figure size 206.852x144.48 with 1 Axes>



================================================
FILE: docs/source/whitepaper/4-gradient-optimization.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Gradient and Variational Optimization
"""

"""
## Overview

TensorCircuit is designed to make optimization of parameterized quantum gates easy, fast, and convenient. In this note, we review how to obtain circuit gradients and run variational optimization.
"""

"""
## Setup
"""

import numpy as np
import scipy.optimize as optimize
import tensorflow as tf
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## PQC

 Consider a variational circuit acting on $n$ qubits, and consisting of $k$ layers, where each layer comprises parameterized $e^{i\theta X\otimes X}$ gates between neighboring qubits followed by a sequence of single qubit parameterized $Z$ and $X$ rotations. We now show how to implement such circuits in TensorCircuit, and how to use one of the machine learning backends to compute cost functions and gradients easily and efficiently.

The circuit for general $n,k$ and set of parameters can be defined as follows:
"""

def qcircuit(n, k, params):
    c = tc.Circuit(n)
    for j in range(k):
        for i in range(n - 1):
            c.exp1(
                i, i + 1, theta=params[j * (3 * n - 1) + i], unitary=tc.gates._xx_matrix
            )
        for i in range(n):
            c.rz(i, theta=params[j * (3 * n - 1) + n - 1 + i])
            c.rx(i, theta=params[j * (3 * n - 1) + 2 * n - 1 + i])
    return c

"""
As an example, we take $n=3, k=2$, set TensorFlow as our backend, and define an energy cost function to minimize
$$E = \langle X_0 X_1\rangle_\theta + \langle X_1 X_2\rangle_\theta.$$ 
"""

n = 3
k = 2


def energy(params):
    c = qcircuit(n, k, params)
    e = c.expectation_ps(x=[0, 1]) + c.expectation_ps(x=[1, 2])
    return K.real(e)

"""
## Grad and JIT
"""

"""
Using the ML backend support for automatic differentiation, we can now quickly compute both the energy and the gradient of the energy with respect to the parameters.
"""

energy_val_grad = K.value_and_grad(energy)

"""
This creates a function that given a set of parameters as input, returns both the energy and the gradient of the energy. If only the gradient is desired, then this can be computed by ``K.grad(energy)``. While we could run the above code directly on a set of parameters, if multiple evaluations of the energy will be performed, significant time savings can be had by using a just-in-time compiled version of the function.
"""

energy_val_grad_jit = K.jit(energy_val_grad)

"""
With ``K.jit``, the initial evaluation of the energy and gradient may take longer, but subsequent evaluations will be noticeably faster than non-jitted code. We recommend always using ``jit`` as long as the function is "tensor-in, tensor-out", and we have worked hard to make all aspects of the circuit simulator compatible with JIT.
"""

"""
## Optimization via ML Backend

With the energy function and gradients available, optimization of the parameters is straightforward.  Below is an example of how to do this via stochastic gradient descent.
"""

learning_rate = 2e-2
opt = K.optimizer(tf.keras.optimizers.SGD(learning_rate))


def grad_descent(params, i):
    val, grad = energy_val_grad_jit(params)
    params = opt.update(grad, params)
    if i % 10 == 0:
        print(f"i={i}, energy={val}")
    return params


params = K.implicit_randn(k * (3 * n - 1))
for i in range(100):
    params = grad_descent(params, i)
# Output:
#   i=0, energy=0.11897378414869308

#   i=10, energy=-0.3692811131477356

#   i=20, energy=-0.7194114923477173

#   i=30, energy=-0.904697597026825

#   i=40, energy=-1.013866662979126

#   i=50, energy=-1.1042678356170654

#   i=60, energy=-1.1998062133789062

#   i=70, energy=-1.308410406112671

#   i=80, energy=-1.4276418685913086

#   i=90, energy=-1.5474387407302856


"""
## Optimization via Scipy Interface

An alternative to using the machine learning backends for the optimization is to use SciPy.
This can be done via the ``scipy_interface`` API call and allows for gradient-based (e.g. BFGS) and non-gradient-based (e.g. COBYLA) optimizers to be used, which are not available via the ML backends.
"""

f_scipy = tc.interfaces.scipy_interface(energy, shape=[k * (3 * n - 1)], jit=True)
params = K.implicit_randn(k * (3 * n - 1))
r = optimize.minimize(f_scipy, params, method="L-BFGS-B", jac=True)
r
# Output:
#   /Users/shixin/Cloud/newwork/quantum-information/codebases/tensorcircuit/tensorcircuit/interfaces.py:237: ComplexWarning: Casting complex values to real discards the imaginary part

#     scipy_gs = scipy_gs.astype(np.float64)

#         fun: -2.000000476837158

#    hess_inv: <16x16 LbfgsInvHessProduct with dtype=float64>

#         jac: array([ 2.43186951e-04, -1.50322914e-04,  8.94665718e-05,  1.18807920e-05,

#           2.95639038e-05,  1.19209290e-07, -5.96046448e-08, -2.98023224e-08,

#           0.00000000e+00, -1.19209290e-07,  3.90738450e-07,  9.34305717e-07,

#          -8.22039729e-05,  1.19209290e-07,  0.00000000e+00,  0.00000000e+00])

#     message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'

#        nfev: 60

#         nit: 19

#        njev: 60

#      status: 0

#     success: True

#           x: array([ 2.35625520e+00,  7.85409154e-01,  1.57088576e+00,  2.10625989e-05,

#          -1.57088425e+00, -1.70256902e+00, -5.33743572e-01,  3.11436816e-01,

#           1.26543793e+00,  1.91663337e+00, -1.15901008e-07, -1.76623396e-05,

#          -1.59972887e-04, -8.97072367e-01,  1.79929630e+00, -9.67278961e-01])

"""
The first line above specifies the shape of the parameters to be supplied to the function to be minimized, which here is the energy function.  The ``jit=True`` argument automatically takes care of jitting the energy function.  Gradient-free optimization can similarly be performed efficiently by supplying the ``gradient=False`` argument to  ``scipy_interface``.
"""

f_scipy = tc.interfaces.scipy_interface(
    energy, shape=[k * (3 * n - 1)], jit=True, gradient=False
)
params = K.implicit_randn(k * (3 * n - 1))
r = optimize.minimize(f_scipy, params, method="COBYLA")
r
# Output:
#        fun: -1.9999911785125732

#      maxcv: 0.0

#    message: 'Optimization terminated successfully.'

#       nfev: 386

#     status: 1

#    success: True

#          x: array([ 7.87597857e-01, -5.14158452e-01, -1.56560250e+00, -3.15230777e-04,

#           9.91532990e-01,  5.95588091e-01,  1.38523058e+00, -3.59642968e-04,

#          -3.23365306e-01, -4.16465772e-01, -7.32259085e-03,  6.53997758e-05,

#           7.71203778e-01,  2.46256921e+00,  8.78602039e-01, -3.51989842e-01])



================================================
FILE: docs/source/whitepaper/4-gradient-optimization_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 梯度和变分优化
"""

"""
## 概述

TensorCircuit 旨在使参数化量子门的优化变得简单、快速和方便。 在本说明中，我们回顾了如何获得电路梯度和运行变分优化。
"""

"""
## 设置
"""

import numpy as np
import scipy.optimize as optimize
import tensorflow as tf
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## PQC

考虑一个作用于 $n$ 个量子比特的变分电路，由 $k$ 层组成，其中每一层包含相邻量子比特之间的参数化 $e^{i\theta X\otimes X}$ 门，
然后是一系列参数化的单量子比特 $Z$ 和 $X$ 旋转。
我们现在展示如何在 TensorCircuit 中实现此类电路，以及如何使用机器学习后端之一轻松高效地计算损失函数和梯度。

一般$n,k$的电路和参数集可以定义如下：:
"""

def qcircuit(n, k, params):
    c = tc.Circuit(n)
    for j in range(k):
        for i in range(n - 1):
            c.exp1(
                i, i + 1, theta=params[j * (3 * n - 1) + i], unitary=tc.gates._xx_matrix
            )
        for i in range(n):
            c.rz(i, theta=params[j * (3 * n - 1) + n - 1 + i])
            c.rx(i, theta=params[j * (3 * n - 1) + 2 * n - 1 + i])
    return c

"""
举个例子，我们取 $n=3, k=2$ ，设置 TensorFlow 作为我们的后端，定义一个能量损失函数来最小化
$$E = \langle X_0 X_1\rangle_\theta + \langle X_1 X_2\rangle_\theta.$$
"""

n = 3
k = 2


def energy(params):
    c = qcircuit(n, k, params)
    e = c.expectation_ps(x=[0, 1]) + c.expectation_ps(x=[1, 2])
    return K.real(e)

"""
## 梯度和即时编译
"""

"""
使用 ML 后端对自动微分的支持，我们现在可以快速计算能量和能量相对于参数的梯度。
"""

energy_val_grad = K.value_and_grad(energy)

"""
这将创建一个函数，给定一组参数作为输入，返回能量和能量梯度。如果只需要梯度，则可以通过 ``K.grad(energy)`` 计算。
虽然我们可以直接在一组参数上运行上述代码，但如果要对能量进行多次评估，则可以通过使用该函数的即时编译版本来节省大量时间。
"""

energy_val_grad_jit = K.jit(energy_val_grad)

"""
使用 ``K.jit``，能量和梯度的初始评估可能需要更长的时间，但随后的评估将明显快于非 jitted 代码。
我们建议始终使用 ``jit``，只要函数是 ``张量输入，张量输出`` 的形式，我们已经努力使电路模拟器的各个方面都与 JIT 兼容。
"""

"""
## 通过 ML 后端进行优化

有了可用的能量函数和梯度，参数的优化就很简单了。下面是一个如何通过随机梯度下降来做到这一点的例子。
"""

learning_rate = 2e-2
opt = K.optimizer(tf.keras.optimizers.SGD(learning_rate))


def grad_descent(params, i):
    val, grad = energy_val_grad_jit(params)
    params = opt.update(grad, params)
    if i % 10 == 0:
        print(f"i={i}, energy={val}")
    return params


params = K.implicit_randn(k * (3 * n - 1))
for i in range(100):
    params = grad_descent(params, i)
# Output:
#   i=0, energy=0.11897378414869308

#   i=10, energy=-0.3692811131477356

#   i=20, energy=-0.7194114923477173

#   i=30, energy=-0.904697597026825

#   i=40, energy=-1.013866662979126

#   i=50, energy=-1.1042678356170654

#   i=60, energy=-1.1998062133789062

#   i=70, energy=-1.308410406112671

#   i=80, energy=-1.4276418685913086

#   i=90, energy=-1.5474387407302856


"""
## 通过 Scipy 界面进行优化

使用机器学习后端进行优化的另一种方法是使用 SciPy。
这可以通过 ``scipy_interface`` API 调用来完成，并允许使用基于梯度（例如 BFGS）和非基于梯度（例如 COBYLA）的优化器，这在 ML 后端是不可用的。
"""

f_scipy = tc.interfaces.scipy_interface(energy, shape=[k * (3 * n - 1)], jit=True)
params = K.implicit_randn(k * (3 * n - 1))
r = optimize.minimize(f_scipy, params, method="L-BFGS-B", jac=True)
r
# Output:
#   /Users/shixin/Cloud/newwork/quantum-information/codebases/tensorcircuit/tensorcircuit/interfaces.py:237: ComplexWarning: Casting complex values to real discards the imaginary part

#     scipy_gs = scipy_gs.astype(np.float64)

#         fun: -2.000000476837158

#    hess_inv: <16x16 LbfgsInvHessProduct with dtype=float64>

#         jac: array([ 2.43186951e-04, -1.50322914e-04,  8.94665718e-05,  1.18807920e-05,

#           2.95639038e-05,  1.19209290e-07, -5.96046448e-08, -2.98023224e-08,

#           0.00000000e+00, -1.19209290e-07,  3.90738450e-07,  9.34305717e-07,

#          -8.22039729e-05,  1.19209290e-07,  0.00000000e+00,  0.00000000e+00])

#     message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'

#        nfev: 60

#         nit: 19

#        njev: 60

#      status: 0

#     success: True

#           x: array([ 2.35625520e+00,  7.85409154e-01,  1.57088576e+00,  2.10625989e-05,

#          -1.57088425e+00, -1.70256902e+00, -5.33743572e-01,  3.11436816e-01,

#           1.26543793e+00,  1.91663337e+00, -1.15901008e-07, -1.76623396e-05,

#          -1.59972887e-04, -8.97072367e-01,  1.79929630e+00, -9.67278961e-01])

"""
上面的第一行指定了要提供给要最小化的函数的参数的形状，这里是能量函数。
``jit=True`` 参数会自动处理能量函数的即时编译。 通过将 ``gradient=False`` 参数提供给``scipy_interface``，同样可以有效地执行无梯度优化。
"""

f_scipy = tc.interfaces.scipy_interface(
    energy, shape=[k * (3 * n - 1)], jit=True, gradient=False
)
params = K.implicit_randn(k * (3 * n - 1))
r = optimize.minimize(f_scipy, params, method="COBYLA")
r
# Output:
#        fun: -1.9999911785125732

#      maxcv: 0.0

#    message: 'Optimization terminated successfully.'

#       nfev: 386

#     status: 1

#    success: True

#          x: array([ 7.87597857e-01, -5.14158452e-01, -1.56560250e+00, -3.15230777e-04,

#           9.91532990e-01,  5.95588091e-01,  1.38523058e+00, -3.59642968e-04,

#          -3.23365306e-01, -4.16465772e-01, -7.32259085e-03,  6.53997758e-05,

#           7.71203778e-01,  2.46256921e+00,  8.78602039e-01, -3.51989842e-01])



================================================
FILE: docs/source/whitepaper/5-density-matrix.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Density Matrix and Mixed State Evolution
"""

"""
## Overview

TensorCircuit provides two methods of simulating noisy, mixed state quantum evolution.  Full density matrix simulation of $n$ qubits is provided by using ``tc.DMCircuit(n)``, and then adding quantum operations -- both unitary gates as well as general quantum operations specified by Kraus operators -- to the circuit.  Relative to pure state simulation of $n$ qubits via ``tc.Circuit``, full density matrix simulation is twice as memory-intensive, and thus the maximum system size simulatable will be half of what can be simulated in the pure state case.  A less memory intensive option is to use the standard ``tc.Circuit(n)`` object and stochastically simulate open system evolution via Monte Carlo trajectory methods.
"""

"""
## Setup
"""

import numpy as np
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## Density Matrix Simulation with ``tc.DMCircuit``
"""

"""
We illustrate this method below, by considering a simple circuit on a single qubit, which takes as input the mixed state corresponding to a probabilistic mixture of the $\vert{0}\rangle$ state and the maximally mixed state
$\rho(\alpha) = \alpha\vert 0\rangle \langle 0\vert + (1-\alpha)I/2.$

This state is then passed through a circuit that applies an $X$ gate, followed by a  quantum operation corresponding to an amplitude damping channel $\mathcal{E}_\gamma$ with parameter $\gamma$. This has Kraus operators
$K_0 = \begin{pmatrix}
1 & 0 \\ 0 & \sqrt{1-\gamma}
\end{pmatrix}, \quad K_1 = \begin{pmatrix}
0 & \sqrt{\gamma} \\ 0 & 0
\end{pmatrix}$
This circuit thus causes the evolution
$\rho(\alpha) \xrightarrow[]{X} X\rho(\alpha)X\xrightarrow[]{\mathcal{E}_\gamma}\sum_{i=0}^1 K_i X\rho(\alpha)X K_i^\dagger$


To simulate this in TensorCircuit, we first create a ``tc.DMCircuit`` (density matrix circuit) object and set the input state using the ``dminputs`` optional argument (note that if a pure state input is provided to ``tc.DMCircuit``, this should be done via the ``inputs`` optional argument).

$\rho(\alpha)$ has matrix form
$\rho(\alpha) = \begin{pmatrix}
\frac{1+\alpha}{2} & \\ & \frac{1-\alpha}{2}
\end{pmatrix},$
and thus (taking $\alpha=0.6$) we initialize the density matrix circuit as follows.

To implement a general quantum operation such as the amplitude damping channel, we use ``general_kraus``, supplied with the corresponding list of Kraus operators.
"""

def rho(alpha):
    return np.array([[(1 + alpha) / 2, 0], [0, (1 - alpha) / 2]])


input_state = rho(0.6)
dmc = tc.DMCircuit(1, dminputs=input_state)

dmc.x(0)


def amp_damp_kraus(gamma):
    K0 = np.array([[1, 0], [0, np.sqrt(1 - gamma)]])
    K1 = np.array([[0, np.sqrt(gamma)], [0, 0]])
    return K0, K1


K0, K1 = amp_damp_kraus(0.3)
dmc.general_kraus([K0, K1], 0)  # apply channel with Kraus operators [K0,K1] to qubit 0

# get the output density matrix
dmc.state()
# Output:
#   <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#   array([[0.44+0.j, 0.  +0.j],

#          [0.  +0.j, 0.56+0.j]], dtype=complex64)>

# evaluate the expectation as a circuit object
print(dmc.expectation_ps(z=[0]), dmc.measure(0))
# Output:
#   tf.Tensor((-0.11999999+0j), shape=(), dtype=complex64) (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, -1.0)


"""
In the above example, we input the Kraus operators for the amplitude damping channel manually, in order to illustrate the general approach to implementing quantum channels. In fact, TensorCircuit includes built-in methods for returning the Kraus operators for a number of common channels, including the amplitude damping, depolarizing, phase damping, and reset channels.
"""

# a set of built-in quantum channels

for k in dir(tc.channels):
    if k.endswith("channel"):
        print(k)
# Output:
#   amplitudedampingchannel

#   depolarizingchannel

#   phasedampingchannel

#   resetchannel


dmc = tc.DMCircuit(2)
dmc.h(0)
gamma = 0.2
K0, K1 = tc.channels.phasedampingchannel(gamma)
dmc.general_kraus([K0, K1], 0)
dmc.state()
# Output:
#   <tf.Tensor: shape=(4, 4), dtype=complex64, numpy=

#   array([[0.49999997+0.j, 0.        +0.j, 0.4472136 +0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j],

#          [0.4472136 +0.j, 0.        +0.j, 0.49999994+0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j]],

#         dtype=complex64)>

# or we can directly use the following API for shorthand

dmc = tc.DMCircuit(2)
dmc.h(0)
gamma = 0.2
dmc.phasedamping(0, gamma=0.2)
dmc.state()
# Output:
#   <tf.Tensor: shape=(4, 4), dtype=complex64, numpy=

#   array([[0.49999997+0.j, 0.        +0.j, 0.4472136 +0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j],

#          [0.4472136 +0.j, 0.        +0.j, 0.49999994+0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j]],

#         dtype=complex64)>

"""
### AD and JIT Compatibility

``tc.DMCircuit``, like ``tc.Circuit`` is also compatible with ML paradigms such as AD, jit, and vmap. See the example below.
"""

n = 3
nbatch = 2


def loss(params, noisep):
    c = tc.DMCircuit(n)
    for i in range(n):
        c.rx(i, theta=params[i])
    for i in range(n):
        c.depolarizing(i, px=noisep, py=noisep, pz=noisep)
    return K.real(K.sum([c.expectation_ps(z=[i]) for i in range(n)]))


loss_vvg = K.jit(
    K.vectorized_value_and_grad(loss, argnums=(0, 1), vectorized_argnums=(0))
)

vs, (gparams, gnoisep) = loss_vvg(0.1 * K.ones([nbatch, n]), 0.1 * K.ones([]))

vs.shape, gparams.shape, gnoisep.shape
# Output:
#   (TensorShape([2]), TensorShape([2, 3]), TensorShape([]))

"""
Note how the noise parameter can also be differentiated and jitted!
"""

"""
## Monte Carlo Noise Simulation with ``tc.Circuit``
"""

"""
For pure state inputs, Monte Carlo methods can be used to sample noisy quantum evolution using ``tc.Circuit`` instead of ``tc.DMCircuit`` where the mixed state is effectively simulated with an ensemble of pure states.
 
As for density matrix simulation, quantum channels $\mathcal{E}$ can be added to a circuit object by providing a list of their associated Kraus operators $\{K_i\}$.  The API is the same as for the full density matrix simulation.
"""

input_state = np.array([1, 1] / np.sqrt(2))
c = tc.Circuit(1, inputs=input_state)
c.general_kraus(tc.channels.phasedampingchannel(0.5), 0)
c.state()
# Output:
#   <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([0.+0.j, 1.+0.j], dtype=complex64)>

"""
In this framework though, the output of a channel acting on $\vert{\psi}\rangle$ , i.e.
$\mathcal{E} ( \vert{\psi}\rangle\langle{\psi}\vert) = \sum_i K_i \vert{\psi}\rangle\langle{\psi}\vert K_i^ \dagger$
is viewed as an ensemble of states $\frac{K_i\vert{\psi}\rangle}{\sqrt{\langle{\psi}\vert K_i^\dagger K_i \vert{\psi}\rangle}}$ that each occur with probability $p_i = \langle{\psi}\vert K_i^\dagger K_i \vert{\psi}\rangle$.  Thus, the code above stochastically produces the output of a single qubit initialized in state $\vert{\psi}\rangle=\frac{\vert{0}\rangle+\vert{1}\rangle}{\sqrt{2}}$ being passed through a phase damping channel with parameter $\gamma=0.5$.  

The Monte Carlo simulation of channels where the Kraus operators are all unitary matrices (up to a constant factor) can be handled with additional efficiency by using ``unitary_kraus`` instead of ``general_kraus``.
"""

px, py, pz = 0.1, 0.2, 0.3
c.unitary_kraus(tc.channels.depolarizingchannel(px, py, pz), 0)
# Output:
#   <tf.Tensor: shape=(), dtype=int32, numpy=3>

"""
Note the int tensor returned above indicates in this trajectory, which operator is applied on the circuit.
"""

"""
### Externalizing the Randomness

The ``general_kraus`` and ``unitary\_kraus`` examples above both handle randomness generation from inside the respective methods. That is, when the list $[K_0, K_1, \ldots, K_{m-1}]$ of Kraus operators is supplied to ``general_kraus`` or ``unitary_kraus``, the method  partitions the interval $[0,1]$ into $m$ contiguous intervals $[0,1] = I_0 \cup I_1 \cup \ldots I_{m-1}$ where the length of $I_i$ is equal to the relative probability of obtaining outcome $i$. Then a uniformly random variable $x$ in $[0,1]$ is generated from within the method, and outcome $i$ selected based on which interval $x$ lies in. 

In TensorCircuit, we have full backend agnostic infrastructure for random number generation and management. However, the interplay between jit, random number, and backend switch is often subtle if we rely on the random number generation inside these methods. See [advance.html\#randoms-jit-backend-agnostic-and-their-interplay](../advance.html#randoms-jit-backend-agnostic-and-their-interplay) for details.

In some situations, it may be preferable to first generate the random variable from outside the method, and then pass the value generated into ``general_kraus`` or ``unitary_kraus``.  This can be done via the optional ``status`` argument:
"""

px, py, pz = 0.1, 0.2, 0.3
x = 0.5
print(c.unitary_kraus(tc.channels.depolarizingchannel(px, py, pz), 0, status=x))
x = 0.8
print(c.unitary_kraus(tc.channels.depolarizingchannel(px, py, pz), 0, status=x))
# Output:
#   tf.Tensor(1, shape=(), dtype=int32)

#   tf.Tensor(3, shape=(), dtype=int32)


"""
This is useful, for instance, when one wishes to use ``vmap`` to batch compute multiple runs of a Monte Carlo simulation. This is illustrated in the example below, where ``vmap`` is used to compute 10 runs of the simulation in parallel.
"""

def f(x):
    c = tc.Circuit(1)
    c.h(0)
    c.unitary_kraus(tc.channels.depolarizingchannel(0.1, 0.2, 0.3), 0, status=x)
    return c.expectation_ps(x=[0])


f_vmap = K.vmap(f, vectorized_argnums=0)
X = K.implicit_randn(10)
f_vmap(X)
# Output:
#   <tf.Tensor: shape=(10,), dtype=complex64, numpy=

#   array([ 0.99999994+0.j,  0.99999994+0.j,  0.99999994+0.j, -0.99999994+0.j,

#           0.99999994+0.j,  0.99999994+0.j,  0.99999994+0.j,  0.99999994+0.j,

#          -0.99999994+0.j,  0.99999994+0.j], dtype=complex64)>



================================================
FILE: docs/source/whitepaper/5-density-matrix_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 密度矩阵和混态演化
"""

"""
## 概述
TensorCircuit 提供了两种含噪声、混态量子演化的方法。
$n$ 量子比特的全密度矩阵模拟是通过使用 ``tc.DMCircuit(n)`` 提供的，然后将量子操作——包括幺正门以及由 Kraus 算子指定的一般量子操作——添加到电路中。
相对于通过 ``tc.Circuit`` 对 $n$ 个量子比特进行纯态模拟，全密度矩阵模拟会占据两倍内存，因此可模拟的最大系统大小将是纯态情况下可以模拟的一半。
内存需求较小的选项是使用标准的 ``tc.Circuit(n)`` 对象并通过蒙特卡罗轨迹方法随机模拟开放系统演化。
"""

"""
## 设置
"""

import numpy as np
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

"""
## 使用 ``tc.DMCircuit`` 进行密度矩阵模拟
"""

"""
我们在下面通过考虑单个量子比特上的简单电路来说明这种方法，该电路将对应于 $\vert{0}\rangle$ 状态和最大混合状态的概率混合的混合状态作为输入
$\rho(\alpha) = \alpha\vert 0\rangle \langle 0\vert + (1-\alpha)I/2。$

然后这个状态通过一个应用 $X$ 门的电路，然后是对应于带有参数 $\gamma$ 的振幅阻尼通道 $\mathcal{E}_\gamma$ 的量子操作。
这有 Kraus 运算符$K_0 = \begin{pmatrix}
1 & 0 \\ 0 & \sqrt{1-\gamma}
\end{pmatrix}, \quad K_1 = \begin{pmatrix}
0 & \sqrt{\gamma} \\ 0 & 0
\end{pmatrix}$
因此，该电路导致演化
$\rho(\alpha) \xrightarrow[]{X} X\rho(\alpha)X\xrightarrow[]{\mathcal{E}_\gamma}\sum_{i=0}^1 K_i X\rho(\alpha)X K_i^\dagger$


为了在 TensorCircuit 中模拟这一点，我们首先创建一个 ``tc.DMCircuit``（密度矩阵电路）对象并使用 ``dminputs`` 可选参数设置输入状态
（请注意，如果将纯状态输入提供给 `` tc.DMCircuit ，这应该通过 ``inputs`` 可选参数来完成)。

$\rho(\alpha)$ 有矩阵形式
$\rho(\alpha) = \begin{pmatrix}
\frac{1+\alpha}{2} & \\ & \frac{1-\alpha}{2}
\end{pmatrix},$
因此（取 $\alpha=0.6$）我们如下初始化密度矩阵电路。

为了实现诸如振幅阻尼通道之类的通用量子操作，我们使用了 ``general_kraus``，并提供了相应的 Kraus 运算符列表。
"""

def rho(alpha):
    return np.array([[(1 + alpha) / 2, 0], [0, (1 - alpha) / 2]])


input_state = rho(0.6)
dmc = tc.DMCircuit(1, dminputs=input_state)

dmc.x(0)


def amp_damp_kraus(gamma):
    K0 = np.array([[1, 0], [0, np.sqrt(1 - gamma)]])
    K1 = np.array([[0, np.sqrt(gamma)], [0, 0]])
    return K0, K1


K0, K1 = amp_damp_kraus(0.3)
dmc.general_kraus([K0, K1], 0)  # 将具有 Kraus 算子 [K0,K1] 的通道应用于 qubit 0

# 得到输出密度矩阵
dmc.state()
# Output:
#   <tf.Tensor: shape=(2, 2), dtype=complex64, numpy=

#   array([[0.44+0.j, 0.  +0.j],

#          [0.  +0.j, 0.56+0.j]], dtype=complex64)>

# 将期望作为电路对象评估
print(dmc.expectation_ps(z=[0]), dmc.measure(0))
# Output:
#   tf.Tensor((-0.11999999+0j), shape=(), dtype=complex64) (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, -1.0)


"""
在上面的例子中，我们手动输入振幅阻尼通道的 Kraus 算子，以说明实现常见量子通道的一般方法。
事实上，TensorCircuit 包含用于返回许多公共通道的 Kraus 算子的内置方法，包括振幅阻尼、去极化、相位阻尼和复位通道。
"""

# 一组内置量子通道

for k in dir(tc.channels):
    if k.endswith("channel"):
        print(k)
# Output:
#   amplitudedampingchannel

#   depolarizingchannel

#   phasedampingchannel

#   resetchannel


dmc = tc.DMCircuit(2)
dmc.h(0)
gamma = 0.2
K0, K1 = tc.channels.phasedampingchannel(gamma)
dmc.general_kraus([K0, K1], 0)
dmc.state()
# Output:
#   <tf.Tensor: shape=(4, 4), dtype=complex64, numpy=

#   array([[0.49999997+0.j, 0.        +0.j, 0.4472136 +0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j],

#          [0.4472136 +0.j, 0.        +0.j, 0.49999994+0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j]],

#         dtype=complex64)>

# 或者我们可以直接使用下面的API进行速记

dmc = tc.DMCircuit(2)
dmc.h(0)
gamma = 0.2
dmc.phasedamping(0, gamma=0.2)
dmc.state()
# Output:
#   <tf.Tensor: shape=(4, 4), dtype=complex64, numpy=

#   array([[0.49999997+0.j, 0.        +0.j, 0.4472136 +0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j],

#          [0.4472136 +0.j, 0.        +0.j, 0.49999994+0.j, 0.        +0.j],

#          [0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j]],

#         dtype=complex64)>

"""
### 自动微分、即时编译兼容性

``tc.DMCircuit`` 和 ``tc.Circuit`` 一样也兼容 ML 范式，例如 自动微分、即时编译和 vmap。请参见下面的示例。
"""

n = 3
nbatch = 2


def loss(params, noisep):
    c = tc.DMCircuit(n)
    for i in range(n):
        c.rx(i, theta=params[i])
    for i in range(n):
        c.depolarizing(i, px=noisep, py=noisep, pz=noisep)
    return K.real(K.sum([c.expectation_ps(z=[i]) for i in range(n)]))


loss_vvg = K.jit(
    K.vectorized_value_and_grad(loss, argnums=(0, 1), vectorized_argnums=(0))
)

vs, (gparams, gnoisep) = loss_vvg(0.1 * K.ones([nbatch, n]), 0.1 * K.ones([]))

vs.shape, gparams.shape, gnoisep.shape
# Output:
#   (TensorShape([2]), TensorShape([2, 3]), TensorShape([]))

"""
注意噪声参数也可以被微分和即时编译！
"""

"""
## 使用 ``tc.Circuit`` 进行蒙特卡罗噪声模拟
"""

"""
对于纯态输入，蒙特卡洛方法可用于使用 ``tc.Circuit`` 而不是 ``tc.DMCircuit`` 对嘈杂的量子演化进行采样，其中混合状态是用纯状态的集合有效模拟的。

至于密度矩阵模拟，可以通过提供相关的 Kraus 算子 $\{K_i\}$ 的列表将量子通道 $\mathcal{E}$ 添加到电路对象中。API 与全密度矩阵模拟相同。
"""

input_state = np.array([1, 1] / np.sqrt(2))
c = tc.Circuit(1, inputs=input_state)
c.general_kraus(tc.channels.phasedampingchannel(0.5), 0)
c.state()
# Output:
#   <tf.Tensor: shape=(2,), dtype=complex64, numpy=array([0.+0.j, 1.+0.j], dtype=complex64)>

"""
不过，在这个框架中，作用于 $\vert{\psi}\rangle$ 的通道的输出，即
$\mathcal{E} ( \vert{\psi}\rangle\langle{\psi}\vert) = \sum_i K_i \vert{\psi}\rangle\langle{\psi}\vert K_i^ \dagger$
被视为状态的集合 $\frac{K_i\vert{\psi}\rangle}{\sqrt{\langle{\psi}\vert K_i^\dagger K_i \vert{\psi}\rangle}}$
每个发生的概率为 $p_i = \langle{\psi}\vert K_i^\dagger K_i \vert{\psi}\rangle$.
因此，上面的代码随机产生在状态 $\vert{\psi}\rangle=\frac{\vert{0}\rangle+\vert{1}\rangle}{\sqrt{2}  }$
中初始化的单个量子比特的输出通过参数 $\gamma=0.5$ 的相位阻尼通道。
通过使用 ``unitary_kraus`` 而不是 ``general_kraus``，可以更有效地处理 Kraus 算子都是幺正矩阵(可相差一个常数因子)的通道的蒙特卡罗模拟。
"""

px, py, pz = 0.1, 0.2, 0.3
c.unitary_kraus(tc.channels.depolarizingchannel(px, py, pz), 0)
# Output:
#   <tf.Tensor: shape=(), dtype=int32, numpy=3>

"""
请注意，上面返回的 int 张量指示在此轨迹中，在电路上应用了哪个算子。
"""

"""
### 外化随机性

上面的 ``general_kraus`` 和 ``unitary\_kraus`` 示例都从各自的方法内部处理随机性生成。
也就是说，当将 Kraus 运算符的列表 $[K_0, K_1, \ldots, K_{m-1}]$ 提供给 ``general_kraus``或`` unitary_kraus``时，
该方法将区间 $[0, 1]$ 到 $m$ 连续区间 $[0,1] = I_0 \cup I_1 \cup \ldots I_{m-1}$ 其中$I_i$的长度等于获得结果$i$的相对概率。
然后从方法内部生成$[0,1]$中的均匀随机变量$x$，并根据$x$所在的区间选择结果$i$。

在 TensorCircuit 中，我们拥有用于随机数生成和管理的完整的后端不可知基础设施。
但是，如果我们依赖这些方法中的随机数生成，jit、随机数和后端切换之间的相互作用通常是微妙的。
有关详细信息，请参阅 [advance.html\#randoms-jit-backend-agnostic-and-their-interplay](../advance.html#randoms-jit-backend-agnostic-and-their-interplay)。

在某些情况下，最好先从方法外部生成随机变量，然后将生成的值传递给 ``general_kraus`` 或 ``unitary_kraus`` 这可以通过可选的 ``status`` 参数来完成：
"""

px, py, pz = 0.1, 0.2, 0.3
x = 0.5
print(c.unitary_kraus(tc.channels.depolarizingchannel(px, py, pz), 0, status=x))
x = 0.8
print(c.unitary_kraus(tc.channels.depolarizingchannel(px, py, pz), 0, status=x))
# Output:
#   tf.Tensor(1, shape=(), dtype=int32)

#   tf.Tensor(3, shape=(), dtype=int32)


"""
这很有用，例如，当希望使用 ``vmap`` 批量计算蒙特卡罗模拟的多次运行时。这在下面的示例中进行了说明，其中 ``vmap`` 用于并行计算 10 次模拟运行。
"""

def f(x):
    c = tc.Circuit(1)
    c.h(0)
    c.unitary_kraus(tc.channels.depolarizingchannel(0.1, 0.2, 0.3), 0, status=x)
    return c.expectation_ps(x=[0])


f_vmap = K.vmap(f, vectorized_argnums=0)
X = K.implicit_randn(10)
f_vmap(X)
# Output:
#   <tf.Tensor: shape=(10,), dtype=complex64, numpy=

#   array([ 0.99999994+0.j,  0.99999994+0.j,  0.99999994+0.j, -0.99999994+0.j,

#           0.99999994+0.j,  0.99999994+0.j,  0.99999994+0.j,  0.99999994+0.j,

#          -0.99999994+0.j,  0.99999994+0.j], dtype=complex64)>



================================================
FILE: docs/source/whitepaper/6-1-conditional-measurements-post-selection.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Different Types of Measurement API
"""

"""
## Overview

TensorCircuit allows for two kinds of operations to be performed that are related to the outcomes of measurements.  These are (i) conditional measurements, the outcomes of which can be used to control downstream conditional quantum gates, and (ii) post-selection, which allows the user to select the post-measurement state corresponding to a particular measurement outcome.
"""

"""
## Setup
"""

import tensorcircuit as tc
import numpy as np

K = tc.set_backend("tensorflow")

"""
## Conditional Measurements


The `cond_measure` command is used to simulate the process of performing a Z measurement on a qubit,  generating a measurement outcome with probability given by the Born rule, and then collapsing the wavefunction in accordance with the measured outcome.  The classical measurement outcome obtained can then act as a control for a subsequent quantum operation via the `conditional_gate` API and can be used, for instance, to implement the canonical teleportation circuit.  
"""

# quantum teleportation of state |psi> = a|0> + sqrt(1-a^2)|1>
a = 0.3
input_state = np.kron(np.array([a, np.sqrt(1 - a**2)]), np.array([1, 0, 0, 0]))

c = tc.Circuit(3, inputs=input_state)
c.h(2)
c.cnot(2, 1)
c.cnot(0, 1)
c.h(0)

# mid-circuit measurements
z = c.cond_measure(0)
x = c.cond_measure(1)

# if x = 0 apply I, if x = 1 apply X (to qubit 2)
c.conditional_gate(x, [tc.gates.i(), tc.gates.x()], 2)

# if z = 0 apply I, if z = 1 apply Z (to qubit 2)
c.conditional_gate(z, [tc.gates.i(), tc.gates.z()], 2)

# we indeed recover the state at the third qubit.

c.measure(2, with_prob=True), a**2, 1 - a**2
# Output:
#   ((<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,

#     <tf.Tensor: shape=(), dtype=float32, numpy=0.90999997>),

#    0.09,

#    0.91)

"""
The teleportation circuit is shown below.
![](../statics/teleportation.png)
"""

"""
## Post Selection

Post-selection is enabled in TensorCircuit via the ``post_select`` method.  This allows the user to select the post-$Z$-measurement state of a qubit via the ``keep`` argument. Unlike ``cond_measure``, the state returned by ``post_select`` is collapsed but not normalized.
"""

c = tc.Circuit(2, inputs=np.array([1, 0, 0, 1] / np.sqrt(2)))
c.post_select(0, keep=1)  # measure qubit 0, post-select on outcome 1
c.state()
# Output:
#   <tf.Tensor: shape=(4,), dtype=complex64, numpy=

#   array([0.        +0.j, 0.        +0.j, 0.        +0.j, 0.70710677+0.j],

#         dtype=complex64)>

"""
This example initialize a $2$-qubit maximally entangled state $\vert{\psi}\rangle = \frac{\vert{00}\rangle+\vert{11}\rangle}{\sqrt{2}}$. The first qubit ($q_0$) is then measured in the $Z$-basis, and the unnormalized state  $\vert{11}\rangle/\sqrt{2}$ corresponding to measurement outcome $1$ is post-selected.

This post-selection scheme with unnormalized states is fast and can, for instance, be used to explore various quantum algorithms and nontrivial quantum physics such as measurement-induced entanglement phase transitions.
"""

"""
## Plain Measurements
"""

c = tc.Circuit(3)
c.H(0)
print(c.measure(0, with_prob=True))
print(c.measure(0, 1, with_prob=True))
# Output:
#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)

#   (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)


"""
Note how the plain measure API is virtual in the sense that the state is not collapsed after measurement.
"""

for _ in range(5):
    print(c.measure(0, with_prob=True))
# Output:
#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)


"""
Let's jit `measure`! (with careful random number manipulation)
"""

n = 3
key = K.get_random_state(42)


def measure_on(param, index, key):
    K.set_random_state(key)
    c = tc.Circuit(n)
    for i in range(n):
        c.rx(i, theta=param[i])
    return c.measure(*index)[0]


measure_on_jit = K.jit(measure_on, static_argnums=1)

key1 = key
for _ in range(30):
    key1, key2 = K.random_split(key1)
    print(measure_on_jit(K.ones([n]), [0, 1, 2], key2))
# Output:
#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 1.], shape=(3,), dtype=float32)

#   tf.Tensor([1. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([1. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 1.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([1. 0. 1.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)


"""
For a summary of the differences between plain `measure` and the two types of measurement we mentioned here, please see [FAQ documentation](../faq.html#how-to-understand-the-difference-between-different-measurement-methods-for-circuit).
"""



================================================
FILE: docs/source/whitepaper/6-1-conditional-measurements-post-selection_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 不同类型的测量 API
"""

"""
## 概述

TensorCircuit 允许执行与测量结果相关的两种操作。
这些是 (i) 条件测量，其结果可用于控制下游条件量子门，以及 (ii) 后选择，它允许用户选择与特定测量结果相对应的测量后状态。
"""

"""
## 设置
"""

import tensorcircuit as tc
import numpy as np

K = tc.set_backend("tensorflow")

"""
## 条件测量


`cond_measure` 命令用于模拟对量子比特执行 Z 测量的过程，以 Born 规则给出的概率生成测量结果，然后根据测量结果坍缩波函数。
获得的经典测量结果可以通过 `conditional_gate` API 作为后续量子操作的控制，并且可以用于例如实现规范的隐形传输电路。
"""

# 状态的量子隐形传态|psi> = a|0> + sqrt(1-a^2)|1>
a = 0.3
input_state = np.kron(np.array([a, np.sqrt(1 - a**2)]), np.array([1, 0, 0, 0]))

c = tc.Circuit(3, inputs=input_state)
c.h(2)
c.cnot(2, 1)
c.cnot(0, 1)
c.h(0)

# 中间电路测量
z = c.cond_measure(0)
x = c.cond_measure(1)

# 如果 x = 0 应用 I，如果 x = 1 应用 X（到 qubit 2）
c.conditional_gate(x, [tc.gates.i(), tc.gates.x()], 2)

# 如果 z = 0 应用 I，如果 z = 1 应用 Z（到 qubit 2）
c.conditional_gate(z, [tc.gates.i(), tc.gates.z()], 2)

# 我们确实在第三个量子位恢复了状态。

c.measure(2, with_prob=True), a**2, 1 - a**2
# Output:
#   ((<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,

#     <tf.Tensor: shape=(), dtype=float32, numpy=0.90999997>),

#    0.09,

#    0.91)

"""
传送电路如下图所示。
![](../statics/teleportation.png)
"""

"""
## 后选择

在 TensorCircuit 中通过 post_select 方法启用后选择。这允许用户通过 ``keep`` 参数选择量子位的 $Z$ 测量后状态。
与 ``cond_measure`` 不同，``post_select`` 返回的状态是折叠的，但不是归一化的。
"""

c = tc.Circuit(2, inputs=np.array([1, 0, 0, 1] / np.sqrt(2)))
c.post_select(0, keep=1)  # 测量 qubit 0，对结果 1 进行后选择
c.state()
# Output:
#   <tf.Tensor: shape=(4,), dtype=complex64, numpy=

#   array([0.        +0.j, 0.        +0.j, 0.        +0.j, 0.70710677+0.j],

#         dtype=complex64)>

"""
这个例子初始化了一个 $2$-qubit 最大纠缠态 $\vert{\psi}\rangle = \frac{\vert{00}\rangle+\vert{11}\rangle}{\sqrt{2}}$。
然后在 $Z$-basis 中测量第一个量子比特 ($q_0$)，并后选择对应于测量结果 $1$ 的非归一化状态 $\vert{11}\rangle/\sqrt{2}$。
这种具有非归一化状态的后选择方案速度很快，例如，可用于探索各种量子算法和非平凡的量子物理学，例如测量引起的纠缠相变。
"""

"""
## 普通测量
"""

c = tc.Circuit(3)
c.H(0)
print(c.measure(0, with_prob=True))
print(c.measure(0, 1, with_prob=True))
# Output:
#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)

#   (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)


"""
请注意，在测量后状态不会坍缩的意义上，普通测量 API 是虚拟的。
"""

for _ in range(5):
    print(c.measure(0, with_prob=True))
# Output:
#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)

#   (<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49999997>)


"""
让我们即时编译 `measure`！（仔细的随机数操作）。
"""

n = 3
key = K.get_random_state(42)


def measure_on(param, index, key):
    K.set_random_state(key)
    c = tc.Circuit(n)
    for i in range(n):
        c.rx(i, theta=param[i])
    return c.measure(*index)[0]


measure_on_jit = K.jit(measure_on, static_argnums=1)

key1 = key
for _ in range(30):
    key1, key2 = K.random_split(key1)
    print(measure_on_jit(K.ones([n]), [0, 1, 2], key2))
# Output:
#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 1.], shape=(3,), dtype=float32)

#   tf.Tensor([1. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([1. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 1.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([1. 0. 1.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)

#   tf.Tensor([0. 1. 0.], shape=(3,), dtype=float32)


"""
有关普通 `measure` 和我们在这里提到的两种测量类型之间差异的摘要，请参阅[FAQ 文档](../faq.html#how-to-understand-the-difference-between-different-measurement-methods-for-circuit).
"""



================================================
FILE: docs/source/whitepaper/6-3-vmap.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Utilizing vmap in Quantum Circuit Simulations
"""

"""
## Overview

We introduce vmap, the advanced feature of the modern machine learning library, to quantum circuit simulations.
By vmapping different ingredients of quantum circuit simulation, we can implement variational quantum algorithms with high efficiency.

It is worth noting that in the following use cases, vmap is supported together with jit and AD which renders highly efficient differentiable simulation.

The ingredients that support vmap paradigm are shown in the following figure.
![vmap ingredients](../statics/vmap_ingredients.png)

We have two different types of APIs for vmap, the first one is ``vmap`` while the second one is ``vectorized_value_and_grad``, aka, ``vvag``.
The latter can also return the gradient information over a batch of the different circuits.
"""

"""
If batch evaluation of gradients as well as function values is required, then this can be done via ``vectorized_value_and_grad``. In the simplest case, consider a function 
$f(x,y)$ where $x\in R^p,y\in R^q$ are both vectors, and one wishes to evaluate both $f(x,y)$ and $\sum_x\nabla_y f(x,y) = \sum_x\left ( \frac{\partial f(x,y_1)}{\partial y_1},\ldots, \frac{\partial f(x,y_q)}{\partial y_q}\right )^\top$ over a batch $x_1, x_2,\ldots, x_k$ of inputs $x$. This is achieved by creating a new, vectorized value-and-gradient function :
"""

%%latex
\begin{equation}
f_{vvg}\left( \begin{pmatrix} \leftarrow x_1 \rightarrow\\ \vdots \\ \leftarrow x_k \rightarrow\end{pmatrix}, y \right) =
\begin{pmatrix} \begin{pmatrix}f(x_1, y) \\ \vdots \\
f(x_k,y)\end{pmatrix},\sum_{i=1}^k \nabla_y f(x_i,y) \end{pmatrix}
\end{equation}
# Output:
#   <IPython.core.display.Latex object>

"""
which takes as zeroth argument the batched inputs expressed as a $k\times p$ tensor, and as first argument the variables we wish to differentiate with respect to. The outputs are a vector of function values evaluated at all points $(x_i,y)$, and the gradient averaged over all those points.
"""

"""
## Setup
"""

import numpy as np
import tensorcircuit as tc

tc.set_backend("tensorflow")
print(tc.__version__)

nwires = 5
nlayers = 2
batch = 6
# Output:
#   0.0.220509


"""
## vmap the Input States

Use case: batch processing of input states in quantum machine learning task.

For applications of batched input state processing, please see [MNIST QML tutorial](../tutorials/mnist_qml.ipynb).
"""

"""
### Minimal Example
"""

def f(inputs, weights):
    c = tc.Circuit(nwires, inputs=inputs)
    c = tc.templates.blocks.example_block(c, weights, nlayers=nlayers)
    loss = c.expectation([tc.gates.z(), [2]])
    loss = tc.backend.real(loss)
    return loss


f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=1, vectorized_argnums=0))
f_vg(tc.backend.ones([batch, 2**nwires]), tc.backend.ones([2 * nlayers, nwires]))
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([10.88678, 10.88678, 10.88678, 10.88678, 10.88678, 10.88678],

#          dtype=float32)>,

#    <tf.Tensor: shape=(4, 5), dtype=complex64, numpy=

#    array([[ 0.0000000e+00+1.3064140e+02j, -1.1444092e-05+1.3064142e+02j,

#             0.0000000e+00+1.3064140e+02j,  0.0000000e+00+1.3064139e+02j,

#             0.0000000e+00+0.0000000e+00j],

#           [-1.9073486e-06-5.1765751e-06j, -5.1105431e+01-5.7347143e-07j,

#            -8.1339760e+01-6.6063179e+01j, -5.1105446e+01+3.3477118e-06j,

#            -7.6293945e-06+1.5500746e-07j],

#           [ 0.0000000e+00+8.4607742e+01j, -1.3292285e+02+1.1209973e+02j,

#            -1.3292284e+02+1.1209971e+02j,  1.5258789e-05+8.4607750e+01j,

#             0.0000000e+00+0.0000000e+00j],

#           [ 1.9073486e-06+5.9908474e+01j, -1.5258789e-05-1.9285599e+01j,

#            -8.1339752e+01+3.8049275e-06j,  3.8146973e-06-1.9285591e+01j,

#            -9.5367432e-06+5.9908482e+01j]], dtype=complex64)>)

"""
## vmap the Circuit Weights

Use case: batched VQE, where different random initialization parameters are optimized simultaneously.

For application on batched VQE, please refer [TFIM VQE tutorial](../tutorials/tfim_vqe.ipynb).
"""

"""
### Minimal Example
"""

def f(weights):
    c = tc.Circuit(nwires)
    c = tc.templates.blocks.example_block(c, weights, nlayers=nlayers)
    loss = c.expectation([tc.gates.z(), [2]])
    loss = tc.backend.real(loss)
    return loss


f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=0, vectorized_argnums=0))
f_vg(tc.backend.ones([batch, 2 * nlayers, nwires]))
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([-2.9802322e-08, -2.9802322e-08, -2.9802322e-08, -2.9802322e-08,

#           -2.9802322e-08, -2.9802322e-08], dtype=float32)>,

#    <tf.Tensor: shape=(6, 4, 5), dtype=complex64, numpy=

#    array([[[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]]], dtype=complex64)>)

"""
## vmap the Quantum Noise

Use case: parallel Monte Carlo noise simulation.

For applications that combine vmapped Monte Carlo noise simulation and quantum machine learning task, please see [noisy QML script](https://github.com/tencent-quantum-lab/tensorcircuit/blob/master/examples/noisy_qml.py).
"""

"""
### Minimal Example
"""

def f(weights, status):
    c = tc.Circuit(nwires)
    c = tc.templates.blocks.example_block(c, weights, nlayers=nlayers)
    for i in range(nwires):
        c.depolarizing(i, px=0.2, py=0.2, pz=0.2, status=status[i])
    loss = c.expectation([tc.gates.x(), [2]])
    loss = tc.backend.real(loss)
    return loss


f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=0, vectorized_argnums=1))


def g(weights):
    status = tc.backend.implicit_randu(shape=[batch, nwires])
    return f_vg(weights, status)


g(tc.backend.ones([2 * nlayers, nwires]))
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([ 0.34873545, -0.34873545, -0.34873545, -0.34873545, -0.34873545,

#            0.34873545], dtype=float32)>,

#    <tf.Tensor: shape=(4, 5), dtype=complex64, numpy=

#    array([[-8.8614023e-01-1.7026657e-08j,  5.7763958e-01+2.3834804e-01j,

#             5.7763910e-01+2.3834780e-01j, -8.8614047e-01+1.2538894e-07j,

#             0.0000000e+00+0.0000000e+00j],

#           [ 0.0000000e+00+6.9313288e-01j,  3.6122650e-01-1.1496974e-02j,

#            -5.2079970e-01-1.7869800e-01j,  3.6122644e-01-1.1496985e-02j,

#            -2.9802322e-08+6.9313288e-01j],

#           [-5.9604645e-08-1.0189922e+00j, -3.0850098e-01-1.4861794e-07j,

#            -3.0850050e-01-2.2304604e-08j,  5.9604645e-08-1.0189921e+00j,

#             0.0000000e+00+0.0000000e+00j],

#           [ 0.0000000e+00+3.1868588e-02j, -8.9406967e-08+2.5656950e-01j,

#            -2.9802322e-08-1.9999983e+00j, -2.9802322e-08+2.5656945e-01j,

#            -1.1920929e-07+3.1868652e-02j]], dtype=complex64)>)

"""
## vmap the Circuit Structure

Use case: differentiable quantum architecture search (DQAS). 

For more detail on DQAS application, see [DQAS tutorial](../tutorials/dqas.ipynb).
"""

"""
### Minimal Example
"""

eye = tc.gates.i().tensor
x = tc.gates.x().tensor
y = tc.gates.y().tensor
z = tc.gates.z().tensor


def f(params, structures):
    c = tc.Circuit(nwires)
    for i in range(nwires):
        c.H(i)
    for j in range(nlayers):
        for i in range(nwires - 1):
            c.cz(i, i + 1)
        for i in range(nwires):
            c.unitary(
                i,
                unitary=structures[i, j, 0]
                * (
                    tc.backend.cos(params[i, j, 0]) * eye
                    + tc.backend.sin(params[i, j, 0]) * x
                )
                + structures[i, j, 1]
                * (
                    tc.backend.cos(params[i, j, 1]) * eye
                    + tc.backend.sin(params[i, j, 1]) * y
                )
                + structures[i, j, 2]
                * (
                    tc.backend.cos(params[i, j, 2]) * eye
                    + tc.backend.sin(params[i, j, 2]) * z
                ),
            )
    loss = c.expectation([tc.gates.z(), (2,)])
    return tc.backend.real(loss)


structures = tc.backend.ones([batch, nwires, nlayers, 3])
params = tc.backend.ones([nwires, nlayers, 3])
f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=0, vectorized_argnums=1))
f_vg(params, structures)
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([2.4917054e+08, 2.4917054e+08, 2.4917054e+08, 2.4917054e+08,

#           2.4917054e+08, 2.4917054e+08], dtype=float32)>,

#    <tf.Tensor: shape=(5, 2, 3), dtype=complex64, numpy=

#    array([[[-4.8252989e+08+2.3603376e+07j, -6.4132224e+08+1.1064736e+08j,

#             -4.5701562e+08-7.4987272e+07j],

#            [-5.4175347e+08+5.2096408e+07j, -5.5254317e+08-4.6495180e+07j,

#             -4.5219101e+08-5.6013205e+06j]],

#    

#           [[-7.1430163e+08-1.2090212e+08j, -6.2410163e+08-4.1363908e+07j,

#             -3.9189485e+08+4.0016840e+06j],

#            [-5.8365677e+08+9.4236816e+07j, -5.7693280e+08-9.7727496e+07j,

#             -3.9540646e+08+3.4906362e+06j]],

#    

#           [[-5.9637555e+08+8.9477632e+07j, -7.6615610e+08+1.1949610e+08j,

#             -3.8039136e+08-4.7556400e+07j],

#            [-1.1637092e+09-4.0144461e+08j, -1.1735478e+09+4.5104198e+08j,

#             -1.5947418e+08+1.5322706e+07j]],

#    

#           [[-7.1430170e+08-1.2090210e+08j, -6.2410170e+08-4.1363864e+07j,

#             -3.9189485e+08+4.0016840e+06j],

#            [-5.8365658e+08+9.4236840e+07j, -5.7693261e+08-9.7727496e+07j,

#             -3.9540637e+08+3.4906552e+06j]],

#    

#           [[-4.8253002e+08+2.3603400e+07j, -6.4132237e+08+1.1064734e+08j,

#             -4.5701565e+08-7.4987248e+07j],

#            [-5.4175334e+08+5.2096460e+07j, -5.5254304e+08-4.6495116e+07j,

#             -4.5219091e+08-5.6013120e+06j]]], dtype=complex64)>)

"""
## vmap the Circuit Measurements

Use case: accelerating evaluation of Pauli string sum by parallel the parameterized measurement.

For applications on evaluation of parameterized measurements via vmap on large-scale systems, see [large-scale vqe example script](https://github.com/tencent-quantum-lab/tensorcircuit/blob/master/examples/vqe_extra.py).
"""

"""
### Minimal Example
"""

def f(params, structures):
    c = tc.Circuit(nwires)
    c = tc.templates.blocks.example_block(c, params, nlayers=nlayers)
    loss = tc.templates.measurements.parameterized_measurements(
        c, structures, onehot=True
    )
    return loss


# measure X0 to X3
structures = tc.backend.eye(nwires)
f_vvag = tc.backend.jit(tc.backend.vvag(f, vectorized_argnums=1, argnums=0))
f_vvag(tc.backend.ones([2 * nlayers, nwires]), structures)
# Output:
#   WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowBackend.vectorized_value_and_grad.<locals>.wrapper at 0x7fe6cbed1af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

#   (<tf.Tensor: shape=(5,), dtype=float32, numpy=

#    array([-0.3118263 ,  0.00371493,  0.3487355 ,  0.00371514, -0.31182614],

#          dtype=float32)>,

#    <tf.Tensor: shape=(4, 5), dtype=complex64, numpy=

#    array([[ 1.6707865e+00-0.40178323j, -1.1992662e+00-0.23834792j,

#            -1.1992660e+00-0.2383478j ,  1.6707866e+00-0.40178335j,

#             0.0000000e+00+0.j        ],

#           [-1.8267021e-01-0.6483071j ,  7.7729575e-02+0.58401704j,

#            -1.0082662e-01-0.52953976j,  7.7729806e-02+0.58401704j,

#            -1.8267024e-01-0.6483072j ],

#           [ 1.6707866e+00+0.19420199j, -1.1992658e+00+0.50487465j,

#            -1.1992657e+00+0.504875j  ,  1.6707867e+00+0.19420168j,

#             0.0000000e+00+0.j        ],

#           [ 7.4505806e-09+0.99540246j,  1.4901161e-08+0.7925009j ,

#            -7.4505806e-09+0.71156096j, -7.4505806e-09+0.7925008j ,

#             2.2351742e-08+0.9954027j ]], dtype=complex64)>)



================================================
FILE: docs/source/whitepaper/6-3-vmap_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 在量子电路模拟中使用 vmap
"""

"""
## 概述

我们将现代机器学习库的高级特性 vmap 引入到量子电路模拟中。
通过对量子电路模拟的不同成分进行映射，我们可以高效地实现变分量子算法。
值得注意的是，在以下用例中，vmap 与 jit 和 AD 一起支持，可呈现高效的可微分模拟。

支持 vmap 范式的成分如下图所示。

![vmap 成分](../statics/vmap_ingredients.png)

我们有两种不同类型的 vmap API，第一种是 ``vmap`` 而第二种是``vectorized_value_and_grad``，又名``vvag``。 后者还可以返回一批不同电路的梯度信息。
"""

"""
如果需要对梯度和函数值进行批量评估，则可以通过 ``vectorized_value_and_grad`` 来完成。在最简单的情况下，考虑一个函数 $f(x,y)$
其中 $x\in R^p,y\in R^q$ 都是向量，并且希望同时评估 $f(x,y)$ 和 $\sum_x\nabla_y f( x,y) = \sum_x\left ( \frac{\partial f(x,y_1)}{\partial y_1},\ldots, \frac{\partial f(x,y_q)}{\partial y_q}\right )^\top$
在输入 $x$ 的批次 $x_1, x_2,\ldots, x_k$ 上。这是通过创建一个新的矢量化值和梯度函数来实现的：



\begin{equation}
f_{vvg}\left( \begin{pmatrix} \leftarrow x_1 \rightarrow\\ \vdots \\ \leftarrow x_k \rightarrow\end{pmatrix}, y \right) =
\begin{pmatrix} \begin{pmatrix}f(x_1, y) \\ \vdots \\
f(x_k,y)\end{pmatrix},\sum_{i=1}^k \nabla_y f(x_i,y) \end{pmatrix}
\end{equation}
"""

"""
它将批处理输入表示为 $k\times p$ 张量作为第零个参数，并将我们希望区分的变量作为第一个参数。
输出是在所有点 $(x_i,y)$ 处评估的函数值向量，以及所有这些点的平均梯度。
"""

"""
## 设置
"""

import numpy as np
import tensorcircuit as tc

tc.set_backend("tensorflow")
print(tc.__version__)

nwires = 5
nlayers = 2
batch = 6
# Output:
#   0.0.220509


"""
## vmap 输入量子态

用例：量子机器学习任务中输入状态的批处理。

批量输入状态处理的应用请参考[MNIST QML 教程](../tutorials/mnist_qml.ipynb)。
"""

"""
### 最小案例
"""

def f(inputs, weights):
    c = tc.Circuit(nwires, inputs=inputs)
    c = tc.templates.blocks.example_block(c, weights, nlayers=nlayers)
    loss = c.expectation([tc.gates.z(), [2]])
    loss = tc.backend.real(loss)
    return loss


f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=1, vectorized_argnums=0))
f_vg(tc.backend.ones([batch, 2**nwires]), tc.backend.ones([2 * nlayers, nwires]))
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([10.88678, 10.88678, 10.88678, 10.88678, 10.88678, 10.88678],

#          dtype=float32)>,

#    <tf.Tensor: shape=(4, 5), dtype=complex64, numpy=

#    array([[ 0.0000000e+00+1.3064140e+02j, -1.1444092e-05+1.3064142e+02j,

#             0.0000000e+00+1.3064140e+02j,  0.0000000e+00+1.3064139e+02j,

#             0.0000000e+00+0.0000000e+00j],

#           [-1.9073486e-06-5.1765751e-06j, -5.1105431e+01-5.7347143e-07j,

#            -8.1339760e+01-6.6063179e+01j, -5.1105446e+01+3.3477118e-06j,

#            -7.6293945e-06+1.5500746e-07j],

#           [ 0.0000000e+00+8.4607742e+01j, -1.3292285e+02+1.1209973e+02j,

#            -1.3292284e+02+1.1209971e+02j,  1.5258789e-05+8.4607750e+01j,

#             0.0000000e+00+0.0000000e+00j],

#           [ 1.9073486e-06+5.9908474e+01j, -1.5258789e-05-1.9285599e+01j,

#            -8.1339752e+01+3.8049275e-06j,  3.8146973e-06-1.9285591e+01j,

#            -9.5367432e-06+5.9908482e+01j]], dtype=complex64)>)

"""
## vmap 电路权重

用例：批量 VQE，其中同时优化不同的随机初始化参数。

关于批量 VQE 的应用，请参考 [TFIM VQE 教程](../tutorials/tfim_vqe.ipynb)。
"""

"""
### 最小案例
"""

def f(weights):
    c = tc.Circuit(nwires)
    c = tc.templates.blocks.example_block(c, weights, nlayers=nlayers)
    loss = c.expectation([tc.gates.z(), [2]])
    loss = tc.backend.real(loss)
    return loss


f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=0, vectorized_argnums=0))
f_vg(tc.backend.ones([batch, 2 * nlayers, nwires]))
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([-2.9802322e-08, -2.9802322e-08, -2.9802322e-08, -2.9802322e-08,

#           -2.9802322e-08, -2.9802322e-08], dtype=float32)>,

#    <tf.Tensor: shape=(6, 4, 5), dtype=complex64, numpy=

#    array([[[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]],

#    

#           [[ 1.1614500e-08+2.1480869e-08j, -9.2439478e-10-1.8808342e-08j,

#              2.6397275e-08-8.0511313e-09j,  2.7981415e-08-1.6564460e-08j,

#              0.0000000e+00+0.0000000e+00j],

#            [ 4.1470027e-09-1.9918247e-08j, -7.7494953e-09+9.5806874e-09j,

#              0.0000000e+00-1.3076999e-08j,  1.2109957e-09+3.2571617e-08j,

#             -1.0110498e-08+1.6951747e-08j],

#            [ 1.1614500e-08-1.0295013e-08j, -1.6102263e-08+2.5077789e-08j,

#             -3.2204525e-08+5.0155577e-08j,  1.8346144e-08-3.5633683e-09j,

#              0.0000000e+00+0.0000000e+00j],

#            [-7.1439974e-09-3.3070933e-09j,  0.0000000e+00+6.8412485e-09j,

#              1.4287995e-08-9.5050003e-09j, -7.1439974e-09-6.5384995e-09j,

#             -2.3792996e-08-7.8779987e-09j]]], dtype=complex64)>)

"""
## vmap 量子噪声

用例：并行蒙特卡罗噪声模拟。

对于结合 vmapped Monte Carlo 噪声模拟和量子机器学习任务的应用程序，请参阅 [noisy QML script](https://github.com/tencent-quantum-lab/tensorcircuit/blob/master/examples/noisy_qml.py)。
"""

"""
### 最小案例
"""

def f(weights, status):
    c = tc.Circuit(nwires)
    c = tc.templates.blocks.example_block(c, weights, nlayers=nlayers)
    for i in range(nwires):
        c.depolarizing(i, px=0.2, py=0.2, pz=0.2, status=status[i])
    loss = c.expectation([tc.gates.x(), [2]])
    loss = tc.backend.real(loss)
    return loss


f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=0, vectorized_argnums=1))


def g(weights):
    status = tc.backend.implicit_randu(shape=[batch, nwires])
    return f_vg(weights, status)


g(tc.backend.ones([2 * nlayers, nwires]))
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([ 0.34873545, -0.34873545, -0.34873545, -0.34873545, -0.34873545,

#            0.34873545], dtype=float32)>,

#    <tf.Tensor: shape=(4, 5), dtype=complex64, numpy=

#    array([[-8.8614023e-01-1.7026657e-08j,  5.7763958e-01+2.3834804e-01j,

#             5.7763910e-01+2.3834780e-01j, -8.8614047e-01+1.2538894e-07j,

#             0.0000000e+00+0.0000000e+00j],

#           [ 0.0000000e+00+6.9313288e-01j,  3.6122650e-01-1.1496974e-02j,

#            -5.2079970e-01-1.7869800e-01j,  3.6122644e-01-1.1496985e-02j,

#            -2.9802322e-08+6.9313288e-01j],

#           [-5.9604645e-08-1.0189922e+00j, -3.0850098e-01-1.4861794e-07j,

#            -3.0850050e-01-2.2304604e-08j,  5.9604645e-08-1.0189921e+00j,

#             0.0000000e+00+0.0000000e+00j],

#           [ 0.0000000e+00+3.1868588e-02j, -8.9406967e-08+2.5656950e-01j,

#            -2.9802322e-08-1.9999983e+00j, -2.9802322e-08+2.5656945e-01j,

#            -1.1920929e-07+3.1868652e-02j]], dtype=complex64)>)

"""
## vmap 电路结构

用例：可微量子架构搜索 (DQAS)。

有关 DQAS 应用程序的更多详细信息，请参阅 [DQAS 教程](../tutorials/dqas.ipynb)。
"""

"""
### 最小案例
"""

eye = tc.gates.i().tensor
x = tc.gates.x().tensor
y = tc.gates.y().tensor
z = tc.gates.z().tensor


def f(params, structures):
    c = tc.Circuit(nwires)
    for i in range(nwires):
        c.H(i)
    for j in range(nlayers):
        for i in range(nwires - 1):
            c.cz(i, i + 1)
        for i in range(nwires):
            c.unitary(
                i,
                unitary=structures[i, j, 0]
                * (
                    tc.backend.cos(params[i, j, 0]) * eye
                    + tc.backend.sin(params[i, j, 0]) * x
                )
                + structures[i, j, 1]
                * (
                    tc.backend.cos(params[i, j, 1]) * eye
                    + tc.backend.sin(params[i, j, 1]) * y
                )
                + structures[i, j, 2]
                * (
                    tc.backend.cos(params[i, j, 2]) * eye
                    + tc.backend.sin(params[i, j, 2]) * z
                ),
            )
    loss = c.expectation([tc.gates.z(), (2,)])
    return tc.backend.real(loss)


structures = tc.backend.ones([batch, nwires, nlayers, 3])
params = tc.backend.ones([nwires, nlayers, 3])
f_vg = tc.backend.jit(tc.backend.vvag(f, argnums=0, vectorized_argnums=1))
f_vg(params, structures)
# Output:
#   (<tf.Tensor: shape=(6,), dtype=float32, numpy=

#    array([2.4917054e+08, 2.4917054e+08, 2.4917054e+08, 2.4917054e+08,

#           2.4917054e+08, 2.4917054e+08], dtype=float32)>,

#    <tf.Tensor: shape=(5, 2, 3), dtype=complex64, numpy=

#    array([[[-4.8252989e+08+2.3603376e+07j, -6.4132224e+08+1.1064736e+08j,

#             -4.5701562e+08-7.4987272e+07j],

#            [-5.4175347e+08+5.2096408e+07j, -5.5254317e+08-4.6495180e+07j,

#             -4.5219101e+08-5.6013205e+06j]],

#    

#           [[-7.1430163e+08-1.2090212e+08j, -6.2410163e+08-4.1363908e+07j,

#             -3.9189485e+08+4.0016840e+06j],

#            [-5.8365677e+08+9.4236816e+07j, -5.7693280e+08-9.7727496e+07j,

#             -3.9540646e+08+3.4906362e+06j]],

#    

#           [[-5.9637555e+08+8.9477632e+07j, -7.6615610e+08+1.1949610e+08j,

#             -3.8039136e+08-4.7556400e+07j],

#            [-1.1637092e+09-4.0144461e+08j, -1.1735478e+09+4.5104198e+08j,

#             -1.5947418e+08+1.5322706e+07j]],

#    

#           [[-7.1430170e+08-1.2090210e+08j, -6.2410170e+08-4.1363864e+07j,

#             -3.9189485e+08+4.0016840e+06j],

#            [-5.8365658e+08+9.4236840e+07j, -5.7693261e+08-9.7727496e+07j,

#             -3.9540637e+08+3.4906552e+06j]],

#    

#           [[-4.8253002e+08+2.3603400e+07j, -6.4132237e+08+1.1064734e+08j,

#             -4.5701565e+08-7.4987248e+07j],

#            [-5.4175334e+08+5.2096460e+07j, -5.5254304e+08-4.6495116e+07j,

#             -4.5219091e+08-5.6013120e+06j]]], dtype=complex64)>)

"""
## vmap 电路测量

用例：通过并行参数化测量加速泡利字符串和的评估。

有关在大型系统上通过 vmap 评估参数化测量的应用，请参阅 [大型 vqe 示例脚本](https://github.com/tencent-quantum-lab/tensorcircuit/blob/master/examples/vqe_extra.py)。
"""

"""
### 最小案例
"""

def f(params, structures):
    c = tc.Circuit(nwires)
    c = tc.templates.blocks.example_block(c, params, nlayers=nlayers)
    loss = tc.templates.measurements.parameterized_measurements(
        c, structures, onehot=True
    )
    return loss


# 测量 X0 到 X3
structures = tc.backend.eye(nwires)
f_vvag = tc.backend.jit(tc.backend.vvag(f, vectorized_argnums=1, argnums=0))
f_vvag(tc.backend.ones([2 * nlayers, nwires]), structures)
# Output:
#   WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowBackend.vectorized_value_and_grad.<locals>.wrapper at 0x7fe6cbed1af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.

#   (<tf.Tensor: shape=(5,), dtype=float32, numpy=

#    array([-0.3118263 ,  0.00371493,  0.3487355 ,  0.00371514, -0.31182614],

#          dtype=float32)>,

#    <tf.Tensor: shape=(4, 5), dtype=complex64, numpy=

#    array([[ 1.6707865e+00-0.40178323j, -1.1992662e+00-0.23834792j,

#            -1.1992660e+00-0.2383478j ,  1.6707866e+00-0.40178335j,

#             0.0000000e+00+0.j        ],

#           [-1.8267021e-01-0.6483071j ,  7.7729575e-02+0.58401704j,

#            -1.0082662e-01-0.52953976j,  7.7729806e-02+0.58401704j,

#            -1.8267024e-01-0.6483072j ],

#           [ 1.6707866e+00+0.19420199j, -1.1992658e+00+0.50487465j,

#            -1.1992657e+00+0.504875j  ,  1.6707867e+00+0.19420168j,

#             0.0000000e+00+0.j        ],

#           [ 7.4505806e-09+0.99540246j,  1.4901161e-08+0.7925009j ,

#            -7.4505806e-09+0.71156096j, -7.4505806e-09+0.7925008j ,

#             2.2351742e-08+0.9954027j ]], dtype=complex64)>)



================================================
FILE: docs/source/whitepaper/6-4-quoperator.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# QuOperator in TensorCircuit
"""

"""
## Overview
"""

"""
`tensorcircuit.quantum.QuOperator`, `tensorcircuit.quantum.QuVector` and `tensorcircuit.quantum.QuAdjointVector` are classes adopted from the TensorNetwork package.
They behave like a matrix/vector (column or row) when interacting with other ingredients while the inner structure is maintained by the TensorNetwork for efficiency and compactness.

Typical tensor network structures for a QuOperator/QuVector correspond to Matrix Product Operators (MPO) / Matrix Product States (MPS). The former represents a matrix as:
$M_{i1,i2,...in; \; j1, j2,... jn}=\prod_k {T_k}^{i_k, j_k}$,
i.e., a product of $d\times d$ matrices $T_k^{i_k, j_k}$, where $d$ is known as the bond dimension. Similarly, an MPS represents a vector as:
$V_{i_1,...i_n} = \prod_k T_k^{i_k}$,
where the $T_k^{i_k}$ are, again, $d\times d$ matrices. MPS and MPO often occur in computational quantum physics contexts, as they give compact representations for certain types of quantum states and operators.

``QuOperator``/``QuVector`` objects can represent any MPO/MPS, but they can additionally express more flexible tensor network structures. Indeed, any tensor network with two sets of dangling edges of the same dimension (i.e., for each $k$, the set $\{T_k^{i_k,j_k}\}_{i_k,j_k}$ of matrices has $i_k$ and $j_k$ running over the same index set) can be treated as a ``QuOperator``. A general ``QuVector`` is even more flexible, in that the dangling edge dimensions can be chosen freely, and thus arbitrary tensor products of vectors can be represented.

In this note, we will show how such tensor network backend matrix/vector data structure is more efficient and compact in several scenarios and how these structures integrated with quantum circuit simulation tasks seamlessly as different circuit ingredients.
"""

"""
## Setup
"""

import numpy as np
import tensornetwork as tn
import tensorcircuit as tc

print(tc.__version__)
# Output:
#   0.0.220509


"""
## Introduction to QuOperator/QuVector
"""

n1 = tc.gates.Gate(np.ones([2, 2, 2]), name="n1")
n2 = tc.gates.Gate(np.ones([2, 2, 2]), name="n2")
n3 = tc.gates.Gate(np.ones([2, 2]), name="n3")
# name is only for debug and visualization, can be omitted
n1[2] ^ n2[2]
n2[1] ^ n3[0]

# initialize a QuOperator by giving two sets of dangling edges for row and col index
matrix = tc.quantum.QuOperator(out_edges=[n1[0], n2[0]], in_edges=[n1[1], n3[1]])
tn.to_graphviz(matrix.nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc4ca670>

n4 = tc.gates.Gate(np.ones([2]), name="n4")
n5 = tc.gates.Gate(np.ones([2]), name="n5")

# initialize a QuVector by giving dangling edges
vector = tc.quantum.QuVector([n4[0], n5[0]])
tn.to_graphviz(vector.nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f8811369190>

nvector = matrix @ vector
tn.to_graphviz(nvector.nodes)
# nvector has two dangling edges
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc4caf40>

assert type(nvector) == tc.quantum.QuVector

nvector.eval_matrix()
# Output:
#   array([[16.],

#          [16.],

#          [16.],

#          [16.]])

# or we can have more matrix/vector-like operation
(3 * nvector).eval_matrix()
# Output:
#   array([[48.],

#          [48.],

#          [48.],

#          [48.]])

matrix.partial_trace([0]).eval_matrix()
# Output:
#   array([[8., 8.],

#          [8., 8.]])

"""
Note how in this example, ``matrix`` is not a typical MPO but still can be expressed as ``QuOperator``. Indeed, any tensor network with two sets of dangling edges of the same dimension can be treated as ``QuOperator``. ``QuVector`` is even more flexible since we can treat all dangling edges as the vector dimension.

Also, note how ``^`` is overloaded as ``tn.connect`` to connect edges between different nodes in TensorNetwork. And indexing the node gives the edges of the node, eg. ``n1[0]`` means the first edge of node ``n1``.

The convention to define the ``QuOperator`` is firstly giving ``out_edges`` (left index or row index of the matrix) and then giving ``in_edges`` (right index or column index of the matrix). The edges list contains edge objects from the TensorNetwork library.

Such QuOperator/QuVector abstraction support various calculations only possible on matrix/vectors, such as matmul (``@``), adjoint (``.adjoint()``), scalar multiplication (``*``), tensor product (``|``), and partial trace (``.partial_trace(subsystems_to_trace_out)``).
To extract the matrix information of these objects, we can use ``.eval()`` or ``.eval_matrix()``, the former keeps the shape information of the tensor network while the latter gives the matrix representation with shape rank 2.
"""

"""
The workflow here can also be summarized and visualized as ![](../statics/quop.png)
"""

"""
## QuVector as the Input State for the Circuit

Since ``QuVector`` behaves like a real vector with a more compact representation, we can feed the circuit input states in the form of ``QuVector`` instead of a plain numpy array vector.
"""

# This examples shows how we feed a |111> state into the circuit

n = 3
nodes = [tc.gates.Gate(np.array([0.0, 1.0])) for _ in range(n)]
mps = tc.quantum.QuVector([nd[0] for nd in nodes])
c = tc.Circuit(n, mps_inputs=mps)
c.x(0)
c.expectation_ps(z=[0])
# Output:
#   array(1.+0.j)

"""
## QuVector as the Output State of the Circuit

The tensor network representation of the circuit can be regarded as a ``QuVector``, namely we can manipulate the circuit as a vector before the real contraction. This is also how we do circuit composition internally.
"""

# Circuit composition example

n = 3
c1 = tc.Circuit(n)
c1.X(0)
c1.cnot(0, 1)
mps = c1.quvector()
c2 = tc.Circuit(n, mps_inputs=mps)
c2.X(2)
c2.X(1)
c2.cz(1, 2)
c2.expectation_ps(z=[1])
tn.to_graphviz(c2.get_quvector().nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc522e50>

# The above is the core internal mechanism for circuit composition
# the user API is as follows

n = 3
c1 = tc.Circuit(n)
c1.X(0)
c1.cnot(0, 1)

c2 = tc.Circuit(n)
c2.X(2)
c2.X(1)
c2.cz(1, 2)
c1.append(c2)

c1.draw()
# Output:
#        ┌───┐             

#   q_0: ┤ X ├──■──────────

#        └───┘┌─┴─┐┌───┐   

#   q_1: ─────┤ X ├┤ X ├─■─

#        ┌───┐└───┘└───┘ │ 

#   q_2: ┤ X ├───────────■─

#        └───┘             

"""
## QuOperator as Operator to be Evaluated on the Circuit

The matrix to be evaluated over the output state of the circuit can also be represented by QuOperator, which is very powerful and efficient for some lattices model Hamiltonian.
"""

# Here we show the simplest model, where we measure <Z_0Z_1>

z0, z1 = tc.gates.z(), tc.gates.z()
mpo = tc.quantum.QuOperator([z0[0], z1[0]], [z0[1], z1[1]])
c = tc.Circuit(2)
c.X(0)
tc.templates.measurements.mpo_expectation(c, mpo)
# the mpo expectation API
# Output:
#   -1.0

"""
## QuOperator as the Quantum Gate Applied on the Circuit

Since quantum gates are also unitary matrices, we can also use QuOperator for quantum gates. In some cases, QuOperator representation for quantum gates is much more compact, such as in multi-control gate case, where the bond dimension can be reduced to 2 for neighboring qubits.
"""

# The general mpo gate API is just ``Circuit.mpo()``

x0, x1 = tc.gates.Gate(np.ones([2, 2, 3]), name="x0"), tc.gates.Gate(
    np.ones([2, 2, 3]), name="x1"
)
x0[2] ^ x1[2]
mpo = tc.quantum.QuOperator([x0[0], x1[0]], [x0[1], x1[1]])
c = tc.Circuit(2)
c.mpo(0, 1, mpo=mpo)
tn.to_graphviz(c._nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fd5f8610>

# the built-in multi-control gate is used as follows:

c = tc.Circuit(3)
c.multicontrol(0, 1, 2, ctrl=[0, 1], unitary=tc.gates.x())
tn.to_graphviz(c._nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc4d5550>

c.to_qir()[0]["gate"].nodes
# Output:
#   {Node

#    (

#    name : '__unnamed_node__',

#    tensor : 

#    array([[[[0.+0.j, 1.+0.j],

#             [0.+0.j, 0.+0.j]],

#    

#            [[0.+0.j, 0.+0.j],

#             [1.+0.j, 0.+0.j]]],

#    

#    

#           [[[0.+0.j, 1.+0.j],

#             [0.+0.j, 0.+0.j]],

#    

#            [[0.+0.j, 0.+0.j],

#             [0.+0.j, 1.+0.j]]]], dtype=complex64),

#    edges : 

#    [

#    Edge('__unnamed_node__'[2] -> '__unnamed_node__'[0] )

#    , 

#    Edge(Dangling Edge)[1] 

#    , 

#    Edge(Dangling Edge)[2] 

#    , 

#    Edge('__unnamed_node__'[3] -> '__unnamed_node__'[0] )

#    ] 

#    ),

#    Node

#    (

#    name : '__unnamed_node__',

#    tensor : 

#    array([[[1.+0.j, 0.+0.j],

#            [0.+0.j, 0.+0.j]],

#    

#           [[0.+0.j, 0.+0.j],

#            [0.+0.j, 1.+0.j]]], dtype=complex64),

#    edges : 

#    [

#    Edge(Dangling Edge)[0] 

#    , 

#    Edge(Dangling Edge)[1] 

#    , 

#    Edge('__unnamed_node__'[2] -> '__unnamed_node__'[0] )

#    ] 

#    ),

#    Node

#    (

#    name : '__unnamed_node__',

#    tensor : 

#    array([[[0.+0.j, 1.+0.j],

#            [1.+0.j, 0.+0.j]],

#    

#           [[1.+0.j, 0.+0.j],

#            [0.+0.j, 1.+0.j]]], dtype=complex64),

#    edges : 

#    [

#    Edge('__unnamed_node__'[3] -> '__unnamed_node__'[0] )

#    , 

#    Edge(Dangling Edge)[1] 

#    , 

#    Edge(Dangling Edge)[2] 

#    ] 

#    )}

c.to_qir()[0]["gate"].eval_matrix()
# Output:
#   array([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],

#         dtype=complex64)



================================================
FILE: docs/source/whitepaper/6-4-quoperator_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# TensorCircuit 中的 QuOperator
"""

"""
## 概述
"""

"""
`tensorcircuit.quantum.QuOperator`, `tensorcircuit.quantum.QuVector` 和 `tensorcircuit.quantum.QuAdjointVector` 是从 TensorNetwork 包中采用的类。
当与其他组件交互时，它们的行为类似于矩阵/向量（列或行），而内部结构由张量网络维护以提高效率和紧凑性。

QuOperator/QuVector 的典型张量网络结构对应于矩阵乘积运算符 (MPO) / 矩阵乘积状态 (MPS)。前者将矩阵表示为：
$M_{i1,i2,...in; \; j1, j2,... jn}=\prod_k {T_k}^{i_k, j_k}$,
即，$d\times d$ 矩阵 $T_k^{i_k, j_k}$ 的乘积，其中 $d$ 称为键维。类似地，MPS 将向量表示为：
$V_{i_1,...i_n} = \prod_k T_k^{i_k}$,
其中 $T_k^{i_k}$ 又是 $d\times d$ 矩阵。 MPS 和 MPO 经常出现在计算量子物理环境中，因为它们为某些类型的量子状态和算子提供了紧凑的表示。

``QuOperator``/``QuVector`` 对象可以表示任何 MPO/MPS，但它们还可以表示更灵活的张量网络结构。
实际上，任何具有两组相同维度的悬垂边的张量网络（即，对于每个 $k$，矩阵集 $\{T_k^{i_k,j_k}\}_{i_k,j_k}$ 具有 $i_k $ 和 $j_k$ 在同一个索引集上运行）
可以被视为“QuOperator”。一般的 QuVector 更加灵活，因为可以自由选择悬垂边的维度，因此可以表示任意向量的张量积。

在本笔记中，我们将展示这种张量网络后端矩阵/向量数据结构如何在多种场景中更加高效和紧凑，以及这些结构如何作为不同的电路成分与量子电路模拟任务无缝集成。
"""

"""
## 设置
"""

import numpy as np
import tensornetwork as tn
import tensorcircuit as tc

print(tc.__version__)
# Output:
#   0.0.220509


"""
## QuOperator/QuVector 简介
"""

n1 = tc.gates.Gate(np.ones([2, 2, 2]), name="n1")
n2 = tc.gates.Gate(np.ones([2, 2, 2]), name="n2")
n3 = tc.gates.Gate(np.ones([2, 2]), name="n3")
# name 仅用于调试和可视化，可以省略
n1[2] ^ n2[2]
n2[1] ^ n3[0]

# 通过为行和列索引提供两组悬垂边来初始化 QuOperator
matrix = tc.quantum.QuOperator(out_edges=[n1[0], n2[0]], in_edges=[n1[1], n3[1]])
tn.to_graphviz(matrix.nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc4ca670>

n4 = tc.gates.Gate(np.ones([2]), name="n4")
n5 = tc.gates.Gate(np.ones([2]), name="n5")

# 通过给出悬垂边来初始化 QuVector
vector = tc.quantum.QuVector([n4[0], n5[0]])
tn.to_graphviz(vector.nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f8811369190>

nvector = matrix @ vector
tn.to_graphviz(nvector.nodes)
# nvector 有两个悬垂边
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc4caf40>

assert type(nvector) == tc.quantum.QuVector

nvector.eval_matrix()
# Output:
#   array([[16.],

#          [16.],

#          [16.],

#          [16.]])

# 或者我们可以有更多类似矩阵/向量的操作
(3 * nvector).eval_matrix()
# Output:
#   array([[48.],

#          [48.],

#          [48.],

#          [48.]])

matrix.partial_trace([0]).eval_matrix()
# Output:
#   array([[8., 8.],

#          [8., 8.]])

"""
注意在这个例子中，``matrix`` 不是一个典型的 MPO，但仍然可以表示为 ``QuOperator``。
事实上，任何具有两组相同维度的悬垂边的张量网络都可以被视为``QuOperator``。
``QuVector`` 更加灵活，因为我们可以将所有悬垂边视为向量维度。

另外，请注意 ``^`` 如何被重载为 ``tn.connect`` 以连接 TensorNetwork 中不同节点之间的边。
并且索引节点给出了节点的边，例如，``n1[0]`` 表示节点 ``n1`` 的第一条边。

定义 ``QuOperator`` 的约定是首先给出 ``out_edges``（矩阵的左索引或行索引），然后给出 ``in_edges``（矩阵的右索引或列索引）。
边列表包含来自 TensorNetwork 库的边对象。

这种 QuOperator/QuVector 抽象支持各种只能在矩阵/向量上进行的计算，例如 matmul (``@``)、伴随 (``.adjoint()``)、标量乘法 (``*``)、
张量积 （``|``）和部分跟踪（``.partial_trace(subsystems_to_trace_out)``）。
要提取这些对象的矩阵信息，我们可以使用 .eval() 或 .eval_matrix() ，前者保留张量网络的形状信息，后者给出带有形状秩的矩阵表示 2.
"""

"""
这里的工作流程也可以总结和可视化为 ![quop](../statics/quop.png)
"""

"""
## QuVector 作为电路的输入量子态

由于 QuVector 表现得像一个具有更紧凑表示的向量，我们可以以 QuVector 的形式而不是普通的 numpy 数组向量来提供电路输入状态。
"""

# 这个例子展示了我们如何将 |111> 状态输入到电路中
n = 3
nodes = [tc.gates.Gate(np.array([0.0, 1.0])) for _ in range(n)]
mps = tc.quantum.QuVector([nd[0] for nd in nodes])
c = tc.Circuit(n, mps_inputs=mps)
c.x(0)
c.expectation_ps(z=[0])
# Output:
#   array(1.+0.j)

"""
## QuVector 作为电路的输出量子态

电路的张量网络表示可以看作是一个 ``QuVector``，即我们可以在真正收缩之前将电路作为一个向量来操作。这也是我们在内部进行电路组合的方式。
"""

# 电路组合示例

n = 3
c1 = tc.Circuit(n)
c1.X(0)
c1.cnot(0, 1)
mps = c1.quvector()
c2 = tc.Circuit(n, mps_inputs=mps)
c2.X(2)
c2.X(1)
c2.cz(1, 2)
c2.expectation_ps(z=[1])
tn.to_graphviz(c2.get_quvector().nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc522e50>

# 以上是电路组成的核心内部机制
# 用户 API 如下
n = 3
c1 = tc.Circuit(n)
c1.X(0)
c1.cnot(0, 1)

c2 = tc.Circuit(n)
c2.X(2)
c2.X(1)
c2.cz(1, 2)
c1.append(c2)

c1.draw()
# Output:
#        ┌───┐             

#   q_0: ┤ X ├──■──────────

#        └───┘┌─┴─┐┌───┐   

#   q_1: ─────┤ X ├┤ X ├─■─

#        ┌───┐└───┘└───┘ │ 

#   q_2: ┤ X ├───────────■─

#        └───┘             

"""
## QuOperator 作为要在电路上评估的算子

对电路的输出状态进行评估的矩阵也可以用 QuOperator 来表示，这对于一些格点模型哈密顿量来说是非常强大和高效的。
"""

# 这里我们展示一个最简单的模型，我们测量 <Z_0Z_1>

z0, z1 = tc.gates.z(), tc.gates.z()
mpo = tc.quantum.QuOperator([z0[0], z1[0]], [z0[1], z1[1]])
c = tc.Circuit(2)
c.X(0)
tc.templates.measurements.mpo_expectation(c, mpo)
# mpo 期望 API
# Output:
#   -1.0

"""
## QuOperator 作为应用于电路的量子门

由于量子门也是幺正矩阵，我们也可以将 QuOperator 用于量子门。
在某些情况下，量子门的 QuOperator 表示要紧凑得多，例如在多控制门情况下，相邻量子位的键维数可以减少到 2。
"""

# 一般的 mpo 门 API 是 ``Circuit.mpo()``

x0, x1 = tc.gates.Gate(np.ones([2, 2, 3]), name="x0"), tc.gates.Gate(
    np.ones([2, 2, 3]), name="x1"
)
x0[2] ^ x1[2]
mpo = tc.quantum.QuOperator([x0[0], x1[0]], [x0[1], x1[1]])
c = tc.Circuit(2)
c.mpo(0, 1, mpo=mpo)
tn.to_graphviz(c._nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fd5f8610>

# 内置多控制门使用如下：

c = tc.Circuit(3)
c.multicontrol(0, 1, 2, ctrl=[0, 1], unitary=tc.gates.x())
tn.to_graphviz(c._nodes)
# Output:
#   <graphviz.graphs.Graph at 0x7f87fc4d5550>

c.to_qir()[0]["gate"].nodes
# Output:
#   {Node

#    (

#    name : '__unnamed_node__',

#    tensor : 

#    array([[[[0.+0.j, 1.+0.j],

#             [0.+0.j, 0.+0.j]],

#    

#            [[0.+0.j, 0.+0.j],

#             [1.+0.j, 0.+0.j]]],

#    

#    

#           [[[0.+0.j, 1.+0.j],

#             [0.+0.j, 0.+0.j]],

#    

#            [[0.+0.j, 0.+0.j],

#             [0.+0.j, 1.+0.j]]]], dtype=complex64),

#    edges : 

#    [

#    Edge('__unnamed_node__'[2] -> '__unnamed_node__'[0] )

#    , 

#    Edge(Dangling Edge)[1] 

#    , 

#    Edge(Dangling Edge)[2] 

#    , 

#    Edge('__unnamed_node__'[3] -> '__unnamed_node__'[0] )

#    ] 

#    ),

#    Node

#    (

#    name : '__unnamed_node__',

#    tensor : 

#    array([[[1.+0.j, 0.+0.j],

#            [0.+0.j, 0.+0.j]],

#    

#           [[0.+0.j, 0.+0.j],

#            [0.+0.j, 1.+0.j]]], dtype=complex64),

#    edges : 

#    [

#    Edge(Dangling Edge)[0] 

#    , 

#    Edge(Dangling Edge)[1] 

#    , 

#    Edge('__unnamed_node__'[2] -> '__unnamed_node__'[0] )

#    ] 

#    ),

#    Node

#    (

#    name : '__unnamed_node__',

#    tensor : 

#    array([[[0.+0.j, 1.+0.j],

#            [1.+0.j, 0.+0.j]],

#    

#           [[1.+0.j, 0.+0.j],

#            [0.+0.j, 1.+0.j]]], dtype=complex64),

#    edges : 

#    [

#    Edge('__unnamed_node__'[3] -> '__unnamed_node__'[0] )

#    , 

#    Edge(Dangling Edge)[1] 

#    , 

#    Edge(Dangling Edge)[2] 

#    ] 

#    )}

c.to_qir()[0]["gate"].eval_matrix()
# Output:
#   array([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],

#          [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],

#         dtype=complex64)



================================================
FILE: docs/source/whitepaper/6-5-custom-contraction.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Customized Contraction
"""

"""
## Overview

If the simulated circuit has large qubit counts, we recommend users try a customized contraction setup instead of the default one, which is greedy.
"""

"""
## Setup

Please refer to the [installation documentation](https://cotengra.readthedocs.io/en/latest/installation.html) for cotengra, which cannot simply be obtained by pip install since it is not uploaded to PyPI. The easiest way for installation is ``pip install -U git+https://github.com/jcmgray/cotengra.git``.
"""

import tensorcircuit as tc
import numpy as np
import cotengra as ctg

"""
We use the following example as a testbed for the contraction, the real contraction is invoked for ``Circuit.expectation`` API,
and there are two stages for the contraction. The first one is contraction path searching which is used to find a better contraction path in terms of space and time. The second stage is the real contraction, where matrix multiplication is called using ML backend API. In this note, we focus on the performance of the first stage. And the contraction path solver can be customized with any type of [opt-einsum compatible path solver](https://optimized-einsum.readthedocs.io/en/stable/custom_paths.html).
"""

def testbed():
    n = 40
    d = 6
    param = K.ones([2 * d, n])
    c = tc.Circuit(n)
    c = tc.templates.blocks.example_block(c, param, nlayers=d, is_split=True)
    # the two-qubit gate is split and truncated with SVD decomposition
    return c.expectation_ps(z=[n // 2], reuse=False)

"""
There are several contractor optimizers provided by opt-einsum and shipped with the TensorNetwork package. Since TensorCircuit is built on top of TensorNetwork, we can use these simple contractor optimizers. Though for any moderate system, only a greedy optimizer works, other optimizers come with exponential scaling and fail in circuit simulation scenarios.

We always set ``contraction_info=True`` (default is ``False``) for the contractor system in this note, which will print contraction information summary including contraction size, flops, and write. For the definition of these metrics, also refer to cotengra docs and [the corresponding paper](https://quantum-journal.org/papers/q-2021-03-15-410/).

Metrics that measure the quality of a contraction path include 
   
   * **FLOPs**: the total number of computational operations required for all matrix multiplications involved when contracting the tensor network via the given path. This metric characterizes the total simulation time.
    
   * **WRITE**: the total size (the number of elements) of all tensors -- including intermediate tensors -- computed during the contraction. 
    
   * **SIZE**: the size of the largest intermediate tensor stored in memory.

Since simulations in TensorCircuit are AD-enabled, where all intermediate results need to be cached and traced, the more relevant spatial cost metric is writes instead of size.

Also, we will enable ``debug_level=2`` in ``set_contractor`` (never use this option in real computation!) By enabling this, the second stage of the contraction, i.e. the real contraction, will not happen. We can focus on the contraction path information, which demonstrates the difference between different customized contractors.
"""

tc.set_contractor("greedy", debug_level=2, contraction_info=True)
# the default contractor
testbed()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  12.393  log2[SIZE]:  30  log2[WRITE]:  35.125

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
**cotengra optimizer**: for hyperparameters tuning, see the [documentation](https://cotengra.readthedocs.io/en/latest/advanced.html).

"""

opt = ctg.ReusableHyperOptimizer(
    methods=["greedy", "kahypar"],
    parallel=True,
    minimize="write",
    max_time=120,
    max_repeats=1024,
    progbar=True,
)
# Caution: for now, parallel only works for "ray" in newer versions of python
tc.set_contractor(
    "custom", optimizer=opt, preprocessing=True, contraction_info=True, debug_level=2
)
# the opt-einsum compatible function interface is passed as the argument of optimizer\
# Also note how preprocessing=True merges the single qubits gate into the neighbor two-qubit gate
testbed()
# Output:
#   log2[SIZE]: 15.00 log10[FLOPs]: 7.56:  45%|██████████████████▊                       | 458/1024 [02:03<02:32,  3.70it/s]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  7.565  log2[SIZE]:  15  log2[WRITE]:  19.192

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
We can even include contraction reconfigure after path searching, which further greatly boosts the space efficiency for the contraction path.
"""

opt = ctg.ReusableHyperOptimizer(
    minimize="combo",
    max_repeats=1024,
    max_time=120,
    progbar=True,
)


def opt_reconf(inputs, output, size, **kws):
    tree = opt.search(inputs, output, size)
    tree_r = tree.subtree_reconfigure_forest(
        progbar=True, num_trees=10, num_restarts=20, subtree_weight_what=("size",)
    )
    return tree_r.get_path()


# there is also a default parallel=True option for subtree_reconfigure_forest,
# this can only be set as "ray" for newer version python as above
# note how different versions of cotengra have breaking APIs in the last line: get_path or path
# the user may need to change the API to make the example work

tc.set_contractor(
    "custom",
    optimizer=opt_reconf,
    contraction_info=True,
    preprocessing=True,
    debug_level=2,
)
testbed()
# Output:
#   log2[SIZE]: 15.00 log10[FLOPs]: 7.46:  32%|█████████████▍                            | 329/1024 [02:00<04:13,  2.74it/s]

#   log2[SIZE]: 14.00 log10[FLOPs]: 7.02: 100%|█████████████████████████████████████████████| 20/20 [01:05<00:00,  3.30s/it]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  7.021  log2[SIZE]:  14  log2[WRITE]:  19.953

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>



================================================
FILE: docs/source/whitepaper/6-5-custom-contraction_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 定制收缩路径
"""

"""
## 概述

如果模拟电路的量子比特数很大，我们建议用户尝试自定义收缩设置，而不是使用贪婪的默认设置。
"""

"""
## 设置

cotengra 安装请参考[安装文档](https://cotengra.readthedocs.io/en/latest/installation.html)，由于没有上传到PyPI，所以无法通过
pip install 简单获取。最简单的安装方式是 ``pip install -U git+https://github.com/jcmgray/cotengra.git``。
"""

import tensorcircuit as tc
import numpy as np
import cotengra as ctg

"""
我们使用以下示例作为收缩的测试平台，真正的 contractor 是为 ``Circuit.expectation`` API 调用的。
收缩有两个阶段，第一个是收缩路径搜索，用于在空间和时间方面找到更好的收缩路径。第二阶段是真正的收缩，使用 ML 后端 API 调用矩阵乘法。
在本说明中，我们关注第一阶段的性能，并且可以使用任何类型的 [opt-einsum 兼容路径求解器](https://optimized-einsum.readthedocs.io/en/stable/custom_paths.html)
自定义收缩路径求解器。
"""

def testbed():
    n = 40
    d = 6
    param = K.ones([2 * d, n])
    c = tc.Circuit(n)
    c = tc.templates.blocks.example_block(c, param, nlayers=d, is_split=True)
    # 用 SVD 分解对两个量子比特门进行分割和截断
    return c.expectation_ps(z=[n // 2], reuse=False)

"""
opt-einsum 提供了几个收缩优化器，并随 TensorNetwork 包一起提供。由于 TensorCircuit 建立在 TensorNetwork 之上，我们可以使用这些简单的收缩优化器。
尽管对于任何中等系统，只有贪婪优化器有效，但其他优化器具有指数标度并且在电路模拟场景中失败。

在本说明中，我们始终为收缩系统设置 ``contraction_info=True``（默认为 ``False``），它将打印收缩信息摘要，包括 size、flops 和 writes。
有关这些指标的定义，另请参阅 cotengra 文档和 [相应论文](https://quantum-journal.org/papers/q-2021-03-15-410/)。

衡量收缩路径质量的指标包括

    * **FLOPs**：通过给定路径收缩张量网络时涉及的所有矩阵乘法所需的计算操作总数。该指标表征了总的模拟时间。

    * **WRITE**：在收缩期间计算的所有张量（包括中间张量）的总大小（元素数量）。

    * **SIZE**：存储在内存中的最大中间张量的大小。

由于 TensorCircuit 中的模拟启用了 AD，所有中间结果都需要缓存和跟踪，因此需要关注的空间开销是 write 而非 size。

此外，我们将在 ``set_contractor`` 中启用 ``debug_level=2``（不要在实际计算中使用此选项！）通过启用此选项，收缩的第二阶段，即真正的收缩，将不会发生。
我们可以关注收缩路径信息，它展示了不同定制 contractor 之间的差异。
"""

tc.set_contractor("greedy", debug_level=2, contraction_info=True)
# 默认 contractor
testbed()
# Output:
#   ------ contraction cost summary ------

#   log10[FLOPs]:  12.393  log2[SIZE]:  30  log2[WRITE]:  35.125

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
**cotengra 优化器**：有关超参数调整，请参阅[文档](https://cotengra.readthedocs.io/en/latest/advanced.html)。
"""

opt = ctg.ReusableHyperOptimizer(
    methods=["greedy", "kahypar"],
    parallel=True,
    minimize="write",
    max_time=120,
    max_repeats=1024,
    progbar=True,
)
# 注意：目前对于新版本 python，仅 "ray" 选项对于 parallel 参数适用
tc.set_contractor(
    "custom", optimizer=opt, preprocessing=True, contraction_info=True, debug_level=2
)
# opt-einsum 兼容函数接口作为优化器的参数传递\
# 还要注意 preprocessing=True 可以将单个量子比特门合并到相邻的两个量子比特门中
testbed()
# Output:
#   log2[SIZE]: 15.00 log10[FLOPs]: 7.56:  45%|██████████████████▊                       | 458/1024 [02:03<02:32,  3.70it/s]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  7.565  log2[SIZE]:  15  log2[WRITE]:  19.192

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>

"""
我们甚至可以在路径搜索之后包含 reconfigure，这进一步大大提高了收缩路径的空间效率。
"""

opt = ctg.ReusableHyperOptimizer(
    minimize="combo",
    max_repeats=1024,
    max_time=120,
    progbar=True,
)


def opt_reconf(inputs, output, size, **kws):
    tree = opt.search(inputs, output, size)
    tree_r = tree.subtree_reconfigure_forest(
        progbar=True, num_trees=10, num_restarts=20, subtree_weight_what=("size",)
    )
    return tree_r.get_path()


# subtree_reconfigure_forest 还有一个默认的 parallel=True 选项，
# 对于上面的较新版本的 python，这只能设置为 “ray”
# 请注意不同版本的 cotengra 在最后一行中 API 如何发生了改变：get_path 或 path
# 用户可能需要更改 API 以使示例工作

tc.set_contractor(
    "custom",
    optimizer=opt_reconf,
    contraction_info=True,
    preprocessing=True,
    debug_level=2,
)
testbed()
# Output:
#   log2[SIZE]: 15.00 log10[FLOPs]: 7.46:  32%|█████████████▍                            | 329/1024 [02:00<04:13,  2.74it/s]

#   log2[SIZE]: 14.00 log10[FLOPs]: 7.02: 100%|█████████████████████████████████████████████| 20/20 [01:05<00:00,  3.30s/it]

#   ------ contraction cost summary ------

#   log10[FLOPs]:  7.021  log2[SIZE]:  14  log2[WRITE]:  19.953

#   <tf.Tensor: shape=(), dtype=complex64, numpy=0j>



================================================
FILE: docs/source/whitepaper/6-6-advanced-automatic-differentiation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Advanced Automatic Differentiation
"""

"""
## Overview

In this section, we review some advanced AD tricks, especially their application to circuit simulations. With these advanced AD tricks, we can evaluate some quantum quantities more efficiently.

The advanced AD is possible in TensorCircuit, as we have implemented several AD-related API in a backend agnostic way, the implementation of them closely follows the design philosophy of [jax AD implementation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).
"""

"""
## Setup
"""

import numpy as np
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

n = 6
nlayers = 3

"""
Backend agnostic AD related APIs include the following:
"""

help(K.grad)
help(K.value_and_grad)
help(K.vectorized_value_and_grad)
help(K.vjp)
help(K.jvp)
help(K.jacfwd)
help(K.jacrev)
help(K.stop_gradient)
help(K.hessian)
# Output:
#   Help on method grad in module tensorcircuit.backends.tensorflow_backend:

#   

#   grad(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0, has_aux: bool = False) -> Callable[..., Any] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Return the function which is the grad function of input ``f``.

#       

#       :Example:

#       

#       >>> f = lambda x,y: x**2+2*y

#       >>> g = tc.backend.grad(f)

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       2

#       >>> g = tc.backend.grad(f, argnums=(0,1))

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       [2, 2]

#       

#       :param f: the function to be differentiated

#       :type f: Callable[..., Any]

#       :param argnums: the position of args in ``f`` that are to be differentiated, defaults to be 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: the grad function of ``f`` with the same set of arguments as ``f``

#       :rtype: Callable[..., Any]

#   

#   Help on method value_and_grad in module tensorcircuit.backends.tensorflow_backend:

#   

#   value_and_grad(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0, has_aux: bool = False) -> Callable[..., Tuple[Any, Any]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Return the function which returns the value and grad of ``f``.

#       

#       :Example:

#       

#       >>> f = lambda x,y: x**2+2*y

#       >>> g = tc.backend.value_and_grad(f)

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       5, 2

#       >>> g = tc.backend.value_and_grad(f, argnums=(0,1))

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       5, [2, 2]

#       

#       :param f: the function to be differentiated

#       :type f: Callable[..., Any]

#       :param argnums: the position of args in ``f`` that are to be differentiated, defaults to be 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: the value and grad function of ``f`` with the same set of arguments as ``f``

#       :rtype: Callable[..., Tuple[Any, Any]]

#   

#   Help on method vectorized_value_and_grad in module tensorcircuit.backends.tensorflow_backend:

#   

#   vectorized_value_and_grad(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0, vectorized_argnums: Union[int, Sequence[int]] = 0, has_aux: bool = False) -> Callable[..., Tuple[Any, Any]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Return the VVAG function of ``f``. The inputs for ``f`` is (args[0], args[1], args[2], ...),

#       and the output of ``f`` is a scalar. Suppose VVAG(f) is a function with inputs in the form

#       (vargs[0], args[1], args[2], ...), where vagrs[0] has one extra dimension than args[0] in the first axis

#       and consistent with args[0] in shape for remaining dimensions, i.e. shape(vargs[0]) = [batch] + shape(args[0]).

#       (We only cover cases where ``vectorized_argnums`` defaults to 0 here for demonstration).

#       VVAG(f) returns a tuple as a value tensor with shape [batch, 1] and a gradient tuple with shape:

#       ([batch]+shape(args[argnum]) for argnum in argnums). The gradient for argnums=k is defined as

#       

#       .. math::

#       

#           g^k = \frac{\partial \sum_{i\in batch} f(vargs[0][i], args[1], ...)}{\partial args[k]}

#       

#       Therefore, if argnums=0, the gradient is reduced to

#       

#       .. math::

#       

#           g^0_i = \frac{\partial f(vargs[0][i])}{\partial vargs[0][i]}

#       

#       , which is specifically suitable for batched VQE optimization, where args[0] is the circuit parameters.

#       

#       And if argnums=1, the gradient is like

#       

#       .. math::

#           g^1_i = \frac{\partial \sum_j f(vargs[0][j], args[1])}{\partial args[1][i]}

#       

#       , which is suitable for quantum machine learning scenarios, where ``f`` is the loss function,

#       args[0] corresponds to the input data and args[1] corresponds to the weights in the QML model.

#       

#       :param f: [description]

#       :type f: Callable[..., Any]

#       :param argnums: [description], defaults to 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :param vectorized_argnums: the args to be vectorized, these arguments should share the same batch shape

#           in the fist dimension

#       :type vectorized_argnums: Union[int, Sequence[int]], defaults to 0

#       :return: [description]

#       :rtype: Callable[..., Tuple[Any, Any]]

#   

#   Help on method vjp in module tensorcircuit.backends.tensorflow_backend:

#   

#   vjp(f: Callable[..., Any], inputs: Union[Any, Sequence[Any]], v: Union[Any, Sequence[Any]]) -> Tuple[Union[Any, Sequence[Any]], Union[Any, Sequence[Any]]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Function that computes the dot product between a vector v and the Jacobian

#       of the given function at the point given by the inputs. (reverse mode AD relevant)

#       Strictly speaking, this function is value_and_vjp.

#       

#       :param f: the function to carry out vjp calculation

#       :type f: Callable[..., Any]

#       :param inputs: input for ``f``

#       :type inputs: Union[Tensor, Sequence[Tensor]]

#       :param v: value vector or gradient from downstream in reverse mode AD

#           the same shape as return of function ``f``

#       :type v: Union[Tensor, Sequence[Tensor]]

#       :return: (``f(*inputs)``, vjp_tensor), where vjp_tensor is the same shape as inputs

#       :rtype: Tuple[Union[Tensor, Sequence[Tensor]], Union[Tensor, Sequence[Tensor]]]

#   

#   Help on method jvp in module tensorcircuit.backends.tensorflow_backend:

#   

#   jvp(f: Callable[..., Any], inputs: Union[Any, Sequence[Any]], v: Union[Any, Sequence[Any]]) -> Tuple[Union[Any, Sequence[Any]], Union[Any, Sequence[Any]]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Function that computes a (forward-mode) Jacobian-vector product of ``f``.

#       Strictly speaking, this function is value_and_jvp.

#       

#       :param f: The function to compute jvp

#       :type f: Callable[..., Any]

#       :param inputs: input for ``f``

#       :type inputs: Union[Tensor, Sequence[Tensor]]

#       :param v: tangents

#       :type v: Union[Tensor, Sequence[Tensor]]

#       :return: (``f(*inputs)``, jvp_tensor), where jvp_tensor is the same shape as the output of ``f``

#       :rtype: Tuple[Union[Tensor, Sequence[Tensor]], Union[Tensor, Sequence[Tensor]]]

#   

#   Help on method jacfwd in module tensorcircuit.backends.abstract_backend:

#   

#   jacfwd(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Compute the Jacobian of ``f`` using the forward mode AD.

#       

#       :param f: the function whose Jacobian is required

#       :type f: Callable[..., Any]

#       :param argnums: the position of the arg as Jacobian input, defaults to 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: outer tuple for input args, inner tuple for outputs

#       :rtype: Tensor

#   

#   Help on method jacrev in module tensorcircuit.backends.abstract_backend:

#   

#   jacrev(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Compute the Jacobian of ``f`` using reverse mode AD.

#       

#       :param f: The function whose Jacobian is required

#       :type f: Callable[..., Any]

#       :param argnums: the position of the arg as Jacobian input, defaults to 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: outer tuple for output, inner tuple for input args

#       :rtype: Tensor

#   

#   Help on method stop_gradient in module tensorcircuit.backends.tensorflow_backend:

#   

#   stop_gradient(a: Any) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Stop backpropagation from ``a``.

#       

#       :param a: [description]

#       :type a: Tensor

#       :return: [description]

#       :rtype: Tensor

#   

#   Help on method hessian in module tensorcircuit.backends.abstract_backend:

#   

#   hessian(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#   


"""
## Forward AD

Using the Jacobian vector product (``jvp``), we can compute the circuit gradient in the forward AD mode, which is more suitable when the number of output elements is much larger than the input.

Suppose we are going to evaluate $\partial \vert \psi(\theta) \rangle$, where $\psi(\theta) = U(\theta)\vert 0\rangle$ is the output state of some parameterized quantum circuit.
"""

def ansatz(thetas):
    c = tc.Circuit(n)
    for j in range(nlayers):
        for i in range(n):
            c.rx(i, theta=thetas[j])
        for i in range(n - 1):
            c.cnot(i, i + 1)
    return c


def psi(thetas):
    c = ansatz(thetas)
    return c.state()

state, partial_psi_partial_theta0 = K.jvp(
    psi,
    K.implicit_randn([nlayers]),
    tc.array_to_tensor(np.array([1.0, 0, 0]), dtype="float32"),
)

"""
We thus obtain $\frac{\partial \psi}{\partial \theta_0}$, since the tangent takes one in the first place and zero in other positions.
"""

state.shape, partial_psi_partial_theta0.shape
# Output:
#   (TensorShape([64]), TensorShape([64]))

"""
## Jacobian

We can compute the Jacobian row by row or col by col using vmap together with reverse mode or forward mode AD.

We still use the above example, to calculate Jacobian $J_{ij}=\frac{\partial \psi_i}{\partial \theta_j}$.
"""

thetas = K.implicit_randn([nlayers])

jac_fun = K.jit(K.jacfwd(psi))

jac_value = jac_fun(thetas)

%timeit jac_fun(thetas)
# Output:
#   601 µs ± 36.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)


jac_value.shape
# Output:
#   TensorShape([64, 3])

"""
We can also use reverse mode AD to obtain Jacobian.
"""

jac_fun2 = K.jit(K.jacrev(psi))

jac_value2 = jac_fun2(thetas)

%timeit jac_fun2(thetas)
# Output:
#   843 µs ± 9.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)


jac_value2.shape
# Output:
#   TensorShape([64, 3])

np.testing.assert_allclose(np.real(jac_value), jac_value2, atol=1e-5)

"""
It is worth noting that forward mode AD Jacobian is faster since the result Jacobian is a tall matrix.
"""

"""
## Quantum Fisher Information

Quantum Fisher Information is a very important quantity in quantum computation, which can be utilized in so-called quantum natural gradient descent optimization as well as variational quantum dynamics. See [reference paper](https://arxiv.org/abs/1909.02108) for more details. 

There are several variants of QFI like object, and the core to evaluate is all related to $\langle \partial \psi \vert \partial \psi\rangle - \langle \partial \psi \vert \psi\rangle\langle \psi \vert \partial \psi\rangle$. Such quantity is easily obtained with an advanced AD framework by first getting the Jacobian for the state and then vmap the inner product over Jacobian rows. The detailed implementation can be found in the codebase ``tensorcircuit/experimental.py``. We directly call the corresponding API in this note.
"""

from tensorcircuit.experimental import qng

qfi_fun = K.jit(qng(psi))
qfi_value = qfi_fun(thetas)
qfi_value.shape
# Output:
#   WARNING:tensorflow:The dtype of the watched primal must be floating (e.g. tf.float32), got tf.complex64

#   TensorShape([3, 3])

%timeit qfi_fun(thetas) # the speed is comparable with a simple Jacobian computation
# Output:
#   609 µs ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)


"""
## Hessian

Hessian is defined as $\partial_{ij} \langle \psi(\theta)\vert H\vert \psi(\theta)\rangle$, where $ij$ is shorthand for $\theta_i\theta_j$.

In the following examples, we use $H=Z_0$ for simplicity.
"""

def h(thetas):
    c = ansatz(thetas)
    return K.real(c.expectation_ps(z=[0]))


hess_f = K.jit(K.hessian(h))

hess_value = hess_f(thetas)
hess_value.shape
# Output:
#   TensorShape([3, 3])

"""
## $\langle \psi \vert H \vert \partial \psi \rangle$

This quantity is very common as the RHS of the variational quantum dynamics equation. And there is no good way to compute this quantity besides constructing a corresponding Hadamard test circuit.

However, we can easily obtain this in the AD framework, as long as the ``stop_gradint`` API exists, which is the case for TensorCircuit. Namely, this quantity is obtained as $\partial (\langle \psi \vert H\vert \bot(\psi)\rangle)$, where the outside $\partial$ is automatically implemented by AD and $\bot$ is for ``stop_gradient`` op which stop the backpropagation.
"""

z0 = tc.quantum.PauliStringSum2Dense([[3, 0, 0, 0, 0, 0]])


def h(thetas):
    w = psi(thetas)
    wr = K.stop_gradient(w)
    wl = K.conj(w)
    wl = K.reshape(wl, [1, -1])
    wr = K.reshape(wr, [-1, 1])
    e = wl @ z0 @ wr
    return K.real(e)[0, 0]

psi_h_partial_psi = K.grad(h)(thetas)
psi_h_partial_psi.shape
# Output:
#   TensorShape([3])



================================================
FILE: docs/source/whitepaper/6-6-advanced-automatic-differentiation_cn.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# 高级自动微分
"""

"""
## 概述

在本节中，我们将回顾一些高级 AD 技巧，尤其是它们在电路模拟中的应用。借助这些高级的 AD 技巧，我们可以更高效地评估一些量子量。

高级 AD 在 TensorCircuit 中是可能的，因为我们已经以后端不可知的方式实现了几个与 AD 相关的 API，它们的实现紧密遵循
[JAX AD 实现](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)的设计理念。
"""

"""
## 设置
"""

import numpy as np
import tensorcircuit as tc

K = tc.set_backend("tensorflow")

n = 6
nlayers = 3

"""
与后端无关的 AD 相关 API 包括以下：
"""

help(K.grad)
help(K.value_and_grad)
help(K.vectorized_value_and_grad)
help(K.vjp)
help(K.jvp)
help(K.jacfwd)
help(K.jacrev)
help(K.stop_gradient)
help(K.hessian)
# Output:
#   Help on method grad in module tensorcircuit.backends.tensorflow_backend:

#   

#   grad(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0, has_aux: bool = False) -> Callable[..., Any] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Return the function which is the grad function of input ``f``.

#       

#       :Example:

#       

#       >>> f = lambda x,y: x**2+2*y

#       >>> g = tc.backend.grad(f)

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       2

#       >>> g = tc.backend.grad(f, argnums=(0,1))

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       [2, 2]

#       

#       :param f: the function to be differentiated

#       :type f: Callable[..., Any]

#       :param argnums: the position of args in ``f`` that are to be differentiated, defaults to be 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: the grad function of ``f`` with the same set of arguments as ``f``

#       :rtype: Callable[..., Any]

#   

#   Help on method value_and_grad in module tensorcircuit.backends.tensorflow_backend:

#   

#   value_and_grad(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0, has_aux: bool = False) -> Callable[..., Tuple[Any, Any]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Return the function which returns the value and grad of ``f``.

#       

#       :Example:

#       

#       >>> f = lambda x,y: x**2+2*y

#       >>> g = tc.backend.value_and_grad(f)

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       5, 2

#       >>> g = tc.backend.value_and_grad(f, argnums=(0,1))

#       >>> g(tc.num_to_tensor(1),tc.num_to_tensor(2))

#       5, [2, 2]

#       

#       :param f: the function to be differentiated

#       :type f: Callable[..., Any]

#       :param argnums: the position of args in ``f`` that are to be differentiated, defaults to be 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: the value and grad function of ``f`` with the same set of arguments as ``f``

#       :rtype: Callable[..., Tuple[Any, Any]]

#   

#   Help on method vectorized_value_and_grad in module tensorcircuit.backends.tensorflow_backend:

#   

#   vectorized_value_and_grad(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0, vectorized_argnums: Union[int, Sequence[int]] = 0, has_aux: bool = False) -> Callable[..., Tuple[Any, Any]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Return the VVAG function of ``f``. The inputs for ``f`` is (args[0], args[1], args[2], ...),

#       and the output of ``f`` is a scalar. Suppose VVAG(f) is a function with inputs in the form

#       (vargs[0], args[1], args[2], ...), where vagrs[0] has one extra dimension than args[0] in the first axis

#       and consistent with args[0] in shape for remaining dimensions, i.e. shape(vargs[0]) = [batch] + shape(args[0]).

#       (We only cover cases where ``vectorized_argnums`` defaults to 0 here for demonstration).

#       VVAG(f) returns a tuple as a value tensor with shape [batch, 1] and a gradient tuple with shape:

#       ([batch]+shape(args[argnum]) for argnum in argnums). The gradient for argnums=k is defined as

#       

#       .. math::

#       

#           g^k = \frac{\partial \sum_{i\in batch} f(vargs[0][i], args[1], ...)}{\partial args[k]}

#       

#       Therefore, if argnums=0, the gradient is reduced to

#       

#       .. math::

#       

#           g^0_i = \frac{\partial f(vargs[0][i])}{\partial vargs[0][i]}

#       

#       , which is specifically suitable for batched VQE optimization, where args[0] is the circuit parameters.

#       

#       And if argnums=1, the gradient is like

#       

#       .. math::

#           g^1_i = \frac{\partial \sum_j f(vargs[0][j], args[1])}{\partial args[1][i]}

#       

#       , which is suitable for quantum machine learning scenarios, where ``f`` is the loss function,

#       args[0] corresponds to the input data and args[1] corresponds to the weights in the QML model.

#       

#       :param f: [description]

#       :type f: Callable[..., Any]

#       :param argnums: [description], defaults to 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :param vectorized_argnums: the args to be vectorized, these arguments should share the same batch shape

#           in the fist dimension

#       :type vectorized_argnums: Union[int, Sequence[int]], defaults to 0

#       :return: [description]

#       :rtype: Callable[..., Tuple[Any, Any]]

#   

#   Help on method vjp in module tensorcircuit.backends.tensorflow_backend:

#   

#   vjp(f: Callable[..., Any], inputs: Union[Any, Sequence[Any]], v: Union[Any, Sequence[Any]]) -> Tuple[Union[Any, Sequence[Any]], Union[Any, Sequence[Any]]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Function that computes the dot product between a vector v and the Jacobian

#       of the given function at the point given by the inputs. (reverse mode AD relevant)

#       Strictly speaking, this function is value_and_vjp.

#       

#       :param f: the function to carry out vjp calculation

#       :type f: Callable[..., Any]

#       :param inputs: input for ``f``

#       :type inputs: Union[Tensor, Sequence[Tensor]]

#       :param v: value vector or gradient from downstream in reverse mode AD

#           the same shape as return of function ``f``

#       :type v: Union[Tensor, Sequence[Tensor]]

#       :return: (``f(*inputs)``, vjp_tensor), where vjp_tensor is the same shape as inputs

#       :rtype: Tuple[Union[Tensor, Sequence[Tensor]], Union[Tensor, Sequence[Tensor]]]

#   

#   Help on method jvp in module tensorcircuit.backends.tensorflow_backend:

#   

#   jvp(f: Callable[..., Any], inputs: Union[Any, Sequence[Any]], v: Union[Any, Sequence[Any]]) -> Tuple[Union[Any, Sequence[Any]], Union[Any, Sequence[Any]]] method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Function that computes a (forward-mode) Jacobian-vector product of ``f``.

#       Strictly speaking, this function is value_and_jvp.

#       

#       :param f: The function to compute jvp

#       :type f: Callable[..., Any]

#       :param inputs: input for ``f``

#       :type inputs: Union[Tensor, Sequence[Tensor]]

#       :param v: tangents

#       :type v: Union[Tensor, Sequence[Tensor]]

#       :return: (``f(*inputs)``, jvp_tensor), where jvp_tensor is the same shape as the output of ``f``

#       :rtype: Tuple[Union[Tensor, Sequence[Tensor]], Union[Tensor, Sequence[Tensor]]]

#   

#   Help on method jacfwd in module tensorcircuit.backends.abstract_backend:

#   

#   jacfwd(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Compute the Jacobian of ``f`` using the forward mode AD.

#       

#       :param f: the function whose Jacobian is required

#       :type f: Callable[..., Any]

#       :param argnums: the position of the arg as Jacobian input, defaults to 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: outer tuple for input args, inner tuple for outputs

#       :rtype: Tensor

#   

#   Help on method jacrev in module tensorcircuit.backends.abstract_backend:

#   

#   jacrev(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Compute the Jacobian of ``f`` using reverse mode AD.

#       

#       :param f: The function whose Jacobian is required

#       :type f: Callable[..., Any]

#       :param argnums: the position of the arg as Jacobian input, defaults to 0

#       :type argnums: Union[int, Sequence[int]], optional

#       :return: outer tuple for output, inner tuple for input args

#       :rtype: Tensor

#   

#   Help on method stop_gradient in module tensorcircuit.backends.tensorflow_backend:

#   

#   stop_gradient(a: Any) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#       Stop backpropagation from ``a``.

#       

#       :param a: [description]

#       :type a: Tensor

#       :return: [description]

#       :rtype: Tensor

#   

#   Help on method hessian in module tensorcircuit.backends.abstract_backend:

#   

#   hessian(f: Callable[..., Any], argnums: Union[int, Sequence[int]] = 0) -> Any method of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend instance

#   


"""
## 前向 AD

使用雅可比向量积（``jvp``），我们可以计算前向 AD 模式下的电路梯度，这在输出元素的数量远大于输入的情况下更合适。

假设我们要计算 $\partial \vert \psi(\theta) \rangle$，其中 $\psi(\theta) = U(\theta)\vert 0\rangle$ 是某个参数化量子电路的输出状态。
"""

def ansatz(thetas):
    c = tc.Circuit(n)
    for j in range(nlayers):
        for i in range(n):
            c.rx(i, theta=thetas[j])
        for i in range(n - 1):
            c.cnot(i, i + 1)
    return c


def psi(thetas):
    c = ansatz(thetas)
    return c.state()

state, partial_psi_partial_theta0 = K.jvp(
    psi,
    K.implicit_randn([nlayers]),
    tc.array_to_tensor(np.array([1.0, 0, 0]), dtype="float32"),
)

"""
因此我们得到 $\frac{\partial \psi}{\partial \theta_0}$，因为切向量在第一个位置取 1，在其他位置取 0。
"""

state.shape, partial_psi_partial_theta0.shape
# Output:
#   (TensorShape([64]), TensorShape([64]))

"""
## Jacobian

我们可以使用 vmap 和反向模式或前向模式 AD 逐行或逐列计算 Jacobian 行列式。

我们仍然使用上面的例子，来计算 Jacobian $J_{ij}=\frac{\partial \psi_i}{\partial \theta_j}$。
"""

thetas = K.implicit_randn([nlayers])

jac_fun = K.jit(K.jacfwd(psi))

jac_value = jac_fun(thetas)

%timeit jac_fun(thetas)
# Output:
#   601 µs ± 36.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)


jac_value.shape
# Output:
#   TensorShape([64, 3])

"""
我们也可以使用反向模式 AD 来获得 Jacobian。
"""

jac_fun2 = K.jit(K.jacrev(psi))

jac_value2 = jac_fun2(thetas)

%timeit jac_fun2(thetas)
# Output:
#   843 µs ± 9.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)


jac_value2.shape
# Output:
#   TensorShape([64, 3])

np.testing.assert_allclose(np.real(jac_value), jac_value2, atol=1e-5)

"""
值得注意的是，前向模式 AD Jacobian 更快，因为结果 Jacobian 是一个高矩阵。
"""

"""
## 量子费舍尔信息

量子Fisher信息是量子计算中一个非常重要的量，可用于所谓的量子自然梯度下降优化以及变分量子动力学。
有关详细信息，请参阅 [参考论文](https://arxiv.org/abs/1909.02108)。

类 QFI 的对象有多种变体，要评估的核心都与
$\langle \partial \psi \vert \partial \psi\rangle - \langle \partial \psi \vert \psi\rangle\langle \psi \vert \partial \psi\rangle$ 有关。
使用高级 AD 框架很容易获得这样的数量，方法是首先获取状态的 Jacobian，然后在 Jacobian 行上进行 vmap 内积。
详细的实现可以在代码库 ``tensorcircuit/experimental.py`` 中找到。我们在本笔记中直接调用对应的 API。
"""

from tensorcircuit.experimental import qng

qfi_fun = K.jit(qng(psi))
qfi_value = qfi_fun(thetas)
qfi_value.shape
# Output:
#   WARNING:tensorflow:The dtype of the watched primal must be floating (e.g. tf.float32), got tf.complex64

#   TensorShape([3, 3])

%timeit qfi_fun(thetas) # 速度与简单的 Jacobian 计算相当
# Output:
#   609 µs ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)


"""
## Hessian

Hessian 定义为 $\partial_{ij} \langle \psi(\theta)\vert H\vert \psi(\theta)\rangle$，其中 $ij$ 是 $\theta_i\theta_j$ 的简写。

在以下示例中，为简单起见，我们使用 $H=Z_0$。
"""

def h(thetas):
    c = ansatz(thetas)
    return K.real(c.expectation_ps(z=[0]))


hess_f = K.jit(K.hessian(h))

hess_value = hess_f(thetas)
hess_value.shape
# Output:
#   TensorShape([3, 3])

"""
## $\langle \psi \vert H \vert \partial \psi \rangle$

这个量作为变分量子动力学方程的 RHS 非常常见。
除了构建相应的 Hadamard 测试电路外，没有很好的方法来计算这个量。

但是，只要存在 ``stop_gradint`` API，我们就可以在 AD 框架中轻松获取，TensorCircuit 就是这种情况。
即这个量得到 $\partial (\langle \psi \vert H\vert \bot(\psi)\rangle)$，其中外部 $\partial$ 由 AD 自动实现，
$\bot$ 为停止反向传播的 ``stop_gradient`` 操作。
"""

z0 = tc.quantum.PauliStringSum2Dense([[3, 0, 0, 0, 0, 0]])


def h(thetas):
    w = psi(thetas)
    wr = K.stop_gradient(w)
    wl = K.conj(w)
    wl = K.reshape(wl, [1, -1])
    wr = K.reshape(wr, [-1, 1])
    e = wl @ z0 @ wr
    return K.real(e)[0, 0]

psi_h_partial_psi = K.grad(h)(thetas)
psi_h_partial_psi.shape
# Output:
#   TensorShape([3])


